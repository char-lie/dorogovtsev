\section{Условное математическое ожидание}
\index{условное!математическое ожидание}
\index{математическое ожидание!условное}
Пусть $\eta$ --- произвольная случайная величина, интегрируемая с квадратом.
Нужно найти случайную величину $\tilde{\eta}$ которая
измерима в $\sigma\left( \xi \right)$
и является наиболее близкой в среднем квадратическом к $\eta$.

\subsection{Проекция вектора}
Сейчас для нас снова будет полезна аналогия с векторами в пространстве.
будет полезна аналогия с векторами в пространстве.
Имеется точка $x$ в пространстве $L'$.
Мы ищем такую точку $y$ в подпространстве $L\subset L'$,
что расстояние между $x$ и $y$ минимальное.
Значит, надо опустить перпендикуляр от $y$ на $L$
(рисунок \ref{fig:tikz:vectorProjection}).
Пусть $e_1, e_2, \dots, e_n$ --- ортонормированный базис в $L$,
тогда $y$ можно найти по формуле
\index{проекция!вектора}
\index{вектор!проекция}
\begin{equation}\label{vectorProjection}
  y = \sum_{k=1}^n \left( x, e_k \right) \cdot e_k
\end{equation}
\begin{comment}
Потому что $y \in L$ должен лежать в пространстве $L$ по условию,
а это значит, что он должен быть линейной комбинацией базисных векторов
$e_1, e_2, \dots, e_n$ и это очевидно выполняется

Также разностью $x-y$ должен быть вектор, перпендикулярный пространству $L$.
То есть скалярное произведение этой разности с любым вектором $z$
из пространства $L$ должно равняться нулю
$$\left( x-y \right) \perp L
  \Leftrightarrow \forall z \in L:
  \left( x-y,z \right)=0$$

Вследствие линейности скалярного произведения можно переписать это условие иначе
\begin{align*}
  \begin{cases}
      \forall z \in L: \left( x-y,z \right)=0\\
      \left( a+b,c \right)= \left( a,c \right)+ \left( b,c \right)
  \end{cases}
  \Rightarrow \forall z \in L: \left( x,z \right)= \left( y,z \right)
\end{align*}

Покажем, что и это выполняется.
$z$ является линейной комбинацией базисных векторов. Запишем это
$$z = \sum_{k=1}^n \beta_k \cdot e_k$$

В таком случае скалярное произведение $\left( x,z \right)$ будет таким
$$\left( x,z \right)= \sum_{k=1}^n \beta_k \cdot \left( x,e_k \right)$$

С произведением $\left( y,x \right)$ придётся чуть-чуть повозиться
$$\left( y,x \right)
  = \left( \sum_{k=1}^n\left( x,e_k \right)\cdot e_k,
      \sum_{k=1}^n \beta_k \cdot e_k \right)
  = \sum_{k=1}^n \left( x,e_k \right)\cdot \beta_k$$

Как видим, суммы равны, а значит, проекция $x$ на $L$ найдена верно.
\end{comment}
\begin{figure}[h!]
  \center\includestandalone{tikz/vectorProjection}
  \caption{Проекция}
  \label{fig:tikz:vectorProjection}
\end{figure}

\subsection{Проекция случайной величины}
Воспользуемся приведённым геометрическим фактом применительно к случайным
величинам.
Возьмём в качестве $L$ множество всех случайных величин, которые
измеримы относительно $\sigma\left( \xi \right)$.
$$L \ni \sum_{k=1}^n c_k \cdot \indicatorof{H_k}, c_k \in \mathbb{R}$$

В качестве скалярного произведения случайных величин
используем математическое ожидание произведения.

\begin{comment}
Оказывается, $H_k$ действительно ортогональны
$$k_1 \neq k_2
  \Rightarrow H_{k_1} \cap H_{k_2} = \emptyset
  \Rightarrow
  \Mean{ \indicatorof{H_{k_1}}\cdot \indicatorof{H_{k_1}} } = 0$$

Теперь нужно нормировать эти базисные вектора,
а для этого их надо поделить на их нормы.
В нашем пространстве норма порождена скалярным произведением,
то есть
$$\left\| x \right\| = \sqrt{\left( x,x \right)}
  = \sqrt{\Mean{ x \cdot x }}
  = \sqrt{\Mean{ x^2 }}, x \in L$$

Теперь у нас есть всё необходимое для того,
чтобы представить ортонормированный базис.
Начнём преобразования $H_k$
$$e_k = \frac{\indicatorof{H_k}}
  {\sqrt{\mean{\left( \indicatorof{H_k} \right)^2}}}$$

Поскольку индикатор может принимать лишь одно из двух значений $0$ или $1$,
а их квадраты равны им самим, то в формуле квадрат тоже можно убрать
$$e_k = \frac{\indicatorof{H_k}}{\sqrt{\mean{\indicatorof{H_k}}}}$$

Также помним, что математическое ожидание в знаменателе есть ни что иное,
как вероятность события $H_k$,
и теперь у нас есть красивый ортонормированный базис
\end{comment}
У нас есть ортонормированный базис
\begin{equation}\label{orthonormalBasis}
e_k = \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}}
\end{equation}

Теперь проекция случайной величины $\eta$ на $L$ имеет вид
\begin{equation}\label{discreteConditionalExpectation}
  \begin{split}
  \tilde{\eta}
  = \sum_{k=1}^n \left( \eta, e_k \right) \cdot e_k
  = \sum_{k=1}^n
      \left( \eta, \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}} \right) 
      \cdot \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}} = \\
  = \sum_{k=1}^n
      \mean{\left( \eta
      \cdot \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}} \right)}
      \cdot \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}} = \\
  = \sum_{k=1}^n
      \frac{\Mean{ \eta \cdot \indicatorof{H_k} }}
      {\sqrt{\probability{H_k}}}
      \cdot \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}} = \\
  = \sum_{k=1}^n
      \frac{\Mean{\eta \cdot \indicatorof{H_k}}}{\probability{H_k}}
      \cdot \indicatorof{H_k}
  \end{split}
\end{equation}

На что стоит обратить внимание в этой формуле:
\begin{enumerate}
  \item $\tilde{\eta}$ --- \textbf{случайная величина},
      так как индикатор вне математического ожидания --- случайная величина,
      и результат суммы будет зависеть от произошедшего $\omega$,
      а точнее от множества уровня $H_k$, которому оно принадлежит
  \item Когда $\omega$ принадлежит $H_k$,
      то результатом суммы будет среднее значение случайной величины $\eta$
      на событии $H_k$
\end{enumerate}

\begin{comment}
Если с первым пунктом всё очевидно,
то небольшое пояснение ко второму не помешает.

Нужно показать, что $k$-я ``координата'' случайной величины $\tilde{\eta}$
действительно даёт среднее значение случайной величины $\eta$ на событии $H_k$
$$\frac{\Mean{ \eta \cdot \indicatorof{H_k} }}{\probability{H_k}}$$

Начнём с определения математического ожидания
\begin{equation}\label{meanIndicator}
  \begin{aligned}
  \Mean{ \eta \cdot \indicatorof{H_k} }
      &= \integrall{\Omega}{\probability{d\omega}}{
      \eta\left( \omega \right) \cdot \indicatorof{H_k}} = \\
      &= \integrall{H_k}{\probability{d\omega}}{\eta\left( \omega \right)}
      + \integrall{\Omega \setminus H_k}{
      \probability{d\omega}}{0}
  \end{aligned}
\end{equation}

Видим математическое ожидание случайной величины,
которая гарантированно принимает нулевое значение
на множестве $\Omega \setminus H_k$,
что в свою очередь искажает желаемую картину и притягивает результат к нулю
с силой, которая пропорциональна $\probability{\Omega \setminus H_k}$.
То есть ``вес'' каждого ненулевого значения случайной величины уменьшился.

Почему так происходит?
Потому что вероятность события $H_k$ в общем случае не равна единице.
Если ввести новую меру
$\mathbb{P}_k\left( A \right)= \frac{\probability{A}}{\probability{H_k}}$,
то наступит гармония, а
вероятность $\mathbb{P}_k\left( H_k \right)$ будет равна единице.

Из контекста понятно, что эта мера будет использоваться лишь в интеграле
по событию $H_k$,
поэтому её значение будет колебаться в пределах $\left[ 0;1 \right]$,
но строгости ради введём небольшую поправку (и увидим, что не напрасно)
$$\mathbb{P}_k\left( A \right)
  = \frac{\probability{A\cap H_k}}{\probability{H_k}}$$

Видим условную вероятность, а это значит, что мы на правильном пути!
Логично, что в поисках условного математического ожидания
должна была встретиться условная вероятность
$$\mathbb{P}_k\left( A \right)
  = \frac{\probability{A\cap H_k}}{\probability{H_k}}
  = \probability{A \mcond H_k}$$

Теперь математическое ожидание \eqref{meanIndicator}
принимает несколько иной вид
$$\Mean{ \eta \cdot \indicatorof{H_k} }
      = \probability{H_k}
      \cdot \integrall{H_k}{\probability{d\omega \mcond H_k}}{
      \eta\left( \omega \right)}$$

Тут уже уровень $H_k$ играет роль целого множества элементарных исходов,
его мера $\probability{H_k \mcond H_k}$ равна единице,
а мы получаем действительно среднее значение случайной величины $\eta$
на множестве $H_k$, умноженное на вероятность $\probability{H_k}$.
Значит, осталось лишь поделить обе части на $\probability{H_k}$
$$\frac{\Mean{ \eta \cdot \indicatorof{H_k} }}{\probability{H_k}}
      = \integrall{H_k}{\probability{d\omega \mcond H_k}}{
      \eta\left( \omega \right)}$$

\begin{definition}[Условное математическое ожидание случайной величины
  относительно случайного события]
  \label{eventConditionalExpectationDefinition}
  Условное математическое ожидание случайной величины $\xi$
  относительно \textbf{события} $A$ \cite[стр.~68]{BorovkovPT}
  обозначается $\Mean{\xi \mcond A}$ и считается по формуле
  $$\Mean{\xi \mcond A}
      = \frac{\Mean{\xi \cdot \indicatorof{A}}}{\probability{A}}
      = \integrall{A}{\probability{d\omega \mcond A}}{
      \xi\left( \omega \right)}$$
\end{definition}

Пользуясь только что введённым обозначением,
можно более красиво переписать формулу \eqref{discreteConditionalExpectation}
для получения проекции случайной величины $\eta$ на $\sigma$-алгебру,
порождённой уровнями $H_1, H_2, \dots, H_n$ 
$$\tilde{\eta}
  = \sum_{k=1}^n \Mean{ \eta \mcond H_k } \cdot \indicatorof{H_k}$$

Забегая наперёд,
введём определение частного случая условного математического ожидания
случайной величины относительно \textbf{$\sigma$-алгебры},
чтобы обратить внимание на этот важный момент.
\end{comment}
\begin{definition}[Условное математическое ожидание случайной величины
  относительно случайного события]
  \label{eventConditionalExpectationDefinition}
  Условное математическое ожидание случайной величины $\xi$
  относительно \textbf{события} $A$ \cite[стр.~68]{BorovkovPT}
  обозначается $\Mean{\xi \mcond A}$ и считается по формуле
  \begin{equation*}
    \Mean{\xi \mcond A}
      = \frac{\Mean{\xi \cdot \indicatorof{A}}}{\probability{A}}
      = \integrall{A}{\probability{d\omega \mcond A}}{
      \xi\left( \omega \right)},
  \end{equation*}
  где
  \begin{equation*}
    d\omega \in A,\qquad
    \probability{d\omega \mcond A}
    = \frac{\probability{d\omega}}{\probability{A}}
  \end{equation*}
\end{definition}

\begin{definition}[Условное математическое ожидание
  случайной величины относительно конечной сигма-алгебры]
  \label{def:SA:finiteSA}
  \index{условное!математическое ожидание!относительно конечной сигма-алгебры}
  \index{математическое ожидание!условное!относительно конечной сигма-алгебры}
  \label{discreteConditionalExpectationDefinition}
  Есть $\sigma$-алгебра $\mathfrak{F}_1$,
  порожждённая множествами $H_1, H_2, \dots, H_n$.
  Тогда условное математическое ожидание случайной величины
  $\eta$ относительно этой $\sigma$-алгебры --- \textbf{случайная величина},
  которая обозначается $\Mean{ \eta \mcond \mathfrak{F_1} }$
  и вычисляется по формуле
  $$\Mean{ \eta \mcond \mathfrak{F}_1 }
      = \sum_{k=1}^n
      \frac{\Mean{ \eta \cdot \indicatorof{H_k} }}
      {\probability{H_k}}
      \cdot \indicatorof{H_k}$$
\end{definition}

\begin{remark}
  У нас есть определения условного математического ожидания
  относительно \textbf{$\sigma$-алгебры} $\mathfrak{F}$ (определение
  \ref{discreteConditionalExpectationDefinition})
  и относительно \textbf{случайного события} $A$ (определение 
  \ref{eventConditionalExpectationDefinition}).
  Из контекста будет ясно, какое именно определение используется,
  поэтому путаницы возникнуть не должно.

  Перепишем последнее орпеделение в терминах математического
  ожидания относительно случайных событий
  \begin{equation*}
    \Mean{ \eta \mcond \mathfrak{F}_1 }
    = \sum_{k=1}^n \Mean{ \eta \mcond H_k }
      \cdot \indicatorof{H_k}
  \end{equation*}

  При детальном рассмотрении из записи очевиден её смысл:
  условное математическое ожидание относительно $\sigma$-алгебры --- вектор
  в пространстве с базисом $\left\{ H_1, \dots, H_n \right\}$, элементами
  которого являются проекции вектора $\eta$ на соответствующие оси.
  
  Ведь $\Mean{ \eta \mcond H_k }$ --- ни что иное, как проекция
  вектора (случайной величины) $\eta$ на ось (уровень) $H_k$.
  Эта величина является скаляром, как и проекция вектора на ось.
\end{remark}

\begin{lemma}[Равенство скалярных произведений для конечной сигма-алгебры]
  \label{lemma:scalarMul:finiteSA}
  \index{лемма!условное математическое ожидание!конечная сигма-алгебра}
  Для случайной величины $\eta$ и её проекции $\tilde{\eta}$
  на $\sigma$-алгебру $\mathfrak{F}_\xi$,
  порождённую случайной величиной $\xi$,
  принимающей конечное количество значений,
  выполняется равенство скалярных произведений
  \begin{equation}\label{scalarMulEquality}
      \forall A \in \mathfrak{F}_\xi:
      \Mean{ \tilde{\eta} \cdot \indicatorof{A} }
      = \Mean{ \eta \cdot \indicatorof{A} }
  \end{equation}
\end{lemma}
\begin{proof}
  \begin{equation*}
    \Mean{ \tilde{\eta} \cdot \indicatorof{A} }
    = \mean{\left( \sum_{k=1}^n
      \frac{\Mean{ \eta \cdot \indicatorof{H_k} }}
      {\probability{H_k}}
      \cdot \indicatorof{H_k} \cdot \indicatorof{A} \right)}
    = \sum_{k=1}^n
      \frac{\Mean{ \eta \cdot \indicatorof{H_k} }}
      {\probability{H_k}}
      \cdot \Mean{ \indicatorof{H_k \cap A} }
  \end{equation*}

Помним, что математическое ожидание индикатора --- вероятность
$$\sum_{k=1}^n
  \frac{\Mean{ \eta \cdot \indicatorof{H_k} }}{\probability{H_k}}
      \cdot \Mean{ \indicatorof{H_k \cap A} }
  = \sum_{k=1}^n \frac{\Mean{ \eta \cdot \indicatorof{H_k} }}
      {\probability{H_k}}
      \cdot \probability{H_k \cap A}$$

Замечаем условную вероятность
\begin{align*}
  \sum_{k=1}^n \frac{\Mean{ \eta \cdot \indicatorof{H_k} }}
      {\probability{H_k}} \cdot \probability{H_k \cap A}
  &= \sum_{k=1}^n \Mean{ \eta \cdot \indicatorof{H_k} }
      \cdot \frac{\probability{H_k \cap A}}{\probability{H_k}} = \\
  &= \sum_{k=1}^n \Mean{ \eta \cdot \indicatorof{H_k} }
      \cdot \probability{A \mcond H_k}
\end{align*}

Событие $A$ принадлежит $\sigma$-алгебре случайных событий $\mathfrak{F}_\xi$.
Значит, условная вероятность $\probability{A\mcond H_k}$ равна
либо нулю, либо единице, поскольку $A$ либо включает в себя уровень $H_k$, либо
не пересекается с ним.
То есть получился индикатор $\indicator{H_k \subseteq A}$.
А этот индикатор говорит о том, что теперь надо суммировать лишь по тем $H_k$,
которые являются частью события $A$
\begin{align*}
\sum_{k=1}^n \Mean{ \eta \cdot \indicatorof{H_k} }
  \cdot \probability{A \mcond H_k}
  = \sum_{k=1}^n \Mean{ \eta \cdot \indicatorof{H_k} }
      \cdot \indicator{H_k \subseteq A} = \\
  = \sum_{H_k \subseteq A} \Mean{ \eta \cdot \indicatorof{H_k} }
  = \Mean{ \sum_{H_k \subseteq A} \eta \cdot \indicatorof{H_k} }
\end{align*}

\begin{comment}
Далее мы имеем полное математическое и моральное право
вынести $\eta$ за знак суммы.
Если с математикой всё очевидно (работает закон дистрибутивности),
то напомню о морально-этической стороне дела: нам нужно,
пройтись по всем возможным индикаторам $\indicatorof{H_k}$,
из которых лишь один сработает (будет равен единице, а не нулю),
поэтому сумма нужна лишь для того,
чтобы не писать в конце каждой строчки ``для тех $\omega$, что входят в $H_k$''
(помним, что случайная величина и индикатор --- функции
от элементарного события $\omega$)
$$\Mean{ \sum_{H_k \subseteq A} \eta \cdot \indicatorof{H_k} }
  = \Mean{\eta \left( \omega \right) \cdot \sum_{H_k \subseteq A}
      \indicatorof{H_k}\left( \omega \right)}$$

Сумма индикаторов непересекающихся событий --- индикатор их объединения,
которое является множеством $A$.
Не забываем, что оно может состоять из объединений уровней и только из них
(или же быть пустым)
$$\Mean{ \eta \cdot \sum_{H_k \subseteq A} \indicatorof{H_k} }
  = \Mean{ \eta \cdot \indicatorof{A} }$$
\end{comment}

Значит, равенство \eqref{scalarMulEquality} выполняется.
\end{proof}

\begin{remark}
  В связи с выполнением равенства скалярных произведений можем сделать вывод,
  что математическое ожидание случайной величины и её проекции тоже равны.
  Это нетрудно показать,
  установив $A$ равным всему множеству элементарных исходов
  (индикатор в таком случае станет просто тождественной единицей)
  $$\mean{\eta}
      = \Mean{ \eta \cdot \indicatorof{\Omega} }
      = \Mean{ \tilde{\eta} \cdot \indicatorof{\Omega} }
      = \mean{ \tilde{\eta} }$$
\end{remark}

\subsection{Условное математическое ожидание}
Введём общее определение для условного математического ожидания
случайной величины относительно $\sigma$-алгебры

\begin{definition}[Условное математическое ожидание случайной величины
  относительно сигма-алгебры]
  \label{def:SA:SA}
  \index{условное!математическое ожидание!в общем виде}
  \index{математическое ожидание!условное!в общем виде}
  Условным математическим ожиданием случайной величины $\eta$
  относительно $\sigma$-алгебры $\mathfrak{F}_1 \subset \mathfrak{F}$
  называется такая случайная величина $\tilde{\eta}$, что
  \begin{enumerate}
      \item Случайная величина $\tilde{\eta}$
      измерима относительно $\sigma$-алгебры $\mathfrak{F}_1$
      \item Выполняется равенство
      $$\forall A \in \mathfrak{F}_1:
      \Mean{ \tilde{\eta} \cdot \indicatorof{A} }
    = \Mean{ \eta \cdot \indicatorof{A} }$$
  \end{enumerate}
  Обозначение $\tilde{\eta} = \Mean{ \eta \mcond \mathfrak{F}_1 }$

\end{definition}

\begin{remark}
  В силу леммы \ref{lemma:scalarMul:finiteSA} определения
  \ref{def:SA:finiteSA} и \ref{def:SA:SA} для $\sigma$-алгебры, порождённой
  дискретной случайной величиной, совпадают.
\end{remark}

\begin{remark}
  Условное математическое ожидание случайной величины $\eta$
  относительно $\sigma$-алгебры, порождённой случайной величиной $\xi$,
  будем обозначать $\Mean{ \eta \mcond \sigma\left( \xi \right) }$,
  а более кратко $\Mean{ \eta \mcond \xi }$.

  То есть имеем три эквивалентных записи
  $$\Mean{ \eta \mcond \mathfrak{F}_{\xi} }
      = \Mean{ \eta \mcond \sigma\left( \xi \right) }
      = \Mean{ \eta \mcond \xi }$$
\end{remark}

Немного остановимся на примере, чтобы понять, что у нас есть на данный момент

\begin{example}\label{discreteConditionalExpectationExample}
  У нас есть две дискретные случайные величины $\xi$ и $\eta$
  с совместным дискретным распределением
  $$\Probability{\eta = a_i, \xi = b_j} = p_{ij}$$
  Числа $p_{ij}$ обладают свойствами
  $$p_{ij} \ge 0, \qquad \sum_{i,j=1}^\infty p_{ij} = 1,
      \qquad \Probability{\xi = b_j} = \sum_{i=1}^\infty p_{ij}$$

  Посчитаем условное математическое ожидание согласно формуле
  из определения \eqref{discreteConditionalExpectationDefinition}
  $$\Mean{\eta \mcond \sigma\left( \xi \right)} = \sum_{k=1}^n
      \frac{\Mean{\eta \cdot \indicatorof{H_k}}}{\probability{H_k}}
      \cdot \indicatorof{H_k}$$

  Для этого выясним, чему равно математическое ожидание $\xi$
  при определённом значении $\eta$ по формуле из определения
  \eqref{eventConditionalExpectationDefinition}
  $$\Mean{\eta \mcond \xi = b_j}
      = \frac{\Mean{\eta \cdot \indicatorof{\eta = b_j}}}{
      \Probability{\xi = b_j}}
      = \frac{\sum_{i=1}^{\infty} a_i \cdot p_{ij}}{
      \sum_{i=1}^{\infty} p_{ij}}$$
\end{example}

Попробуем обобщить определение условного математического ожидания,
чтобы обладать универсальной формулой, из которой можно делать какие-то выводы.
Начнём с формулы для случая конечного числа значений
$$\Mean{ \eta \mcond \mathfrak{F}_\xi }
  = \sum_{k=1}^n \Mean{ \eta \mcond H_k } \cdot \indicatorof{H_k}$$

Вспомним, что
\begin{equation*}
  H_k = \left\{ \omega \mcond \xi\left( \omega \right) = a_k \right\}
\end{equation*}
Для краткости будем использовать запись $H_k = \left\{ \xi = a_k \right\}$.
Тогда
\begin{equation*}
  \Mean{\eta \mcond \sigma\left( \xi \right)}
  = \sum_{k=1}^{n} \Mean{\eta \mcond \xi = a_k} \cdot \Indicator{\xi = a_k}
\end{equation*}
Введём функцию
\begin{equation*}
  \forall x \in \left\{ a_1, \dots, a_n \right\}:\qquad
  \varphi^{\eta}\left( x \right) = \Mean{\eta \mcond \xi = x}
\end{equation*}
Получаем
\begin{equation*}
  \Mean{\eta \mcond \sigma\left( \xi \right)}
  = \varphi^{\eta}\left( \xi \right)
\end{equation*}

Пусть случайные величины $\xi$ и $\eta$ имеют совместную плотность
распределения
$$\Probability{\left( \xi, \eta \right) \in \Delta}
  = \iint\limits_\Delta\pdf{x,y} \, dx \, dy$$
В таком случае компонента $\xi$ имеет плотность $r$
$$r\left( x \right) = \integral{\mathbb{R}}{}{y}{\pdf{x,y}}$$
Компонента $\eta$ имеет плотность $q$
$$q\left( y \right) = \integral{\mathbb{R}}{}{x}{\pdf{x,y}}$$
Для $f \in C\left( \mathbb{R}^2 \right)$ найдём
$\Mean{f\left( \xi, \eta \right) \mcond \sigma\left( \xi \right)}$.
Помним, что для некоторой борелевской функции $\varphi$ справедливо равенство
\begin{equation*}
  \Mean{f\left( \xi, \eta \right) \mcond \sigma\left( \xi \right)}
  = \varphi\left( \xi \right)
\end{equation*}
В определении условного математического ожидания возьмём
$A = \left\{ \xi \in \left( -\infty; x \right]
  \in \sigma\left( \xi \right) \right\}$
\begin{equation*}
  \Mean{\varphi\left( \xi \right) \cdot \Indicator{\xi \le x}}
  = \Mean{f\left( \xi, \eta \right) \cdot \Indicator{\xi \le x}}
\end{equation*}
Перепишем равенство через интегралы
\begin{equation*}
  \integral{-\infty}{x}{t}{\varphi\left( t \right) \cdot r\left( t \right)}
  = \integral{-\infty}{x}{t}{
    \integralr{y}{f\left( t, y \right) \cdot \pdf{t, y}}}
\end{equation*}
Продифференцируем его по $dt$ с обеих сторон
\begin{equation*}
  \varphi\left( x \right) \cdot r\left( x \right)
  = \integralr{y}{f\left( x, y \right) \cdot \pdf{x, y}}
\end{equation*}
Получаем
\begin{equation*}
  \varphi\left( x \right)
  = \frac{\integralr{y}{f\left( x, y \right) \cdot \pdf{x, y}}}{
    r\left( x \right)}
  = \frac{\integralr{y}{f\left( x, y \right) \cdot \pdf{x, y}}}{
    \integralr{y}{\pdf{x, y}}}
\end{equation*}

Следовательно,
\begin{equation}\label{phiIntegral}
  \Mean{f\left( \xi, \eta \right) \mcond \sigma\left( \xi \right)}
  = \frac{\integral{\mathbb{R}}{}{y}{
      f\left( \xi, y \right) \cdot \pdf{\xi, y}}}{
    \integral{\mathbb{R}}{}{y}{\pdf{\xi, y}}}
\end{equation}
Для случая $f\left( \xi, \eta \right) = \eta$ имеем
\begin{equation}
  \Mean{\eta \mcond \sigma\left( \xi \right)}
  = \frac{\integral{\mathbb{R}}{}{y}{y \cdot \pdf{\xi, y}}}
    {\integral{\mathbb{R}}{}{y}{\pdf{\xi, y}}}
\end{equation}
\begin{comment}
Докажем снова, что $\varphi^\eta\left( \xi \right)$ является
условным математическим ожиданием случайной величины $\eta$
относительно $\sigma$-алгебры, порождённой случайной величиной $\xi$.
Чтобы не было скучно, будем доказывать несколько иначе, чем ранее.

\begin{lemma}[Равенство скалярных произведений
  условного математического ожидания
  случайных величин с совместной плотностью]
  Пускай имеются две случайные величины $\left( \xi, \eta \right)$
  с совместной плотностью $\pdf{x,y}$.
  \index{условное!математическое ожидание!
      случайных величин с совместной плотностью}
  Тогда функция
  $$\varphi^{\eta}\left( \xi \right)
      = \left. \frac{\integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y}}}
      {\integral{\mathbb{R}}{}{y}{\pdf{x,y}}} \right|_{x= \xi}$$

  Является условным математическим ожиданием
  $\Mean{ \eta \mcond \xi }$

\end{lemma}
\begin{proof}
Первое свойство снова очевидно, поэтому надо доказать
\begin{equation}\label{conditionalExpectationContinualStart}
  \forall A \in \sigma\left( \xi \right):
      \Mean{\varphi^{\eta}\left( \xi \right) \cdot \indicatorof{A}}
      = \Mean{ \eta \cdot \indicatorof{A} }
\end{equation}

У нас есть совместная плотность и мы хотим посчитать математическое ожидание,
пользуясь именно ею.
Для этого превратим индикатор $\indicator{\omega \in A}$
в функцию случайной величины $\xi$.
Поскольку любое событие $A$ принадлежит $\sigma\left( \xi \right)$,
то его можно представить в виде
$\xi^{-1}\left( \Delta \right), \Delta \in \mathfrak{B}$.
Перепишем индикатор следующим образом:
$\indicator{\omega \in A} = \indicator{\xi \in \Delta}$.
И вот теперь мы готовы к тому,
чтобы записать определение математического ожидания
$$\Mean{ \varphi^{\eta}\left( \xi \right) \cdot \indicatorof{A} }
  = \integral{\mathbb{R}}{}{y}{ \integral{\mathbb{R}}{}{x}{
      \varphi^{\eta}\left( x \right) \cdot \indicator{x \in \Delta}
      \cdot \pdf{x,y}}}$$

От $y$ зависит лишь совместная плотность, а интеграл от неё по всей оси $y$
является плотностью распределения $\xi$.
То есть интеграл по $y$ уходит,
а вместо $\pdf{x,y}$ появляется $r\left( x \right)$.
Также учтём индикатор и сузим область интегрирования с $\mathbb{R}$ до $\Delta$
$$\integral{\mathbb{R}}{}{y}{ \integral{\mathbb{R}}{}{x}{
  \varphi^{\eta}\left( x \right) \cdot \indicator{x \in \Delta}
      \cdot \pdf{x,y}}}
  = \integral{\Delta}{}{x}{
      \varphi^{\eta}\left( x \right) \cdot r\left( x \right)}$$

Дальше распишем функцию $\varphi^{\eta}$,
пользуясь формулой \eqref{phiIntegral}
$$\integral{\Delta}{}{x}{
  \varphi^{\eta}\left( x \right) \cdot r\left( x \right)}
  = \integral{\Delta}{}{x}{\left(
      \frac{\integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y}}}
      {r\left( x \right)}
      \cdot r\left( x \right) \right)}$$

Сократим одинаковые плотности и получим интересный двойной интеграл
$$\integral{\Delta}{}{x}{\left(
      \frac{\integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y}}}
      {r\left( x \right)}
      \cdot r\left( x \right) \right)}
  = \integral{\Delta}{}{x}{
      \integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y}}}$$

Вернём индикатор обратно в интеграл
$$\integral{\Delta}{}{x}{
  \integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y} \cdot}}
  = \integral{\mathbb{R}}{}{x}{
      \integral{\mathbb{R}}{}{y}{y \cdot \indicator{x \in \Delta}
      \cdot \pdf{x,y}}}$$

Видим, что это и есть то математическое ожидание, которое нам нужно
$$\integral{\mathbb{R}}{}{x}{
  \integral{\mathbb{R}}{}{y}{y \cdot \indicator{x \in \Delta}
      \cdot \pdf{x,y}}}
  = \Mean{ \eta \cdot \indicatorof{\xi \in \Delta} }
  = \Mean{ \eta \cdot \indicatorof{A} }$$

Это значит, что тождество доказано и условное математическое ожидание
для случайных величин с совместной плотностью считается с помощью
$$\varphi^{\eta}\left( x \right)
  = \frac{\integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y}}}
      {\integral{\mathbb{R}}{}{y}{\pdf{x,y}}}$$

По формуле
$$\Mean{ \eta \mcond \xi }
  = \varphi^{\eta}\left( \xi \right)
  = \left. \varphi^{\eta}\left( x \right) \right|_{x= \xi}$$
\end{proof}
\end{comment}

\begin{remark}
  Предыдущее доказательство предполагает существание условного математического
  ожидание, что гарантирует следующая теорема.
\end{remark}

\begin{theorem}[Существование условного математического ожидания]
  \index{теорема!о существовании!условного математического ожидания}
  Для случайной величины $\xi$ такой, что $\mean{\left| \xi \right|} < +\infty$
  и произвольной $\sigma$-алгебры $\mathfrak{F}' \subset \mathfrak{F}$ условное
  математическое ожидание $\Mean{\xi \mcond \mathfrak{F}'}$ существует и
  единственно с точностью до равенства почти наверное.
  \cite[стр.~142]{BorovkovMS}
\end{theorem}

\subsection{Свойства условного математического ожидания}
\index{условное!математическое ожидание!свойства}
\index{математическое ожидание!условное!свойства}
\footnote{Также
со свойствами и их доказательствами можно ознакомиться в книгах
Ширяева \cite[стр.~270]{Shiryayev1} и Боровкова \cite[стр.~143]{BorovkovMS}
}

\begin{enumerate}[label= \Roman*]
  \item\label{conditionalExpectationProperty:totalProbability}
      Формула полной вероятности \cite[стр.~144]{BorovkovMS}
      $$\mean{\Mean{ \eta \mcond \mathfrak{F}_1 }} = \mean{\eta}$$
  \item Условное математическое ожидание неотрицательной случайной величины
      неотрицательно почти наверное
      $$\eta \ge 0
      \Rightarrow \Mean{ \eta \mcond \mathfrak{F}_1 } \ge 0$$
  \item\label{conditionalExpectationProperty:Jensen}
      Неравенство Йенсена. Если функция $\varphi$ выпуклая вниз, то
      $$\varphi\left( \Mean{ \eta \mcond \mathfrak{F}_1 } \right)
      \le \Mean{\varphi\left( \eta \right) \mcond \mathfrak{F}_1}$$
  \item Телескопическое свойство
      $$\mathfrak{F}_2 \subset \mathfrak{F}_1 \Rightarrow
      \Mean{ \Mean{\eta \mcond \mathfrak{F}_1} \mcond \mathfrak{F}_2}
      = \Mean{ \eta \mcond \mathfrak{F}_2 }$$
  \item\label{conditionalExpectationProperty:measurableRandomVariable}
      Если случайная величина $\eta$ измерима
      относительно $\sigma$-алгебры $\mathfrak{F}_1$,
      то её условное математическое ожидание равно ей самой
      $$\Mean{ \eta \mcond \mathfrak{F}_1 } = \eta$$
  \item\label{conditionalExpectationProperty:measurableProduct}
      Если случайная величина $\eta$ измерима
      относительно $\mathfrak{F}_1$, то для любой случайной величины $\xi$
      $$\Mean{ \eta \cdot \xi \mcond \mathfrak{F}_1 }
      = \eta \cdot \Mean{ \xi \mcond \mathfrak{F}_1 }$$
  \item\label{conditionalExpectationProperty:independence}
      Если $\eta$ не зависит от $\mathfrak{F}_1$,
      то её условное математическое ожидание
      равно простому математическому ожиданию
      \begin{equation*}
        \Mean{ \eta \mcond \mathfrak{F}_1 } = \mean{\eta}
      \end{equation*}
  \item\label{conditionalExpectationProperty:linearity}
      Условное математическое ожидание линейно
      $$\forall a, b \in \mathbb{R}:
      \Mean{a \cdot \xi + b \cdot \eta \mcond \mathfrak{F}_1}
      = \Mean{a \cdot \xi \mcond \mathfrak{F}_1}
    + \Mean{b \cdot \eta \mcond \mathfrak{F}_1}$$
  \item Сохраняется теорема Лебега о возможности предельного перехода
      под знаком условного математического ожидания
      \cite[стр.~302]{KolmogorovFA} \cite[стр.~272]{Shiryayev1}
      $$\left|\xi_n\right| \le \eta,
      \;\mean{\eta} < \infty,
      \;\xi_n \acovergence \xi
      \Rightarrow
      \Mean{\xi_n \mcond \mathfrak{F}_1}
      \acovergence \Mean{\xi \mcond \mathfrak{F}_1}$$
\end{enumerate}

Пара полезных частных случая неравенства Йенсена
(\ref{conditionalExpectationProperty:Jensen} свойство)
$$\begin{array}{crcl}
  \varphi\left( x \right) = \left|x\right|:&
      \left| \Mean{\eta \mcond \mathfrak{F}_1} \right|
      &\le& \Mean{\left| \eta \right| \mcond \mathfrak{F}_1} \\
  \varphi\left( x \right) = x^2:&
      \left( \Mean{\eta \mcond \mathfrak{F}_1} \right)^2
      &\le& \Mean{\eta^2 \mcond \mathfrak{F}_1}
\end{array}$$

\subsection{Условное математическое ожидание по функции
  от случайного вектора}\label{conditionalExpectationSubsection}

Чем вызвала интерес эта тема?
Допустим, у нас есть \xsample --- выборка с функцией правдоподобия $L$
$$L\left( \vec{x}, \theta \right) = \prod_{k=1}^n \pdf{x_k, \theta}$$

Также есть $\hat{\theta}$ --- несмещённая оценка параметра $\theta$.
Как улучшить $\hat{\theta}$?
Возьмём статистику $T = T\left( \vec{x} \right)$,
обладающую определёнными свойствами.
Тогда улучшенной оценкой $\theta$ будет условное математическое ожидание
$\Mean{ \hat{\theta} \mcond T }$.

О свойствах, которыми должна обладать статистика $T$, поговорим позже.
Одно ясно уже сейчас: $T$ является функцией от выборки $\vec{x}$,
как и оценка $\hat{\theta}$.
Это значит, что нам не нужно погружаться в слишком абстрактные размышления,
а достаточно выяснить, как считать математическое ожидание
одной функции выборки (случайного вектора) $f\left( \vec{x} \right)$
при условии другой функции $g\left( \vec{x} \right)$
\textbf{той же} выборки $\vec{x}$.
$$f,g: \mathbb{R}^n \rightarrow \mathbb{R}$$

Вспомним, что для поиска условного математического ожидания
мы находили функцию
$\varphi^{\eta}\left( x \right) = \Mean{ \eta \mcond \xi = x }$.
Сейчас изменилось совсем немного --- лишь обозначения:
вместо $\eta$ у нас $f\left( \vec{x} \right)$,
а вместо $\xi$ тут $g\left( \vec{x} \right)$.
Значит, нужно найти вид такого условного математического ожидания
$$\Mean{f\left( \vec{x} \right) \mcond g\left( \vec{x} \right) = t} = ?$$

Для начала нужно понять, что из себя представляет множество точек
$S_t = \left\{ \vec{u} \mcond g\left( \vec{u} \right) = t \right\}$.
Функция $g$ описывает скалярное поле в $n$-мерном пространстве.
Для скалярного поля множество $S_t$
имеет своё название --- поверхность уровня (изоповерхность) --- то есть
поверхность, на которой функция принимает одно и то же значение.

Для понимания ситуации рассмотрим несколько примеров.

\begin{example}
  Пусть
  \begin{equation*}
    n=2,\qquad g: \mathbb{R}^2 \rightarrow \mathbb{R}
  \end{equation*}
  Функция $g$ имеет вид
  $$g\left( x, y \right) = x$$
  Очевидно, что поверхности уровней --- вертикальные линии
  (параллельные оси ординат) вида

  \begin{equation*}
    S_t = \left\{ \left( x, y \right) \mcond x = t \right\}
  \end{equation*}
\end{example}

\begin{example}
  $$n=2, g: \mathbb{R}^2 \rightarrow \mathbb{R}$$
  В этот раз функция $g\left( x, y \right)$ будет квадратом расстояния
  от начала координат $\left( 0, 0 \right)$ до точки $\left( x, y \right)$
  $$g\left( x, y \right) = x^2 + y^2$$

  Поверхностями уровня будут окружности радиуса $\sqrt{t}$
  \begin{equation*}
    S_t
      = \left\{ \left( x, y \right) \mcond x^2 + y^2 = t \right\}
      = \left\{ \left( x, y \right) \mcond x^2 + y^2 = t \right\}
  \end{equation*}
\end{example}

\begin{example}
  Рассмотрим $n$-мерное пространство ($n \ge 1$).
  Возьмём единичный вектор $\vec{e}: \left\| \vec{e} \right\| = 1$.
  Функция $g$ будет скалярным произведением аргумента $\vec{u}$
  с вектором $\vec{e}$
      $$g\left( \vec{u} \right) = \left( \vec{u}, \vec{e} \right)$$
  В таком случае поверхностью уровня $S_t$ будет гиперплоскость,
  проходящая через точку $t \cdot \vec{e}$, с нормалью $\vec{e}$,
  которую описывает следующее уравнение
  \begin{comment}\footnote{
      Уравнение гиперплоскости с нормалью $\vec{e}$,
      проходящую через точку с радиус-вектором $\vec{x}$,
      выглядит следующим образом
      $$\left( \vec{u}, \vec{e} \right) = \left( \vec{x}, \vec{e} \right)$$

      Вследствие линейности скалярного произведения получаем
      $$\left( t \cdot \vec{e}, \vec{e} \right)
      = t \cdot \left( \vec{e}, \vec{e} \right) = t$$

      Значит, указав $\vec{x} = t \cdot \vec{e}$, получим уравнение,
      данное в примере
      $$\left( \vec{u}, \vec{e} \right)
      = \left( \vec{x}, \vec{e} \right)
      = \left( t \cdot \vec{e}, \vec{e} \right)
      = t \cdot \left( \vec{e}, \vec{e} \right) = t$$}
      \end{comment}
  $$S_t = \left\{ \vec{u} \mcond \left( \vec{u}, \vec{e} \right) = t \right\}$$
\end{example}

Опираясь на предыдущий опыт (для величин с совместной плотностью),
хотелось бы найти совместную плотность случайных величин
$f\left( \vec{x} \right)$ и $g\left( \vec{x} \right)$.
\begin{comment}
И оказывается, что это желание является верной догадкой.

Нетрудно догадаться, что для того, чтобы найти ``вес'' поверхности уровня,
нужно будет взять поверхностный интеграл от плотности.

Чтобы мы не получали нулевой вес поверхности
$S_t = \left\{ g\left( \vec{x} \right) = t \right\}$,
будем считать объём её раздутия.
Поместим поверхность в своеобразный кокон,
толщина которого в каждой точке будет тем меньше,
чем больше скорость перехода в этой точке от текущего уровня к следующему.

Чтобы значение $t$ было близким к $g\left( \vec{u} \right)$,
нужно, чтобы точка $\vec{u}$ была близка к поверхности $S_t$.
Обозначим расстояние между $t$ и $g\left( \vec{u} \right)$
как $\tilde{\varepsilon}$,
а расстояние между точкой $\vec{u}$ и поверхностью $S_t$
как $\varepsilon$. Чему равны эти расстояния, будет выяснено ниже,
а значение $t$ и точки $\vec{u}$ будет ясно из контекста.
\end{comment}

Попробуем найти плотность распределения $g\left( \vec{x} \right)$.

Вероятность того, что значение $g\left( \vec{x} \right)$
отдалено от $t$ не больше, чем на $\tilde{\varepsilon}$,
будет приблизительно равна плотности распределения $g\left( \vec{x} \right)$
в этой точке (если таковая имеется),
умноженной на это расстояние.
Обозначим плотность случайной величины $g\left( \vec{x} \right)$
в точке $t$ через $q\left( t \right)$. Тогда
$$\Probability{g\left( \vec{x} \right)
  \in \left[t-\tilde{\varepsilon}, t+ \tilde{\varepsilon}\right]}
  \approx q\left( t \right) \cdot 2 \cdot \tilde{\varepsilon}$$

\begin{comment}
Вернёмся к раздутию. Помним, что $\varepsilon$ --- расстояние от
точки $\vec{u}$ до ближайшей к нему точки кокона, а также то,
что это расстояние должно быть обратно пропорционально
стремительности изменения уровней в этой окрестности.
Понимаем, что нам необходима численная мера этой скорости.
Под описание такой величины прекрасно подходит модуль градиента.
Поскольку значение $g\left( \vec{x} \right)$ не меняется вдоль поверхности $S_t$
и равно $t$, то градиент будет направлен по нормали к данной точке поверхности.

Норма градиента --- отношение прироста функции к приросту координат.
Нас интересует прирост координат $\varepsilon$ в окрестности точки $\vec{u}$,
мы располагаем приростом функции $\tilde{\varepsilon}$
и нормой градиента
$\left\| \vec{\nabla} \cdot {g\left( \vec{u} \right)} \right\|$.
Напрашивается формула
\begin{equation}\label{widthEpsilon}
  \varepsilon \approx \frac{\tilde{\varepsilon}}
      {\left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|}
\end{equation}

Обозначим раздутие поверхности $S_t$ как $G_{\tilde{\varepsilon}}$,
тогда вероятность попадания значения $g\left( \vec{x} \right)$ в коридорчик
ширины $\tilde{\varepsilon}$ будет приблизительно равно интегралу
плотности распределения вектора $\vec{x}$ по этому коридорчику

$$\Probability{g\left( \vec{x} \right)
  \in \left[t-\tilde{\varepsilon}, t+ \tilde{\varepsilon}\right]}
  \approx q\left( t \right) \cdot 2 \cdot \tilde{\varepsilon}
  \approx \integrall{G_{\tilde{\varepsilon}}}{d\vec{u}}{\pdf{\vec{u}}}$$
\end{comment}

Множество таких точек $\vec{u} \in \mathbb{R}^n$, что
$\left| g\left( \vec{u} \right) - t \right| \le \tilde{\varepsilon}$
можно считать приблизительно равным множеству точек, удовлетворяющих
соотношению
\begin{equation}\label{widthEpsilon}
  \rho\left( \vec{u}, S'_t \right) \le \varepsilon
  = \frac{\tilde{\varepsilon}}
    {\left\| \vec{\nabla} \cdot g\left( \vec{u}_0 \right) \right\|},
\end{equation}
где $\vec{u}_0$ --- ближайшая к $\vec{u}$ точка поверхности $S_t$.
Обозначим это можество точек через $G_{\varepsilon}$.
Окончательно
\begin{equation*}
  q\left( t \right) \cdot 2 \cdot \tilde{\varepsilon}
  \approx \integrall{G_{\tilde{\varepsilon}}}{d\vec{u}}{\pdf{\vec{u}}}
\end{equation*}

Следовательно, у нас почти готова формула для плотности $q\left( t \right)$
случайной величины $g\left( \vec{x} \right)$
$$q\left( t \right) \cdot 2 \cdot \tilde{\varepsilon}
      \approx \integrall{G_{\tilde{\varepsilon}}}{d\vec{u}}{\pdf{\vec{u}}}
      \Rightarrow q\left( t \right)
      \approx \frac{1}{2 \cdot \tilde{\varepsilon}}
      \cdot \integrall{G_{\tilde{\varepsilon}}}{d\vec{u}}{
    \pdf{\vec{u}}}$$

Чтобы убрать неточность и было обычное равенство,
устремим ширину коридорчика к нулю
\begin{equation}\label{isosurfaceDencity}
  q\left( t \right)
      = \lim_{\tilde{\varepsilon} \to 0} \frac{1}{2 \cdot \tilde{\varepsilon}}
      \cdot \integrall{G_{\tilde{\varepsilon}}}{d\vec{u}}{\pdf{\vec{u}}}
\end{equation}

\begin{comment}
Распишем $\tilde{\varepsilon}$, воспользовавшись формулой \eqref{widthEpsilon}
$$\varepsilon
  \approx \frac{\tilde{\varepsilon}}
      {\left\| \vec{\nabla} \cdot {g\left( \vec{u} \right)} \right\|}
  \Rightarrow
      \tilde{\varepsilon} \approx \varepsilon
      \cdot \left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|$$

Вернёмся к плотности в формуле \eqref{isosurfaceDencity}.
Заменив $\tilde{\varepsilon}$ на $\varepsilon
\cdot \left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|$,
нужно разобраться, что теперь нужно устремлять к нулю.
Поскольку модуль градиента --- величина, зависящая от координат,
и стремиться к нулю будет лишь при изменении поведения функции,
то устремлять будем $\varepsilon$ (толщину кокона)
\end{comment}
$$q\left( t \right)
      = \lim_{\tilde{\varepsilon} \to 0} \frac{1}{2 \cdot \tilde{\varepsilon}}
      \cdot \integrall{G_{\tilde{\varepsilon}}}{d\vec{u}}{\pdf{\vec{u}}}
      = \lim_{\varepsilon \to 0} \frac{1}{2 \cdot \varepsilon
      \cdot \left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|}
      \cdot \integrall{G_{\tilde{\varepsilon}}}{d\vec{u}}{\pdf{\vec{u}}}$$

Поскольку $\varepsilon$ играет роль половины толщины
(отсюда и возникает множитель двойка),
а $d\vec{u}$ --- маленький элемент объёма,
то при делении объёма на толщину получим площадь.
Поскольку толщина стремится к нулю,
то она становится соразмерна с объёмом и мы получаем ненулевое значение площади,
а коридорчик $G_{\tilde{\varepsilon}}$ вырождается в поверхность уровня $S_t$.
Обозначив меру площади на поверхности $S_t$ как
$\sigma_{t}\left( d\vec{u} \right)$,
получаем поверхностный интеграл первого рода

\begin{equation}\label{formula:conditionDencity}
  q\left( t \right)
  = \integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{
      \pdf{\vec{u}} \cdot \frac{1}
      {\left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|}}
\end{equation}

А теперь вспомним, как обстояло дело со случайными величинами,
имеющими совместную плотность \eqref{phiIntegral}
$$\varphi^{\eta}\left( x \right)
  = \frac{\integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y}}}
      {r\left( x \right)}
  = \frac{\integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y}}}
      {\integral{\mathbb{R}}{}{y}{\pdf{x,y}}}$$

В знаменателе у нас стоял вес поверхности уровня,
раскрыв который, мы получали интеграл от совместной плотности.
Что же есть у нас?
Вес поверхности уровня --- функция одного аргумента $q\left( t \right)$,
которая равна интегралу от плотности по всей той части пространства,
где случайная величина $g\left( \vec{x} \right)$
принимает одно и то же значение $t$ --- по поверхности уровня $S_t$.

То есть в нашем случае роль $\mathbb{R}$ играет поверхность $S_t$,
роль совместной плотности $\pdf{x,y}$ играет плотность случайного вектора
$\pdf{\vec{u}}$, а вместо дифференциала $dy$ у нас мера площади,
делённая на норму градиента $\frac{\sigma_{t}\left( d\vec{u} \right)}
{\left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|}$.
В числителе дроби в формуле \eqref{phiIntegral} стоит $y$,
в нашем же случае это функция $f\left( \vec{u} \right)$,
поскольку там случайная величина присутствовала в плотности сама по себе,
тут же у нас есть плотность случайного вектора $\pdf{\vec{x}}$,
а найти нужно среднее функции случайной величины $f\left( \vec{x} \right)$.
Итого, получается переход

$$\frac{\integral{\mathbb{R}}{}{y}{y \cdot \pdf{x,y}}}{r\left( x \right)}
  \rightarrow
      \frac{\integrall{S_t}{\frac{\sigma_{t}\left( d\vec{u} \right)}{
      \left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|}}{
      f\left( \vec{u} \right) \cdot \pdf{\vec{u}}}}{q\left( t \right)}$$

И конечная формула

$$\Mean{ f\left( \vec{x} \right) \mcond g\left( \vec{x} \right) = t }
  = \frac{\integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{
      f\left( \vec{u} \right) \cdot \pdf{\vec{u}} \cdot \frac{1}{
      \left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|}}}
      {\integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{
      \pdf{\vec{u}} \cdot \frac{1}{
      \left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|}}}$$

\begin{theorem}[Условное математическое ожидание гладких функций]
  \index{теорема!условное математическое ожидание гладких функций}
  \index{условное!математическое ожидание!гладких функций}
  \index{математическое ожидание!условное!гладких функций}
  \label{conditionalExpectationDefinition}
  Если есть случайный вектор $\vec{x}$ с известной плотностью распределения
  $\pdf{\vec{x}}$, а также гладкая функция $g\left( \vec{x} \right)$
  с невырожденным градиентом, то математическое ожидание случайной величины
  $f\left( \vec{x} \right)$ при условии $g\left( \vec{x} \right) = t$
  считается по формуле
  $$\Mean{f\left( \vec{x} \right) \mcond g\left( \vec{x} \right) = t}
      = \frac{\integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{
      f\left( \vec{u} \right) \cdot \pdf{\vec{u}} \cdot \frac{1}{
      \left\| \vec{\nabla} \cdot g\left( \vec{u} \right) \right\|}}}
      {\integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{
      \pdf{\vec{u}} \cdot \frac{1}{
    \left\| \vec{\nabla}
        \cdot g\left( \vec{u} \right) \right\|}}}$$
\end{theorem}

\begin{remark}
  Формула остаётся справедливой,
  если поверхности уровня функции $g$ состоят из нескольких гладких кусков.
\end{remark}

\subsection{Пример}
Поскольку этот пример оказался достаточно громоздким,
было решено посвятить ему целый подраздел.

Есть выборка \xsample из равномерного распределения
с на отрезке $\left[ 0, \theta \right]$.
Нужно посчитать условное математическое ожидание
первого элемента выборки $f\left( \vec{x} \right) = x_1$
при условии максимального $g\left( \vec{x} \right) = x_{\left( n \right)}$

$$\Mean{x_1 \mcond x_{\left( n \right)}} = ?$$

\subsubsection{Поверхности уровня}
Для начала сообразим, что из себя представляет поверхность уровня
$S_t$
$$S_t = \left\{ \vec{u} \mcond
  u_i \ge 0,\; \max_{i= \overline{1,n}} u_i = t \right\}$$

На двумерной плоскости это будет два отрезка,
перпендикулярных друг другу и осям.
Они будут выходить из точки $\left( t, t \right)$
и заканчиваться на осях.

Рассмотрим, почему всё именно так
\begin{enumerate}
  \item Нас не устраивают точки, которые находятся за пределами
      квадрата, ограниченного прямыми, проходящими через точки
      $\left( 0, t \right)$ и $\left( t, 0 \right)$,
      так как в таком случае максимум будет больше $t$,
      что по условию быть не может
  \item Максимальное значение зафиксировано, а это значит,
      что хотя бы одна координата должна
      всегда равняться максимуму
\end{enumerate}

Таким образом, закрасив квадрат, нижняя левая грань которого
находится в начале координат, а верхняя правая в точке $\left( t, t \right)$,
удаляем все те точки, где нет такой координаты, которая равна $t$.
Все точки внутри контура пропадут, так как лишь на контуре квадрата
могут находиться точки, имеющие хоть одну координату равной $t$.
Теперь осталось отбросить те отрезки, что лежат на координатных осях,
потому что на них у точек меняется лишь одна координата
(например, на оси ординат меняется лишь $x$, а $y=0$ на всей оси),
а желаемое значение $t$ принимается лишь на самом конце отрезка,
но это лишь одна точка, и в пространстве размерностью $n \ge 1$
имеет меру Лебега $0$, поэтому её тоже можно отбросить.

Так уж получилось, что только что был выведен
практически универсальный способ построения поверхности уровня
$g\left( \vec{u} \right) = \max_{i= \overline{1,n}} u_i = t$.
Осталось лишь ввести небольшие правки и распространить его на
пространство размерности $n$.

Например, в трёхмерном пространстве поверхностью уровня будет
совокупность граней куба, где исключены те грани,
что соприкасаются с одной из осей --- те грани, одна точка которых
находится в начале координат $\left( 0, 0 \right)$.

Так же будет и в многомерном пространстве --- чертим гиперкуб
и отбрасываем те его грани, один из углов которых находится в начале координат.

Также отметим, что каждая грань перпендикулярна $n-1$ осям,
а пересекается с ними лишь в точках со значением $t$.
Так как нельзя провести две разные гиперплоскости, имеющих одну нормаль
и проходящих через одну точку (гиперплоскость по определению
определяется этой парой), то делаем вывод:
поверхность $S_t$ у нас состоит из $n$ граней.

\subsubsection{Норма градиента}
Найдём норму градиента функции
$g\left( \vec{x} \right) = \max_{i= \overline{1,n}} x_i$.

Возможны два случая:
\begin{enumerate}
  \item Максимальный элемент в векторе один
  \item Максимальных элементов в векторе несколько
      (от двух до $n$) и они равны друг другу (если $n \ge 2$)
\end{enumerate}

Рассмотрим первый случай. Без потери общности предположим,
что максимальный элемент --- первый
$$\max_{i= \overline{1,n}} x_i = x_1$$

Очевидно, что очень малое изменение $x_1$ приведёт к
очень малому (причём такому же) изменению максимального элемента выборки.
Грубо говоря, если у нас есть числа $x_1 = 3, x_2 = 2, x_3 = 1$,
то максимальное из них --- $x_1$. Если оно изменится на $1$
в какую-либо сторону, то максимальное значение выборки изменится
так же на $1$.

Мы это всё рассматриваем для того, чтобы показать,
что производная по максимальной координате будет равна единице,
так как производная --- отношение очень малого прироста функции
к очень малому изменению аргумента (который привёл к такому изменению функции).
Даже если у нас были числа $x_1 = 1$ и $x_2 = 1.001$,
мы всё равно сможем найти такой маленький прирост $\delta < 0.001$
(между двумя разными действительными числами найдётся ещё континуальное число
действительных чисел, поэтому такое $\delta$ найдётся всегда),
при котором $x_2$ останется максимальным элементом и прирост градиента
по этой координате будет равен приросту самой координаты.

Не забываем, что мы сейчас рассматриваем тот случай,
когда максимальное значение одно, а это значит,
что остальные значения выборки строго меньше максимального.

Поскольку очень малые изменения других элементов выборки не приведут к тому,
что они станут максимальными (а если приведут, то возьмём ещё более
маленький прирост), то их изменение не повлияет на значение функции $g$,
а это значит, что частные производные по ним обращаются в нули.

Итого, к чему мы пришли.
Когда $x_1$ является максимальным элементом выборки,
то градиент функции $g$ в точке $\vec{x}$ равен
$$\vec{\nabla} \cdot g\left( \vec{x} \right)
  = \frac{\partial g}{\partial x_1} \cdot \vec{e_1}
      + \frac{\partial g}{\partial x_2} \cdot \vec{e_2}
      + \cdots
      + \frac{\partial g}{\partial x_n} \cdot \vec{e_n}
  = \vec{e_1} + 0 + \cdots + 0 = \vec{e_1}$$

Когда у нас не $x_1$ является максимальным элементом, а $x_k$,
где $1 \le k \le n$, то очевидно, что результат будет следующим:
$$\vec{\nabla} \cdot g\left( \vec{x} \right) = \vec{e_k}$$

Нас интересует лишь норма градиента, которая в данном случае равна единице
$$\left\| \vec{\nabla} \cdot g\left( \vec{x} \right) \right\|
  = \left\| \vec{e_k} \right\| = 1$$

Второй случай (когда несколько элементов максимальны и равны между собой)
нас не интересует, так как такое возможно лишь на рёбрах ($n \ge 3$) и в точках
пересечения рёбер ($n \ge 2$).
Что те, что другие (прямые и точки в пространстве) имеют нулевой объём.
В одномерном пространстве (с одной осью) у нас есть лишь одна
случайная величина, и она же является максимальной.

\subsubsection{Условное математическое ожидание относительно события}
Посчитаем условное математическое ожидание по формуле
\begin{align*}
  \Mean{x_1 \mcond x_{\left( n \right)} = t}
      = \frac{\integrall{S_t}{
    \sigma_{t}\left(d\vec{u} \right)}{
    u_1 \cdot \theta^{-n} \cdot \frac{1}{1}}}
      {\integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{
    \theta^{-n} \cdot \frac{1}{1}}}
      = \frac{\integrall{S_t}{
    \sigma_{t}\left(d\vec{u} \right)}{u_1}}{
      \integrall{S_t}{
    \sigma_{t}\left(d\vec{u} \right)}{}}
\end{align*}

Поверхность уровня $S_t$ в верхнем интеграле можно (и нужно!)
разбить на две части: ту, где $u_1$ меняется, и ту, где $u_1$
принимает постоянное значение.

Вспоминаем, что поверхность $S_t$ --- часть гиперкуба,
содержащая лишь грани, на которых которых та случайная величина,
что не меняется, имеет значение $t$ (максимум).
Частью поверхности, на которой $u_1$ принимает постоянное значение $t$,
будет одна грань --- та грань гиперкуба, что проходит через точку $u_1 = t$
и перпендикулярна оси (вектору $\vec{e_1}$).

Обозначим грань, где $u_1 = t$, как $U_t$,
а оставшуюся часть поверхности как $Y_t = S_t \setminus U_t$.

Имеем более конкретную формулу для подсчёта условного математического ожидания
\begin{align*}
  \Mean{x_1 \mcond x_{\left( n \right)} = t}
  = \frac{\integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{u_1}}{
      \integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{}}
  = \frac{\integrall{U_t}{\sigma_{t}\left(d\vec{u} \right)}{
      \Bigl. u_1 \Bigr|_{u_1 = t}}
      + \integrall{Y_t}{\sigma_{t}\left(d\vec{u} \right)}{
      u_1}}{
      \integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{}}
\end{align*}

Вспоминаем, что имеется $n$ граней, а это значит, что в знаменателе
у нас $n$ объёмов $\left( n-1 \right)$-мерных гиперкубов
($n$ площадей $n$-мерных квадратов)
со стороной $t$, которые в свою очередь равняются числу $t^{n-1}$.

В числителе у нас два интеграла.
Первый интеграл --- интеграл по одной грани, что опять же является
$\left( n-1 \right)$-мерным объёмом ($n$-мерной площадью).
Не забываем про константу $t$, что там находится: она умножается
на результат интегрирования $t^{n-1}$ и в результате получаем $t^n$.

Теперь подошли к самому сложному кусочку этой дроби ---
интеграл по оставшимся $n-1$ граням.
Поскольку интеграл не зависит ни от чего кроме $u_1$, то мы преспокойнейше
можем вынести $\left( n-2 \right)$-мерный объём, а по оставшемуся измерению
придётся интегрировать от $0$ до $t$
$$\frac{\integrall{U_t}{\sigma_{t}\left(d\vec{u} \right)}{
      \Bigl. u_1 \Bigr|_{u_1 = t}}
      + \integrall{Y_t}{\sigma_{t}\left(d\vec{u} \right)}{
      u_1}}{
      \integrall{S_t}{\sigma_{t}\left(d\vec{u} \right)}{}}
  = \frac{t^n + \left( n-1 \right) \cdot t^{n-2}
      \cdot \integral{0}{t}{u_1}{u_1}}{n \cdot t^{n-1}}$$

Дальше идут нехитрые математические преобразования,
которые называются интегрированием
\begin{align*}
  \Mean{x_1 \mcond x_{\left( n \right)} = t}
  = \frac{t^n + \left( n-1 \right) \cdot t^{n-2} 
      \cdot \integral{0}{t}{u_1}{u_1}}{n \cdot t^{n-1}} = \\
  = \frac{t^n + \left( n-1 \right) \cdot t^{n-2}
      \cdot \frac{t^2}{2}}{n \cdot t^{n-1}}
  = \frac{t + \left( n-1 \right) \cdot t \cdot \frac{1}{2}}{n}
  = t \cdot \frac{n+1}{2 \cdot n}
\end{align*}

\subsubsection{Условное математическое ожидание}
Мы получили условное математическое ожидание относительно события
$\left\{ g\left( x \right) = t \right\}$ --- конкретное значение
$t \cdot \frac{n+1}{2 \cdot n}$.
Чтобы получить условное математическое ожидание одной случайной величины
относительно другой, нужно лишь подставить вместо $t$
наше значение $g\left( \vec{x} \right) = x_{\left( n \right)}$
$$\Mean{x_1 \mcond x_{\left( n \right)}}
  = \Bigl. \Mean{x_1 \mcond x_{\left( n \right)} = t}
      \Bigr|_{t = x_{\left( n \right)}}
  = \Bigl. t \cdot \frac{n+1}{2 \cdot n} \Bigr|_{t = x_{\left( n \right)}}
  = x_{\left( n \right)} \cdot \frac{n+1}{2 \cdot n}$$

Видим, что получили случайную величину,которая не зависит от параметра
$\theta$, а это значит, что мы на правильном пути
$$\Mean{x_1 \mcond x_{\left( n \right)}}
  = \frac{n+1}{2 \cdot n} \cdot x_{\left( n \right)}$$

\subsubsection{Проверка}
Проведём небольшую очевидную проверку: положим $n=1$ (одна случайная величина,
одномерное пространство). Тогда формула примет следующий вид
$$\Mean{x_1 \mcond x_{\left( 1 \right)}}
  = \frac{1+1}{2 \cdot 1} \cdot x_{\left( 1 \right)}
  = \frac{2}{2} \cdot x_{\left( 1 \right)}
  = x_{\left( 1 \right)} = x_1$$

Всё сходится с нашими интуитивными предположениями: у нас имеется лишь один
элемент в выборке, а мы знаем значение максимального. Значит, мы знаем
значение этого единственного элемента (иначе кому ещё быть максимальным
в этой выборке?).
