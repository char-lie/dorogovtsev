\chapter{Достаточные статистики}
\section{Оптимальная оценка}
\begin{definition}[Симметризация]\index{симметризация}
    Симметризация $\Lambda$ оценки $\hat{\theta}$ --- среднее
    оценок $\hat{\theta}$ для
    всевозможных перестановок $\sigma\in S_n$
    элементов выборки $x_1, x_2, \dots, x_n$
    $$\Lambda\hat{\theta}
        =\frac{1}{n!}\cdot\sum_{\sigma\in S_n} \hat{\theta}\left(
            x_{\sigma\left(1\right)}, x_{\sigma\left(2\right)},
                \dots, x_{\sigma\left(n\right)}\right)$$
\end{definition}
\begin{lemma}
    Для произвольной несмещённой оценки $\hat{\theta}$
    её симметризация $\Lambda{\hat{\theta}}$
    не хуже её самой в среднем квадратическом
    \begin{align*}
    \meanof{\theta}{\hat{\theta}}
        =\theta
    \Rightarrow
        \begin{cases}
            \meanof{\theta}{\Lambda{\hat{\theta}}}
                =\meanof{\theta}{\hat{\theta}}
                =\theta\\
            \dispersionof{\theta}{\Lambda{\hat{\theta}}}
                \le\dispersionof{\theta}{\hat{\theta}}
        \end{cases}
    \end{align*}
\end{lemma}
\begin{proof}
    Берём $x_1, x_2, \dots, x_n$ --- независимые одинаково распределённые
    случайные величины.

    Введём обозначения для более короткой записи
    используемых в доказательстве случайных векторов.

    Вектор, состоящий из элементов выборки в их изначальном порядке,
    обозначим привычным $\vec{x}$

    $$\left(x_1, x_2, \dots, x_n\right)=\vec{x}$$

    Вектор, состоящий из элементов, изменивших своё местоположение под влиянием
    перестановки $\sigma$ (значение которой будет ясно из контекста),
    будем обозначать через $\vec{x}_\sigma$

    $$\left(x_{\sigma\left(1\right)}, x_{\sigma\left(2\right)},
        \dots, x_{\sigma\left(n\right)}\right)=\vec{x}_\sigma$$

    Тогда и оценки примут более красивый вид
    \begin{align*}
        \hat{\theta}\left(x_1, x_2, \dots, x_n\right)
            &= \hat{\theta}\left(\vec{x}\right)\\
        \hat{\theta}\left(x_{\sigma\left(1\right)},
                x_{\sigma\left(2\right)},
                \dots, x_{\sigma\left(n\right)}\right)
            &= \hat{\theta}\left(\vec{x}_\sigma\right)
    \end{align*}

    Теперь приступим непосредственно к доказательству.
    \begin{enumerate}
        \item
            Начнём с первого пункта --- докажем несмещённость
            симметризации оценки $\hat{\theta}$.

            Нетрудно показать, что  вектора $\vec{x}$ и $\vec{x}_\sigma$
            имеют одинаковое распределение для любой перестановки $\sigma$,
            а это значит, что и оценки $\hat{\theta}\left(\vec{x}\right)$
            и $\hat{\theta}\left(\vec{x}_\sigma\right)$
            распределены одинаково как функции случайных
            одинаково распределённых векторов.
            Следовательно, их математические ожидания равны между собой
            при любой перестановке $\sigma$
            $$\meanof{\theta}{\hat{\theta}\left(\vec{x}\right)}
                =\meanof{\theta}{\hat{\theta}\left(\vec{x}_\sigma\right)}
                =\theta$$

            Посчитаем математическое ожидание симметризации оценки
            $\hat{\theta}$
            \begin{align*}
                \meanof{\theta}{\Lambda\hat{\theta}}
                    &=\meanof{\theta}{
                        \left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                        \hat{\theta}\left(\vec{x}_\sigma\right)\right\}}
            \end{align*}

            Помним, что математическое ожидание линейно и
            константы можно выносить за знак математического ожидания,
            а математическое ожидание суммы равно сумме математических ожиданий
            \begin{align*}
                \meanof{\theta}{\left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \hat{\theta}\left(\vec{x}_\sigma\right)\right\}}
                    =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                        \meanof{\theta}{\hat{\theta}\left(\vec{x}_\sigma\right)}
            \end{align*}

            Не забываем, что математическое ожидание оценки
            любого вектора $\vec{x}_\sigma$ одинаково и равно параметру $\theta$

            \begin{align*}
                \frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \meanof{\theta}{\hat{\theta}\left(\vec{x}_\sigma\right)}
                    =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}\theta
            \end{align*}

            Сумма имеет $n!$ слагаемых (количество перестановок $\sigma\in S_n$)
            \begin{align*}
                \frac{1}{n!}\cdot\sum_{\sigma\in S_n}\theta
                    =\frac{1}{n!}\cdot n!\cdot\theta
                    =\theta
            \end{align*}

            А это значит, что первый пункт доказан и симметризация
            несмещённой оценки $\hat{\theta}$ действительно несмещённая
            $$\meanof{\theta}{\Lambda\hat{\theta}}=\theta$$
        \item
            Теперь посмотрим, чему равна дисперсия симметризации
            оценки $\hat{\theta}$

            Воспользуемся определением
            \begin{align*}
                \dispersionof{\theta}{\Lambda\hat{\theta}}
                    =\meanof{\theta}{
                        \left(\Lambda\hat{\theta}-\theta\right)^2}
                    =\meanof{\theta}{
                        \left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                            \hat{\theta}\left(\vec{x}_\sigma\right)
                            -\theta\right\}^2}
            \end{align*}

            Внесём параметр $\theta$ в сумму.
            Для этого нужно умножить и поделить его на $n!$
            (так как сумма имеет $n!$ слагаемых)
            \begin{align*}
                \meanof{\theta}{\left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \hat{\theta}\left(\vec{x}_\sigma\right)-\theta\right\}^2}=\\
                    =\meanof{\theta}{
                        \left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                            \hat{\theta}\left(\vec{x}_\sigma\right)
                            -\frac{1}{n!}\cdot n!\cdot\theta\right\}^2}=\\
                    =\meanof{\theta}{
                            \left\{\frac{1}{n!}\cdot\left(\sum_{\sigma\in S_n}
                            \hat{\theta}\left(\vec{x}_\sigma\right)
                            -n!\cdot\theta\right)\right\}^2}=\\
                    =\meanof{\theta}{
                        \left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                            \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                            -\theta\right)\right\}^2}=\\
                    =\meanof{\theta}{
                        \left\{\sum_{\sigma\in S_n}\frac{1}{n!}\cdot
                            \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                            -\theta\right)\right\}^2}
            \end{align*}

            Вспомним неравенство Йенсена для выпуклой функции $f$
            $$f\left(\sum_{i=1}^n q_i\cdot x_i\right)
                \le \sum_{i=1}^n q_i\cdot f\left(x_i\right),
                    \qquad\sum_{i=1}^n q_i=1$$

            В нашем случае
            $x_i=\left(\hat{\theta}
                \left(\vec{x}_{\sigma_i}\right)-\theta\right)$,
            функция $f\left(x\right)=x^2$,
            сумма проходит по всевозможным перестановкам $\sigma$,
            а роль $q_i$ выполняет $\frac{1}{n!}$,
            так как
                $$\sum_{\sigma\in S_n} q_i
                    =\sum_{\sigma\in S_n}\frac{1}{n!}=n!\cdot\frac{1}{n!}=1$$

            Перепишем неравенство Йенсена для нашего случая
            \begin{equation}\label{eq:jensen_symmetrization}
                \meanof{\theta}{\left\{\sum_{\sigma\in S_n}\frac{1}{n!}\cdot
                    \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                    -\theta\right)\right\}^2}
                    \le\meanof{\theta}{\sum_{\sigma\in S_n}\frac{1}{n!}\cdot
                        \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                        -\theta\right)^2}
            \end{equation}

            Воспользуемся линейностью математического ожидания,
            внеся его под знак суммы
            \begin{align*}
                \meanof{\theta}{\sum_{\sigma\in S_n}\frac{1}{n!}\cdot
                    \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                    -\theta\right)^2}
                =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \meanof{\theta}{
                        \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                        -\theta\right)^2}
            \end{align*}

            Видим сумму дисперсий.
            Дисперсии одинаковы, так как оценки имеют одинаковые распределения
            \begin{align*}
                \frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \meanof{\theta}{
                        \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                        -\theta\right)^2}
                =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \dispersionof{\theta}{
                        \hat{\theta}\left(\vec{x}_\sigma\right)}=\\
                =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \dispersionof{\theta}{\hat{\theta}\left(\vec{x}\right)}
                =\frac{1}{n!}\cdot n!
                    \cdot\dispersionof{\theta}{\hat{\theta}\left(\vec{x}\right)}
                =\dispersionof{\theta}{\hat{\theta}\left(\vec{x}\right)}
            \end{align*}

            Из неравенства Йенсена \eqref{eq:jensen_symmetrization} видим,
            что дисперсия симметризации не хуже дисперсии самой оценки
            $$\dispersionof{\theta}{\Lambda\hat{\theta}}
                \le\dispersionof{\theta}{\hat{\theta}\left({\vec{x}}\right)}$$

    \end{enumerate}

    То есть, симметризация не ухудшает оценку,
    а в общем случае (когда неравенство строгое) даже делает её лучше.
\end{proof}

\begin{remark}
    Равенство в неравенстве Йенсена (в доказательстве выше)
    возможно только в случае симметричной функции.
    Значит,
    в качестве оценки достаточно брать только симметричные функции выборки
\end{remark}
\begin{definition}[Функция вариационного ряда]\index{функция!вариационного ряда}
    Если оценка $\hat{\theta}$ симметрична относительно перестановок аргументов,
    то она является функцией вариационного ряда
\end{definition}
\begin{remark}
    Все оценки, которые претендуют быть оптимальными,
    должны быть функциями вариационного ряда
\end{remark}
\section{Условное математическое ожидание и условные распределения}
\subsection{$\sigma$-алгебра, порождённая случайной величиной}

