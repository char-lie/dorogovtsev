\chapter{Достаточные статистики}
\section{Оптимальная оценка}
\begin{definition}[Симметризация]\index{симметризация}
    Симметризация $\Lambda$ оценки $\hat{\theta}$ --- среднее
    оценок $\hat{\theta}$ для
    всевозможных перестановок $\sigma\in S_n$
    элементов выборки $x_1, x_2, \dots, x_n$
    $$\Lambda\hat{\theta}
        =\frac{1}{n!}\cdot\sum_{\sigma\in S_n} \hat{\theta}\left(
            x_{\sigma\left(1\right)}, x_{\sigma\left(2\right)},
                \dots, x_{\sigma\left(n\right)}\right)$$
\end{definition}
\begin{lemma}
    Для произвольной несмещённой оценки $\hat{\theta}$
    её симметризация $\Lambda{\hat{\theta}}$
    не хуже её самой в среднем квадратическом
    \begin{align*}
    \meanof{\theta}{\hat{\theta}}
        =\theta
    \Rightarrow
        \begin{cases}
            \meanof{\theta}{\Lambda{\hat{\theta}}}
                =\meanof{\theta}{\hat{\theta}}
                =\theta\\
            \dispersionof{\theta}{\Lambda{\hat{\theta}}}
                \le\dispersionof{\theta}{\hat{\theta}}
        \end{cases}
    \end{align*}
\end{lemma}
\begin{proof}
    Берём $x_1, x_2, \dots, x_n$ --- независимые одинаково распределённые
    случайные величины.

    Введём обозначения для более короткой записи
    используемых в доказательстве случайных векторов.

    Вектор, состоящий из элементов выборки в их изначальном порядке,
    обозначим привычным $\vec{x}$

    $$\left(x_1, x_2, \dots, x_n\right)=\vec{x}$$

    Вектор, состоящий из элементов, изменивших своё местоположение под влиянием
    перестановки $\sigma$ (значение которой будет ясно из контекста),
    будем обозначать через $\vec{x}_\sigma$

    $$\left(x_{\sigma\left(1\right)}, x_{\sigma\left(2\right)},
        \dots, x_{\sigma\left(n\right)}\right)=\vec{x}_\sigma$$

    Тогда и оценки примут более красивый вид
    \begin{align*}
        \hat{\theta}\left(x_1, x_2, \dots, x_n\right)
            &= \hat{\theta}\left(\vec{x}\right)\\
        \hat{\theta}\left(x_{\sigma\left(1\right)},
                x_{\sigma\left(2\right)},
                \dots, x_{\sigma\left(n\right)}\right)
            &= \hat{\theta}\left(\vec{x}_\sigma\right)
    \end{align*}

    Теперь приступим непосредственно к доказательству.
    \begin{enumerate}
        \item
            Начнём с первого пункта --- докажем несмещённость
            симметризации оценки $\hat{\theta}$.

            Нетрудно показать, что  вектора $\vec{x}$ и $\vec{x}_\sigma$
            имеют одинаковое распределение для любой перестановки $\sigma$,
            а это значит, что и оценки $\hat{\theta}\left(\vec{x}\right)$
            и $\hat{\theta}\left(\vec{x}_\sigma\right)$
            распределены одинаково как функции случайных
            одинаково распределённых векторов.
            Следовательно, их математические ожидания равны между собой
            при любой перестановке $\sigma$
            $$\meanof{\theta}{\hat{\theta}\left(\vec{x}\right)}
                =\meanof{\theta}{\hat{\theta}\left(\vec{x}_\sigma\right)}
                =\theta$$

            Посчитаем математическое ожидание симметризации оценки
            $\hat{\theta}$
            \begin{align*}
                \meanof{\theta}{\Lambda\hat{\theta}}
                    &=\meanof{\theta}{
                        \left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                        \hat{\theta}\left(\vec{x}_\sigma\right)\right\}}
            \end{align*}

            Помним, что математическое ожидание линейно и
            константы можно выносить за знак математического ожидания,
            а математическое ожидание суммы равно сумме математических ожиданий
            \begin{align*}
                \meanof{\theta}{\left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \hat{\theta}\left(\vec{x}_\sigma\right)\right\}}
                    =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                        \meanof{\theta}{\hat{\theta}\left(\vec{x}_\sigma\right)}
            \end{align*}

            Не забываем, что математическое ожидание оценки
            любого вектора $\vec{x}_\sigma$ одинаково и равно параметру $\theta$

            \begin{align*}
                \frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \meanof{\theta}{\hat{\theta}\left(\vec{x}_\sigma\right)}
                    =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}\theta
            \end{align*}

            Сумма имеет $n!$ слагаемых (количество перестановок $\sigma\in S_n$)
            \begin{align*}
                \frac{1}{n!}\cdot\sum_{\sigma\in S_n}\theta
                    =\frac{1}{n!}\cdot n!\cdot\theta
                    =\theta
            \end{align*}

            А это значит, что первый пункт доказан и симметризация
            несмещённой оценки $\hat{\theta}$ действительно несмещённая
            $$\meanof{\theta}{\Lambda\hat{\theta}}=\theta$$
        \item
            Теперь посмотрим, чему равна дисперсия симметризации
            оценки $\hat{\theta}$

            Воспользуемся определением
            \begin{align*}
                \dispersionof{\theta}{\Lambda\hat{\theta}}
                    =\meanof{\theta}{
                        \left(\Lambda\hat{\theta}-\theta\right)^2}
                    =\meanof{\theta}{
                        \left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                            \hat{\theta}\left(\vec{x}_\sigma\right)
                            -\theta\right\}^2}
            \end{align*}

            Внесём параметр $\theta$ в сумму.
            Для этого нужно умножить и поделить его на $n!$
            (так как сумма имеет $n!$ слагаемых)
            \begin{align*}
                \meanof{\theta}{\left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \hat{\theta}\left(\vec{x}_\sigma\right)-\theta\right\}^2}=\\
                    =\meanof{\theta}{
                        \left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                            \hat{\theta}\left(\vec{x}_\sigma\right)
                            -\frac{1}{n!}\cdot n!\cdot\theta\right\}^2}=\\
                    =\meanof{\theta}{
                            \left\{\frac{1}{n!}\cdot\left(\sum_{\sigma\in S_n}
                            \hat{\theta}\left(\vec{x}_\sigma\right)
                            -n!\cdot\theta\right)\right\}^2}=\\
                    =\meanof{\theta}{
                        \left\{\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                            \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                            -\theta\right)\right\}^2}=\\
                    =\meanof{\theta}{
                        \left\{\sum_{\sigma\in S_n}\frac{1}{n!}\cdot
                            \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                            -\theta\right)\right\}^2}
            \end{align*}

            Вспомним неравенство Йенсена для выпуклой функции $f$
            $$f\left(\sum_{i=1}^n q_i\cdot x_i\right)
                \le \sum_{i=1}^n q_i\cdot f\left(x_i\right),
                    \qquad\sum_{i=1}^n q_i=1$$

            В нашем случае
            $x_i=\left(\hat{\theta}
                \left(\vec{x}_{\sigma_i}\right)-\theta\right)$,
            функция $f\left(x\right)=x^2$,
            сумма проходит по всевозможным перестановкам $\sigma$,
            а роль $q_i$ выполняет $\frac{1}{n!}$,
            так как
                $$\sum_{\sigma\in S_n} q_i
                    =\sum_{\sigma\in S_n}\frac{1}{n!}=n!\cdot\frac{1}{n!}=1$$

            Перепишем неравенство Йенсена для нашего случая
            \begin{equation}\label{eq:jensen_symmetrization}
                \meanof{\theta}{\left\{\sum_{\sigma\in S_n}\frac{1}{n!}\cdot
                    \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                    -\theta\right)\right\}^2}
                    \le\meanof{\theta}{\sum_{\sigma\in S_n}\frac{1}{n!}\cdot
                        \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                        -\theta\right)^2}
            \end{equation}

            Воспользуемся линейностью математического ожидания,
            внеся его под знак суммы
            \begin{align*}
                \meanof{\theta}{\sum_{\sigma\in S_n}\frac{1}{n!}\cdot
                    \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                    -\theta\right)^2}
                =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \meanof{\theta}{
                        \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                        -\theta\right)^2}
            \end{align*}

            Видим сумму дисперсий.
            Дисперсии одинаковы, так как оценки имеют одинаковые распределения
            \begin{align*}
                \frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \meanof{\theta}{
                        \left(\hat{\theta}\left(\vec{x}_\sigma\right)
                        -\theta\right)^2}
                =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \dispersionof{\theta}{
                        \hat{\theta}\left(\vec{x}_\sigma\right)}=\\
                =\frac{1}{n!}\cdot\sum_{\sigma\in S_n}
                    \dispersionof{\theta}{\hat{\theta}\left(\vec{x}\right)}
                =\frac{1}{n!}\cdot n!
                    \cdot\dispersionof{\theta}{\hat{\theta}\left(\vec{x}\right)}
                =\dispersionof{\theta}{\hat{\theta}\left(\vec{x}\right)}
            \end{align*}

            Из неравенства Йенсена \eqref{eq:jensen_symmetrization} видим,
            что дисперсия симметризации не хуже дисперсии самой оценки
            $$\dispersionof{\theta}{\Lambda\hat{\theta}}
                \le\dispersionof{\theta}{\hat{\theta}\left({\vec{x}}\right)}$$

    \end{enumerate}

    То есть, симметризация не ухудшает оценку,
    а в общем случае (когда неравенство строгое) даже делает её лучше.
\end{proof}

\begin{remark}
    Равенство в неравенстве Йенсена (в доказательстве выше)
    возможно только в случае симметричной функции.
    Значит,
    в качестве оценки достаточно брать только симметричные функции выборки
\end{remark}
\begin{definition}[Функция вариационного ряда]\index{функция!вариационного ряда}
    Если оценка $\hat{\theta}$ симметрична относительно перестановок аргументов,
    то она является функцией вариационного ряда
\end{definition}
\begin{remark}
    Все оценки, которые претендуют быть оптимальными,
    должны быть функциями вариационного ряда
\end{remark}
\section{Условное математическое ожидание и условные распределения}
\subsection{$\sigma$-алгебра, порождённая случайной величиной}
\index{сигма-алгебра!порождённая случайной величиной}
Имеем вероятностное пространство
$\left( \Omega, \mathfrak{F}, \mathbb{P} \right)$,
также есть функция $\xi: \Omega\rightarrow\mathbb{R}$
такая, что связанные с ней множества измеримы по Лебегу
$$\left\{\omega
    \mid \xi\left(\omega\right) < c\right\} \in \mathfrak{F}, c\in \mathbb{R}$$

Но это будет неудобно при использовании,
поэтому возьмём борелевские подмножества $\mathfrak{B}$ множества $\mathbb{R}$
$$\mathbb{R}\supset\mathfrak{B}\ni\Delta:
    \xi^{-1}\left( \Delta \right) \in \mathfrak{F}$$

Рассмотрим более подробно,
что же означает запись $\xi^{-1}\left( \Delta \right)$
$$\xi^{-1}\left( \Delta \right)
    = \left\{ \omega \mid \xi\left( \omega \right) \in \Delta \right\},
    \qquad \Delta\in\mathfrak{B}, \omega\in\Omega$$

\begin{definition}[Сигма-алгебра, порождённая случайной величиной]
    \index{Сигма-алгебра, порождённая случайной величиной}
    $\mathfrak{F}_\xi = \sigma\left( \xi \right)$
    --- $\sigma$-алгебра, порождённая случайной величиной $\xi$
    $$\mathfrak{F}_\xi
        =\left\{ \xi^{-1}\left( \Delta \right)
            \mid \Delta\in\mathfrak{B} \right\}$$
\end{definition}

Из курса теории вероятностей помним лемму, которая утверждает,
что $\xi$ --- случайная величина тогда и только тогда, когда
$$\forall\Delta\in\mathfrak{B}:
    \left\{ \omega \mid \xi\left( \omega \right) \in \Delta \right\}
    = \left\{ \xi\in\Delta \right\}
    = \xi^{-1}\left( \Delta \right) \in \mathfrak{F}$$

А это значит, что все элементы $\sigma$-алгебры $\mathfrak{F}_\xi$
входят в $\sigma$-алгебру $\mathfrak{F}$, а сама $\mathfrak{F}_\xi$
является подмножеством $\mathfrak{F}$
\begin{align*}
    \begin{cases}
        \mathfrak{F}_\xi
            = \left\{ \xi^{-1}\left( \Delta \right)
                \mid \Delta\in\mathfrak{B} \right\}\\
        \forall\Delta\in\mathfrak{B}:
            \xi^{-1}\left( \Delta \right) \in \mathfrak{F}
    \end{cases}
    \Rightarrow
    \mathfrak{F}_\xi \subset \mathfrak{F}
\end{align*}

Проверим, что $\mathfrak{F}_\xi$ действительно является $\sigma$-алгеброй
\begin{enumerate}
    \item Множество элементарных исходов $\Omega$ входит в $\mathfrak{F}_\xi$.
        Поскольку случайная величина $\xi$ принимает действительные значения,
        то прообраз множества действительных чисел $\mathbb{R}$
        и будет множеством элементарных исходов $\Omega$.
        А поскольку $\mathbb{R}$ принадлежит борелевской $\sigma$-алгебре,
        то его прообраз по определению принадлежит
        $\sigma$-алгебре $\mathfrak{F}_\xi$
        \begin{align*}
            \begin{cases}
                \xi^{-1}\left( \Delta \in \mathfrak{B} \right) \in\mathfrak{F}\\
                \mathbb{R}\in\mathfrak{B}\\
                \xi^{-1}\left( \mathbb{R} \right)=\Omega
            \end{cases}
            \Rightarrow
            \Omega \in \mathfrak{F}_\xi
        \end{align*}
    \item Если событие $A$ принадлежит $\mathfrak{F}_\xi$,
        то его дополнение $\stcomp{A}$ тоже принадлежит $\mathfrak{F}_\xi$
        \begin{align*}
            A=\xi^{-1}\left( \Delta \right)
                &=\left\{ \omega \mid \xi\left( \omega \right)
                    \in \Delta \right\}\\
            \Rightarrow
            \stcomp{A}
                =\left\{ \omega \mid \xi\left( \omega \right)
                    \notin \Delta \right\}
                &=\left\{ \omega \mid \xi\left( \omega \right)
                    \in \stcomp{\Delta}\right\}\\
            \stcomp{A}&=\xi^{-1}\left( \stcomp{\Delta} \right)
        \end{align*}

        Поскольку $\mathfrak{B}$ является $\sigma$-алгеброй,
        а $\Delta$ --- её элемент,
        то дополнение $\stcomp{\Delta}$ тоже принадлежит
        $\sigma$-алгебре $\mathfrak{B}$.
        Из этого следует, что свойство выполняется
        \begin{align*}
            \begin{cases}
                \xi^{-1}\left( \Delta \right) \in \mathfrak{F}\\
                \Delta \in \mathfrak{B}
                    \Rightarrow \stcomp{\Delta} \in \mathfrak{B}
            \end{cases}
            \Rightarrow
            \stcomp{\xi^{-1}\left( \Delta \right)}
                =\xi^{-1}\left( \stcomp{\Delta} \right) \in \mathfrak{F}
        \end{align*}
    \item Замкнутость относительно счётных пересечений.

        Начнём с замкнутости относительно пересечения двух множеств
        $$A=\xi^{-1}\left( \Delta_1 \right), B=\xi^{-1}\left( \Delta_2 \right)$$

        Начинаем считать
        \begin{align*}
            A \cap B
                &=\xi^{-1}\left( \Delta_1 \right)
                    \cap \xi^{-1}\left( \Delta_2 \right) =\\
                &=\left\{ \omega \mid \xi\left( \omega \right)
                    \in \Delta_1 \right\}
                    \cap \left\{ \omega \mid \xi\left( \omega \right)
                        \in \Delta_2 \right\} =\\
                &=\left\{ \omega \mid \xi\left( \omega \right) \in \Delta_1
                    \wedge \xi\left( \omega \right) \in \Delta_2 \right\} =\\
                &=\left\{ \omega \mid \xi\left( \omega \right)
                    \in \Delta_1 \cap \Delta_2 \right\}
                =\xi^{-1}\left( \Delta_1 \cap \Delta_2 \right)
        \end{align*}

        Значит, имеем равенство
        $$\xi^{-1}\left( \Delta_1 \right) \cap \xi^{-1}\left( \Delta_2 \right)
            =\xi^{-1}\left( \Delta_1 \cap \Delta_2 \right)$$

        Пользуясь методом математической индукции нетрудно показать,
        что для любого $n$ выполняется
        $$\xi^{-1}\left( \bigcap_{i=1}^n \Delta_i  \right)
            =\bigcap_{i=1}^n \xi^{-1}\left( \Delta_i \right),
                \Delta_i \in \mathfrak{B}$$
\end{enumerate}

Как устроена эта $\sigma$-алгебра?
Каждому элементарному исходу отвечает одно и только одно значение
случайной величины, а каждому значению случайной величины
отвечает один и больше элементарных исходов.
Допустим, есть некое $a\in\mathbb{R}$, которое является образом по крайней мере
двух элементарных исходов $\omega_1$ и $\omega_2$

$$\xi\left( \omega_1 \right) = \xi\left( \omega_2 \right) = a$$

Теперь рассмотрим элемент $\Delta$ борелевской $\sigma$-алгебры $\mathfrak{B}$.
Из вышесказанного следует, что,
если число $a$ принадлежит множеству $\Delta$, то прообраз этого множества
содержит элементы $\omega_1$ и $\omega_2$,
в противном случае оба элементарных исхода не входят в прообраз
\begin{align*}
    a \in \Delta
        \Rightarrow \xi^{-1}\left( \Delta \right) \ni \omega_1, \omega_2 \\
    a \notin \Delta
        \Rightarrow \xi^{-1}\left( \Delta \right) \not\ni \omega_1, \omega_2 \\
\end{align*}

То есть, множество $\mathfrak{F}_\xi$ не будет различать
элементы $\omega_1$ и $\omega_2$.
Это в свою очередь означает, что можно разбить $\mathfrak{F}_\xi$
на уровни --- непересекающиеся подмножества

\begin{definition}[Множество уровня]\index{множество уровня}
    Множество уровня $H_t$ --- полный прообраз
    значения $t\in\mathbb{R}$ случайной величины $\xi$
    $$H_t
        = \left\{ \omega \mid \xi\left( \omega \right) = t \right\}
        = \xi^{-1}\left( t \right)$$
\end{definition}

\begin{remark}
    Уровни $H_i$ составляют разбиение множества элементарных исходов $\Omega$.
    \begin{enumerate}
        \item Множества $H_i$ не пересекаются
            $$H_{t_1} \neq H_{t_2} \Leftrightarrow t_1 \neq t_2$$
        \item Объединение всех $H_i$ даёт множество элементарных исходов
            $$\bigcup_{t \in \mathbb{R}} H_t
                = \bigcup_{t \in \mathbb{R}} \xi^{-1}\left( t \right)
                = \xi^{-1}\left( \mathbb{R} \right)
                = \Omega$$
    \end{enumerate}
\end{remark}

Очень похоже на гипотезы из курса теории вероятностей с той лишь разницей,
что уровней может быть бесконечное и даже континуальное количество,
из чего также следует, что вероятность некоторых из них может быть нулевой.

\subsection{Случайная величина, измеримая относительно $\sigma$-алгебры,
    порождённой случайной величиной}
\index{случайная величина!измеримая относительно сигма-алгебры}
В общем случае вероятностное пространство может быть разбито
на континуальное количество множеств уровней.

Начнём же с рассмотрения того случая,
когда случайная величина $\xi$ принимает $n$ значений:
$a_1, a_2, \dots, a_n$
$$\xi: \Omega \rightarrow \left\{ a_1, a_2, \dots, a_n \right\}$$

Это в свою очередь означает, что у нас есть $n$ уровней
$$H_k = \left\{ \omega \mid \xi\left( \omega \right) = a_k \right\},
    k=\overline{1,n}$$

Нетрудно понять,
что $\sigma$-алгебра $\sigma\left( \xi \right)$ содержит $2^n$ элементов
$$\sigma\left( \xi \right) = \left\{ \bigcup_{k=1}^n H_k^{\eta_k}
    \mid \eta_k = \overline{0,1}, H_k^0 = \emptyset, H_k^1 = H_k \right\}$$

Нам нет смысла пользоваться лишь одной случайной величиной $\xi$.
Нас интересует, как устроены случайные величины,
которые измеримы относительно $\sigma$-алгебры $\sigma\left( \xi \right)$.
Возьмём $\varkappa$ --- случайная величина,
измеримая относительно $\sigma\left( \xi \right)$.
Это значит, что все прообразы случайной величины $\varkappa$ должны лежать в
$\sigma$-алгебре $\sigma\left( \xi \right)$
$$\left\{ \omega \mid \varkappa\left( \omega \right) \le c \right\}
    \in \sigma\left( \xi \right)$$

То есть, прообразы $\varkappa$ выражаются через объединения уровней $H_k$
$$\left\{ \omega \mid \varkappa\left( \omega \right) \le c \right\}
    = \bigcup_{k=1}^n H_k^{\eta_k}$$

Введём обозначение
$$A\left( c \right)
    = \left\{ \omega \mid \varkappa\left( \omega \right) \le c \right\}$$

Очевидно, что при $c\to-\infty$ прообразом является пустое множество,
а когда $c\to+\infty$, то прообразом является всё множество элементарных исходов
\begin{align*}
    \left\{ \omega \mid \varkappa\left( \omega \right) \le -\infty \right\}
        &= \left\{ \omega \mid \varkappa\left(\omega\right)\in\emptyset \right\}
        = \varkappa^{-1}\left( \emptyset \right)
        = \emptyset \\
    \left\{ \omega \mid \varkappa\left( \omega \right) \le +\infty \right\}
        &= \left\{ \omega \mid\varkappa\left(\omega\right)\in\mathbb{R} \right\}
        = \varkappa^{-1}\left( \mathbb{R} \right)
        = \Omega
\end{align*}

Также ясно,
что, если имеются два элемента борелевского множества и один включён в другой,
то полный прообраз первого элемента тоже будет включён в прообраз второго
\begin{align*}
    \Delta_1, \Delta_2 \in \mathfrak{B},
    \Delta_1 \subseteq \Delta_2 \\
    \Rightarrow \varkappa^{-1}\left( \Delta_1 \right)
        \subseteq \varkappa^{-1}\left( \Delta_1 \right)
            \cup \varkappa^{-1}\left( \Delta_2 \right) = \\
        = \varkappa^{-1}\left( \Delta_1 \cup \Delta_2 \right)
        = \varkappa^{-1}\left( \Delta_2 \right)
\end{align*}

Ни у кого не возникает сомнений, что справедливо и такое утверждение
$$c_1, c_2 \in \mathbb{R}, c_1 \le c_2
    \Rightarrow A\left( c_1 \right) \subseteq A\left( c_2 \right)$$

Объединим и проанализируем вышеописанное:
\begin{enumerate}
    \item Количество элементов в множестве $A\left( c \right)$
        не уменьшается с ростом $c$
        $$c_1 \le c_2
            \Rightarrow A\left( c_1 \right) \subseteq A\left( c_2 \right)$$

    \item Множество $A\left( c \right)$ ``разрастается''
        от пустого множества $\emptyset$
        до множества элементарных событий $\Omega$
        с ростом $c$ от $-\infty$ до $+\infty$
        $$A\left( -\infty \right)=\emptyset, A\left( +\infty \right)=\Omega$$

    \item Множество $A\left( c \right)$ растёт дискретными шагами.
        Это связано с тем, что уровни $H_k$ в нашей $\sigma$-алгебре неделимы,
        а каждый её элемент должен состоять из
        объединений этих уровней и ничего другого.
\end{enumerate}

Из этого всего делаем более конкретные выводы о том,
как изменяется значение функции $A\left( c \right)$ с ростом параметра $c$.
Должны быть опорные точки, на которых происходит ``скачок'' ---
точки, на которых к объединению добавляется ещё один или более уровней.

Поскольку имеется $n$ уровней, то может быть не более $n$ скачков:
ведь самый ``медленный'' рост будет происходить,
если добавлять по одному уровню на определённых константах,
а нужно пройти всё от пустого множества $\emptyset$
до множества элементарных исходов $\Omega$.

Выделим $m$ точек ($m \le n$) $c_1<c_2<\dots<c_m$
на числовой прямой $\mathbb{R}$ как значения случайной величины $\varkappa$
$$\varkappa: \Omega \rightarrow \left\{ c_1, c_2, \dots, c_m \right\}$$

Посмотрим, как соотносятся между собой
$A\left( c_i \right)$ и $A\left( c_{i-1} \right)$,
чтобы лучше понять природу скачков.

Сначала покажем, что $A\left( c_1 \right)$ является прообразом $c_1$
$$\varkappa^{-1}\left( c_1 \right)
    = \left\{ \omega \mid \varkappa\left( \omega \right) = c_1 \right\}$$

Поскольку случайная величина не принимает значений до $c_1$,
то множество $A\left( c_1-0 \right)
=\left\{ \omega \mid \varkappa\left( \omega \right) < c_1 \right\}$ пустое.
Получаем то, что хотели
\begin{align*}
    \varkappa^{-1}\left( c_1 \right)
        &= \left\{ \omega \mid \varkappa\left( \omega \right) = c_1 \right\}
            \cup \emptyset = \\
        &= \left\{ \omega \mid \varkappa\left( \omega \right) = c_1 \right\}
            \cup \left\{ \omega
                \mid \varkappa\left( \omega \right) < c_1 \right\} = \\
        &= \left\{ \omega \mid \varkappa\left( \omega \right) \le c_1 \right\}
        = A\left( c_1 \right)
\end{align*}

Идём дальше. Обозначим $c_0 = -\infty$.
Тогда в каждой точке $A\left( c_i \right), i = \overline{1,m}$
происходит скачок на множество $\varkappa^{-1}\left( c_i \right)$, то есть 
$$A\left( c_i \right)
    = A\left( c_{i-1} \right) \cup \varkappa^{-1}\left( c_i \right)$$

Так происходит, потому что имеет место равенство,
которое выполняется из-за того,
что функция имеет скачки лишь на параметрах $c_i$,
а между ними не меняет значения
$$A\left( c_i \right) = A\left( c_{i+1} - 0 \right)$$

В таком случае тождество очевидно
\begin{align*}
A\left( c_i \right)
    = \left\{ \omega \mid \varkappa\left( \omega \right) \le c_i \right\} = \\
    = \left\{ \omega \mid \varkappa\left( \omega \right) < c_i \right\} \cup
        \left\{ \omega \mid \varkappa\left( \omega \right) = c_i \right\} = \\
    = A\left( c_{i-1}-0 \right) \cup \varkappa^{-1}\left( c_i \right)
    = A\left( c_{i-1} \right) \cup \varkappa^{-1}\left( c_i \right)
\end{align*}

Поскольку $\varkappa$ --- случайная величина, принимающая $m$ значений,
то её прообразы составляют разбиение пространства элементарных исходов $\Omega$.
А поскольку $A\left( c_{i-1} \right)$ состоит из объединений этих прообразов,
то оно не пересекается с $\varkappa^{-1}\left( c_i \right)$.
Это в свою очередь означает, что мы знаем, как вычислять прообраз $\varkappa$
\begin{align*}
    \begin{cases}
        A\left( c_{i-1} \right) \cap \varkappa^{-1}\left( c_i \right)
            = \emptyset \\
        A\left( c_i \right)
            = A\left( c_{i-1} \right) \cup \varkappa^{-1}\left( c_i \right)
    \end{cases}
    \Rightarrow \varkappa^{-1}\left( c_i \right) =
        A\left( c_{i} \right) \setminus A\left( c_{i-1} \right)
\end{align*}

Это в свою очередь означает,
что случайная величина $\varkappa$ принимает значение $c_i$
при выпадении любого элементарного исхода $\omega$
из множества $A\left( c_{i} \right) \setminus A\left( c_{i-1} \right)$
$$\varkappa\left( \omega \right) = c_i:
    \omega \in A\left( c_{i} \right) \setminus A\left( c_{i-1} \right)$$

Запишем это в более удобном виде
$$\varkappa\left( \omega \right)
    = \sum_{i=1}^m c_i \cdot \Indicator{\omega
        \in A\left( c_{i} \right) \setminus A\left( c_{i-1} \right)}$$

Но эта сумма кажется уродливой из-за длинного индикатора и непонятного $m$.
Попытаемся разобраться,
в чём же дело и как прийти к изначальной $n$ и милым $H_k$.

Помним, что $A\left( c_{i} \right) \setminus A\left( c_{i-1} \right)$ ---
объединение нескольких множеств уровня $H_k$.

Предположим, есть некое $t$ такое,
что $A\left( c_{t} \right) \setminus A\left( c_{t-1} \right)$
является объединением двух
(нетрудно показать, что для любого количества, в том числе и одного)
уровней, которые обозначим $H_1^t$ и $H_2^t$.
Тогда $t$-ое слагаемое примет следующий вид
$$c_t \cdot \Indicator{\omega \in
    A\left( c_{t} \right) \setminus A\left( c_{t-1} \right)}
    = c_t \cdot \Indicator{\omega \in H_1^t \cup H_2^t}$$

Поскольку уровни не пересекаются, то можно разбить индикатор на сумму
\begin{align*}
c_t \cdot \Indicator{\omega \in H_1^t \cup H_2^t}
    &= c_t \cdot \left( \Indicator{\omega \in H_1^t}
        + \Indicator{\omega \in H_2^t} \right) \\
    &= c_t \cdot \Indicator{\omega \in H_1^t}
        + c_t \cdot \Indicator{\omega \in H_2^t}
\end{align*}

Если ввести две константы $c_1^t$ и $c_2^t$, которые будут равны старой $c_t$,
то равенство примет более симпатичный вид
$$c_t \cdot \Indicator{\omega \in H_1^t}
    + c_t \cdot \Indicator{\omega \in H_2^t}
    = c_1^t \cdot \Indicator{\omega \in H_1^t}
    + c_2^t \cdot \Indicator{\omega \in H_2^t}$$

Таким же образом можно поступить со всеми объединениями.
В итоге получим $n$ констант $d_1, d_2, \dots, d_n$
вместо $m$ $c_1, c_2, \dots, c_m$.

Теперь сумма примет более приятный для глаз
и понятный из контекста начала раздела вид
\begin{align*}
    \varkappa\left( \omega \right)
        &= \sum_{i=1}^m c_i \cdot \Indicator{\omega
            \in A\left( c_{i} \right) \setminus A\left( c_{i-1} \right)} \\
        &= \sum_{i=1}^n d_i \cdot \Indicator{\omega \in H_i}
\end{align*}

Видим, что теперь можно определить отображение из множества значений,
принимаемых случайной величиной $\xi$, в множество значений,
принимаемых случайной величиной $\varkappa$

$$f: \left\{ a_1, a_2, \dots, a_n \right\}
    \rightarrow \left\{ d_1, d_2, \dots, d_n \right\}$$

Попробуем показать, что $\varkappa$ является функцией от $\xi$.
Очевидно, что случайная величина $\xi$ имеет такой же вид,
что и $\varkappa$ --- сумма с константами и индикаторами
$$f\left( \xi\left( \omega \right) \right)
    = f\left( \sum_{i=1}^n a_i \cdot \Indicator{\omega \in H_i} \right)$$

Поскольку уровни $H_i$ не пересекаются,
то лишь одно слагаемое не будет равно нулю:
$\omega$ может принадлежать лишь одному уровню.
В таком случае запись принимает свой первый вид
$$f\left( \xi\left( \omega \right) \right)
    = f\left( a_i \right): \omega \in H_i$$

Замечаем, что $f\left( a_i \right) = d_i$, а это и есть то значение,
которое принимает случайная величина $\varkappa$ на уровне $H_i$
$$f\left( \xi\left( \omega \right) \right)
    = f\left( a_i \right) = d_i
    = \varkappa\left( \omega \right): \omega \in H_i$$

Поскольку мы не привязывались к конкретным $i$ и конкретным $\omega$,
то получаем желаемое равенство
$$\varkappa = f\left( \xi \right)$$

Отсюда делаем вывод, что случайной величине $\varkappa$
необходимо и достаточно быть функцией случайной величины $\xi$,
чтобы быть измеримой относительно $\sigma$-алгебры
$\sigma\left( \xi \right)$, порождённой случайной величиной $\xi$

\subsection{Условное математическое ожидание}
\index{математическое ожидание!условное}
\index{условное математическое ожидание}
Имеется произвольная случайная величина $\eta$, интегрируемая с квадратом.
Нужно найти случайную величину $\tilde{\eta}$ которая
измерима в $\sigma\left( \xi \right)$
и ближайшая в среднем квадратическом к $\eta$.

\subsubsection{Проекция вектора}
Для наглядности начнём с геометрической интерпретации задачи.
Если представить $\eta$ как вектор в некоем пространстве $\mathfrak{L}$,
а $\sigma\left( \xi \right)$ как подпространство пространства $\mathfrak{L}$,
то $\tilde{\eta}$ будет ни что иное, как проекция случайной величины $\eta$
на пространство $\sigma\left( \xi \right)$.

Отдохнём от случайных величин и вспомним геометрию.

Имеется точка $x$ в пространстве $L'$.
Мы ищем такую точку $y$ в подпространстве $L\subset L'$,
что расстояние между $x$ и $y$ минимальное.
Значит, надо опустить перпендикуляр от $y$ к $L$.

У нас есть $e_1, e_2, \dots, e_n$ --- ортонормированный базис в $L$,
тогда $y$ можно найти по формуле
\begin{equation}\label{vectorProjection}
    y = \sum_{k=1}^n \left( x, e_k \right) \cdot e_k
\end{equation}

Потому что $y \in L$ должен лежать в пространстве $L$ по условию,
а это значит, что он должен быть линейной комбинацией базисных векторов
$e_1, e_2, \dots, e_n$ и это очевидно выполняется

Также разностью $x-y$ должен быть вектор, перпендикулярный пространству $L$.
То есть, скалярное произведение этой разности с любым вектором $z$
из пространства $L$ должно равняться нулю
$$\left( x-y \right) \perp L
    \Leftrightarrow \forall z \in L:
    \left( x-y,z \right)=0$$

Вследствие линейности скалярного произведения можно переписать это условие иначе
\begin{align*}
    \begin{cases}
        \forall z \in L: \left( x-y,z \right)=0\\
        \left( a+b,c \right)=\left( a,c \right)+\left( b,c \right)
    \end{cases}
    \Rightarrow \forall z \in L: \left( x,z \right)=\left( y,z \right)
\end{align*}

Покажем, что и это выполняется.
$z$ является линейной комбинацией базисных векторов. Запишем это
$$z = \sum_{k=1}^n \beta_k\cdot e_k$$

В таком случае скалярное произведение $\left( x,z \right)$ будет таким
$$\left( x,z \right)=\sum_{k=1}^n \beta_k\cdot \left( x,e_k \right)$$

С произведением $\left( y,x \right)$ придётся чуть-чуть повозиться
$$\left( y,x \right)
    =\left( \sum_{k=1}^n\left( x,e_k \right)\cdot e_k,
        \sum_{k=1}^n \beta_k\cdot e_k \right)
    =\sum_{k=1}^n \left( x,e_k \right)\cdot\beta_k$$

Как видим, суммы равны, а значит, проекция $x$ на $L$ найдена верно.

\subsubsection{Проекция случайной величины}
Возьмём $L$ --- множество всех случайных величин, которые
измеримы относительно $\sigma\left( \xi \right)$.
$$L \ni \sum_{k=1}^n c_k \cdot \indicatorof{H_k}, c_k \in \mathbb{R}$$

Но что же взять в качестве ортонормированного базиса?
По внешнему виду элементов пространства $L$ кажется, что это $\indicatorof{H_k}$.
В качестве скалярного произведения случайных величин
возьмём математическое ожидание произведения.

Оказывается, $H_k$ действительно ортогональны
$$k_1 \neq k_2
    \Rightarrow H_{k_1} \cap H_{k_2} = \emptyset
    \Rightarrow
    \mean{\left( \indicatorof{H_{k_1}}\cdot \indicatorof{H_{k_1}} \right)} = 0$$

Теперь нужно нормировать эти базисные вектора,
а для этого их надо поделить на их нормы.
В нашем пространстве норма порождена скалярным произведением,
то есть
$$\left\| x \right\| = \sqrt{\left( x,x \right)}
    = \sqrt{\mean{\left( x \cdot x \right)}}
    = \sqrt{\mean{\left( x^2 \right)}}, x \in L$$

Теперь у нас есть всё необходимое для того,
чтобы представить ортонормированный базис.
Начнём преобразования $H_k$
$$e_k = \frac{\indicatorof{H_k}}
    {\sqrt{\mean{\left( \indicatorof{H_k} \right)^2}}}$$

Поскольку индикатор может принимать лишь одно из двух значений $0$ или $1$,
а их квадраты равны им самим, то в формуле квадрат тоже можно убрать
$$e_k = \frac{\indicatorof{H_k}}{\sqrt{\mean{\indicatorof{H_k}}}}$$

Также помним, что математическое ожидание в знаменателе есть ни что иное,
как вероятность события $H_k$,
и теперь у нас есть красивый ортонормированный базис
\begin{equation}\label{orthonormalBasis}
e_k = \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}}
\end{equation}

Идём дальше, ищем проекцию.
Вспомним снова пример с векторами \eqref{vectorProjection}
$$y = \sum_{k=1}^n \left( x, e_k \right) \cdot e_k$$

Если заменить $y$ на $\tilde{\eta}$, а $x$ на $\eta$,
то получаем следующую картину, имеющую непосредственное отношение к задаче
$$\tilde{\eta} = \sum_{k=1}^n \left( \eta, e_k \right) \cdot e_k$$

Осталось заменить $e_k$ на то, что получили выше \eqref{orthonormalBasis}
$$\tilde{\eta}
    = \sum_{k=1}^n
        \left( \eta, \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}} \right) 
        \cdot \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}}$$

Заменяем скалярное произведение на математическое ожидание произведения
и получаем то, с чем можно дальше работать, не отвлекаясь на геометрию
$$\tilde{\eta}
    = \sum_{k=1}^n
        \mean{\left( \eta
            \cdot \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}} \right)}
        \cdot \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}}$$

Поскольку вероятность $\probability{H_k}$ --- константа,
то её можно вынести за математическое ожидание
$$\tilde{\eta}
    = \sum_{k=1}^n
        \frac{\mean{\left( \eta \cdot \indicatorof{H_k} \right)}}
            {\sqrt{\probability{H_k}}}
        \cdot \frac{\indicatorof{H_k}}{\sqrt{\probability{H_k}}}$$

Произведения корней вероятности даёт саму вероятность.
Теперь у нас есть красивая формула для проекции случайной величины
\index{проекция!случайной величины}
\begin{equation}\label{discreteConditionalExpectation}
    \tilde{\eta} = \sum_{k=1}^n
        \frac{\mean{\left( \eta \cdot \indicatorof{H_k} \right)}}
            {\probability{H_k}}
        \cdot \indicatorof{H_k}
\end{equation}

На что стоит обратить внимание в этой формуле:
\begin{enumerate}
    \item $\tilde{\eta}$ --- \textbf{случайная величина},
        так как индикатор вне математического ожидания никуда не девается
        и результат суммы будет зависеть от произошедшего $\omega$,
        а точнее от того, какому уровню $H_k$ оно принадлежит
    \item Когда $\omega$ принадлежит $H_k$,
        то результатом суммы будет среднее значение случайной величины $\eta$
        на событии $H_k$
\end{enumerate}

Если с первым пунктом всё очевидно,
то небольшое пояснение ко второму не помешает.

Нужно показать, $k$-я ``координата'' случайной величины $\tilde{\eta}$
действительно даёт среднее значение случайной величины $\eta$ на событии $H_k$
$$\frac{\mean{\left( \eta \cdot \indicatorof{H_k} \right)}}{\probability{H_k}}$$

Начнём с определения математического ожидания
\begin{equation}\label{meanIndicator}
    \begin{aligned}
    \mean{\left( \eta \cdot \indicatorof{H_k} \right)}
        &=\integralc{\Omega}{}{}{\probability{d\omega}}{
            \eta\left( \omega \right) \cdot \indicatorof{H_k}\cdot} =\\
        &=\integralc{H_k}{}{}{\probability{d\omega}}{
            \eta\left( \omega \right)}
            + \integralc{\Omega \setminus H_k}{}{}{
                \probability{d\omega}}{0 \cdot}
    \end{aligned}
\end{equation}

Видим математическое ожидание случайной величины,
которая гарантированно принимает нулевое значение
на множестве $\Omega \setminus H_k$,
что в свою очередь искажает желаемую картину и притягивает результат к нулю
с силой, которая пропорциональна $\probability{\Omega \setminus H_k}$.
То есть, ``вес'' каждого ненулевого значения случайной величины уменьшился.

Почему так происходит?
Потому что вероятность события $H_k$ в общем случае не равна единице.
Если ввести новую меру
$\mathbb{P}_k\left( A \right)=\frac{\probability{A}}{\probability{H_k}}$,
то наступит гармония, а
вероятность $\mathbb{P}_k\left( H_k \right)$ будет равна единице.

Из контекста понятно, что нигде кроме как в интеграле по событию $H_k$ эта
мера использоваться не будет,
поэтому она будет колебаться в пределах $\left[ 0;1 \right]$,
но строгости ради введём небольшую поправку (и увидим, что не напрасно)
$$\mathbb{P}_k\left( A \right)
    =\frac{\probability{A\cap H_k}}{\probability{H_k}}$$

Видим условную вероятность, а значит, мы на правильном пути!
Логично, что в поисках условного математического ожидания
должна была встретиться условная вероятность
$$\mathbb{P}_k\left( A \right)
    =\frac{\probability{A\cap H_k}}{\probability{H_k}}
    =\probability{A \mid H_k}$$

Теперь математическое ожидание \eqref{meanIndicator}
принимает несколько иной вид
$$\mean{\left( \eta \cdot \indicatorof{H_k} \right)}
        =\integralc{H_k}{}{}{\probability{d\omega \mid H_k}}{
            \eta\left( \omega \right)} \cdot \probability{H_k}$$

Тут уже уровень $H_k$ играет роль целого множества элементарных исходов,
его мера $\probability{H_k \mid H_k}$ равна единице,
а мы получаем действительно среднее значение случайной величины $\eta$
на множестве $H_k$, умноженное на вероятность $H_k$.
Значит, осталось лишь поделить обе части на $\probability{H_k}$
$$\frac{\mean{\left( \eta \cdot \indicatorof{H_k} \right)}}{\probability{H_k}}
        =\integralc{H_k}{}{}{\probability{d\omega \mid H_k}}{
            \eta\left( \omega \right)}$$

\begin{definition}[Условное математическое ожидание случайной величины
    относительно случайного события]
Условное математическое
ожидание случайной величины $\xi$
относительно \textbf{события} $A$ \cite[стр.~68]{BorovkovPT}
обозначается $\mean{\left( \xi \mid A \right)}$ и считается по формуле
$$\mean{\left( \xi \mid A \right)}
    =\frac{\mean{\left( \xi \cdot \indicatorof{A} \right)}}{\probability{A}}
        =\integralc{A}{}{}{\probability{d\omega \mid A}}{
            \xi\left( \omega \right)}$$
\end{definition}

Пользуясь только что введённым обозначением,
можно более красиво переписать формулу \eqref{discreteConditionalExpectation}
для получения проекции случайной величины $\eta$ на $\sigma$-алгебру,
порождённой уровнями $H_1, H_2, \dots, H_n$ 
$$\tilde{\eta}
    = \sum_{k=1}^n \mean{\left( \eta \mid H_k \right)} \cdot \indicatorof{H_k}$$

Забегая наперёд,
введём определение частного случая условного математического ожидания,
чтобы обратить внимание на этот важный момент.

\begin{definition}[Условное математическое ожидание
    случайной величины относительно сигма-алгебры,
    порождённой случайной величиной,
    принимающей конечное количество значений]
    \index{условное математическое ожидание}
    \index{математическое ожидание!условное}
    Есть $\sigma$-алгебра $\mathfrak{F}_1$,
    разбитая на $n$ уровней $H_1, H_2, \dots, H_n$.
    Тогда условное математическое ожидание дискретной случайной величины
    $\eta$ относительно этой $\sigma$-алгебры --- \textbf{случайная величина},
    которая обозначается $\mean{\left[ \eta \mid \mathfrak{F_1} \right]}$
    и вычисляется по формуле
    $$\mean{\left[ \eta \mid \mathfrak{F}_1 \right]}
        = \sum_{k=1}^n
            \frac{\mean{\left( \eta \cdot \indicatorof{H_k} \right)}}
                {\probability{H_k}}
            \cdot \indicatorof{H_k}$$
\end{definition}

\begin{remark}
    Далее не будем использовать условное математическое случайной величины
    относительно \textbf{случайного события},
    так как это может вызвать путаницу.
    Например, последнее определение может вызвать недоумение,
    если его переписать, пользуясь этим определением
    $$\mean{\left[ \eta \mid \mathfrak{F}_1 \right]}
        = \sum_{k=1}^n \mean{\left( \eta \mid H_k \right)}
            \cdot \indicatorof{H_k}$$

    Поэтому далее запись $\mean{\left[ \eta \mid \mathfrak{F}_1 \right]}$
    будет означать лишь условное математическое ожидание
    случайной величины $\eta$
    относительно \textbf{$\sigma$-алгебры} $\mathfrak{F}$
\end{remark}
