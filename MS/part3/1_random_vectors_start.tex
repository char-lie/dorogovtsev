Мы уже знаем, что нам не нужна вся выборка для построения хороших оценок ---
нам хватит достаточных статистик. Введя метод наименьших квадратов,
мы избавимся от неприятной процедуры вычисления интегралов.

Тем не менее, чтобы перейти непосредственно к изучению метода, необходимо
владеть инструментарием, коим являются случайные вектора.

\section{Основные характеристики случайного вектора}

\index{вектор!случайный}
Вспомним, что случайный вектор $\vec{\xi}$ --- это набор случайных величин
$\vec{\xi} = \left[ \xi_1, \xi_2, \dots, \xi_n \right]$, которые определены
на одном вероятностном пространстве.
Распределением случайного вектора называют совместное распределение его
компонент.

\index{плотность распределения!случайного вектора}
\index{случайный вектор!плотность распределения}
Если величины $\xi_1, \xi_2, \dots, \xi_n$ имеют совместную плотность
распределения, говорят о плотности распределения случайного вектора.
Обознчим эту плотность $p$. Тогда
\begin{enumerate}
  \item Вероятность того, что вектор $\vec{\xi}$ окажется
    в множестве $\Delta$, равна интегралу от плотности по этой области
    \begin{equation*}
      \Probability{\vec{\xi} \in \Delta}
      = \integrall{\Delta}{d\vec{u}}{\pdf{\vec{u}}}
    \end{equation*}
  \item Во всех точках плотность неотрицательна
    \begin{equation*}
      \forall \vec{x} \in \mathbb{R}^n: \pdf{\vec{x}} \ge 0
    \end{equation*}
  \item Выполняется условие нормировки
    \begin{equation*}
      \integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}}} = 1
    \end{equation*}
\end{enumerate}

Естественным образом вводится определение характеристической функции.

\begin{definition}[Характеристическая функция случайного вектора]
  \label{def:characteristicFunction}
  \index{характеристическая функция!случайного вектора}
  \index{случайный вектор!характеристическая функция}
  Значение характеристической функции случайного вектора $\vec{\xi}$
  в точке $\vec{\lambda}$ считается по формуле
  $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
      = \mean{e^{i \cdot \left( \vec{\lambda}, \vec{\xi} \right)}}
      = \mean{
      \exp{\left\{ i \cdot \sum_{k=1}^n \lambda_k \cdot \xi_k \right\}}}$$

  Когда существует плотность, имеем преобразование Фурье
  $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
      = \integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}} \cdot
      e^{i \left( \vec{\lambda}, \vec{u} \right)}}$$
\end{definition}

\begin{definition}[Математическое ожидание случайного вектора]
  \index{математическое ожидание!случайного вектора}
  \index{случайный вектор!математическое ожидание}
  Математическое ожидание случайного вектора
  $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- вектор,
  элементы которого --- математические ожидания компонент
  случайного вектора $\vec{\xi}$
  $$\mean{\vec{\xi}} = \left( \mean{\xi_1}, \dots, \mean{\xi_n} \right)$$
\end{definition}

Важным свойством является линейность математического ожидания.
\begin{affirmation}\label{af:mean:linearity}
  Пусть $\vec{\xi} = \left[ \xi_1, \xi_2, \dots, \xi_n \right]$ --- случайный
  вектор, $\operatorname{A}$ --- матрица $m \times n$,
  вектор $\vec{b} \in \mathbb{R}^n$.
  Тогда случайный вектор $\vec{\eta} = \operatorname{A}\vec{\xi} + \vec{b}$
  имеет математичское ожидание
  \begin{equation*}
    \mean{\vec{\eta}} = \operatorname{A} \mean{\vec{\xi}} + \vec{b}
  \end{equation*}
\end{affirmation}
\begin{proof}
  Действительно, для каждой компоненты $k = \overline{1, n}$
  справедливо
  \begin{equation*}
    \begin{split}
      \left( \mean{\vec{\eta}} \right)_k
      = \Mean{\left( \operatorname{A} \vec{\xi} \right)_k + b_i}
      &= \mean{\sum_{i=1}^{n} \left( a_{k,i} \cdot \xi_i \right)} + b_k = \\
      &= \sum_{i=1}^{n} \left( a_{k,i} \cdot \mean{\xi_i} \right) + b_k
      = \left( \operatorname{A} \mean{\vec{\xi}} + \vec{b} \right)_k
    \end{split}
  \end{equation*}
\end{proof}

Но что же является дисперсией случайного вектора?
Набор дисперсий компонент $\left[ \dispersion{\xi_1}, \dispersion{\xi_2},
\dots, \dispersion{\xi_n} \right]$ даёт о случайном векторе слишком мало
информации.
Например, в теории оценивания нас интересовали выражениия вида
$\dispersion{f\left( \vec{\xi} \right)}$.
По набору $\left[ \dispersion{\xi_1}, \dispersion{\xi_2}, \dots,
\dispersion{\xi_n} \right]$ невозможно в общем случае вычислить
$\dispersion{f\left( \vec{\xi} \right)}$ даже для линнейной функции
$f\left( \vec{x} \right) = \alpha_1 \cdot x_1 + \dots + \alpha_n \cdot x_n$.
Нужна какая-либо информация о совместном распределении компонент вектора.

\subsection{Ковариационная матрица случайного вектора}

Начнём с определения ковариации двух случайных величин.

\begin{definition}[Ковариация]
  \index{ковариация}
  Ковариация двух случайных величин $\xi$ и $\eta$, принимающих действительные
  значения, обозначается $\cov{\xi, \eta}$ и считается по формуле
  \begin{equation}\label{eq:cov:def}
    \cov{\xi, \eta}
    = \Mean{\left( \xi - \mean{\xi} \right)
      \cdot \left( \eta - \mean{\eta} \right)}
  \end{equation}
\end{definition}

\begin{remark}
  Ковариация случайной величины $\xi$ с ней же --- её дисперсия
  $$\cov{\xi, \xi}
      = \Mean{\left( \xi - \mean{\xi} \right)
      \cdot \left( \xi - \mean{\xi} \right)}
      = \Mean{\left( \xi - \mean{\xi} \right)^2}
      = \dispersion{\xi}$$
\end{remark}

\begin{remark}
  Ковариация симметрична
  $$\cov{\xi, \eta}
      = \Mean{\left( \xi - \mean{\xi} \right)
      \cdot \left( \eta - \mean{\eta} \right)}
      = \Mean{\left( \eta - \mean{\eta} \right)
      \cdot \left( \xi - \mean{\xi} \right)}
      = \cov{\eta, \xi}$$
\end{remark}

\begin{remark}\label{rem:covIndepentent}
  Ковариация двух независимых случайных величин равна нулю
  \cite[с.~244]{Feller1}
\end{remark}

Раскрыв скобки в \eqref{eq:cov:def}, получим ещё одно выражение для
ковариации
\begin{equation*}
  \cov{\xi, \eta} = \Mean{\xi \cdot \eta} - \mean{\xi} \cdot \mean{\eta}
\end{equation*}

\begin{definition}[Ковариационная матрица случайного вектора]
  \label{def:vectorCovMatrix}
  \index{ковариационная матрица!случайного вектора}

  Ковариационная матрица случайного вектора
  $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- матрица, на пересечении
  $i$ строки и $j$ столбца которой находятся ковариации $i$ и $j$ элементов
  вектора $\xi$
  $$\dCov{\vec{\xi}}
      = \left\| \cov{\xi_i, \xi_j} \right\|_{i,j=1}^n
      = \left\| \mean{
      \left\{ \left( \xi_i - \mean{\xi_i} \right)
          \cdot \left( \xi_j - \mean{\xi_j} \right)
      \right\}} \right\|_{i,j=1}^n$$

  $$\dCov{\vec{\xi}} =
  \begin{bmatrix}
      \cov{\xi_1, \xi_1} & \cdots & \cov{\xi_1, \xi_n} \\
      \vdots & \ddots & \vdots \\
      \cov{\xi_n, \xi_1} & \cdots & \cov{\xi_n, \xi_n}
  \end{bmatrix}$$

\end{definition}

\begin{remark}
  На диагонали ковариационной матрицы $\dCov{\vec{\xi}}$
  случайного вектора $\xi$ стоят дисперсии компонент вектора.
\end{remark}

Случайный вектор находится во многомерном пространстве, а это значит,
что имеется много направлений его размазывания, поэтому в качестве дисперсии
нам нужна матрица.

\begin{example}
  Возьмём двумерный вектор с одним и тем же элементом
  в каждой координате --- случайной величиной из стандартного нормального
  распределения
  $$\vec{\xi} = \left( \xi, \xi \right),\; \xi \sim N\left( 0, 1 \right)$$

  Нетрудно посчитать, что ковариационная матрица будет заполнена единицами,
  так как во всех ячейках будет ковариация $\cov{\xi, \xi}$, равная
  дисперсии случайной величины $\xi$, то есть единице
  $$\dCov{\vec{\xi}} =
  \begin{bmatrix}
      1 & 1 \\
      1 & 1
  \end{bmatrix}$$
\end{example}

\begin{example}
  Возьмём опять же двумерный вектор, но с двумя независимыми
  случайными величинами из стандартного нормального распределения
  $$\vec{\xi} = \left( \xi_1, \xi_2 \right),\;
      \xi_1, \xi_2 \sim N\left( 0, 1 \right)$$

  На диагонали будут стоять единицы --- дисперсии случайных величин.
  Если две случайные величины независимы, то их ковариация равна нулю
  (замечание \ref{rem:covIndepentent}).
  Это в свою очередь означает, что вне диагонали
  будут нули
  $$\dCov{\vec{\xi}} =
  \begin{bmatrix}
      1 & 0 \\
      0 & 1
  \end{bmatrix}$$
\end{example}

\begin{definition}[Сопряжённая матрица]
  \index{сопряжённая матрица}
  \index{матрица!сопряжённая}
  Есть матрица $A$ размером $n \times m$ с комплексными элементами.
  Тогда сопряжённая к ней матрица $A^*$ получается путём транспонирования
  матрицы $A$ и замены всех элементов на комплексно-сопряжённые
  \cite[с.~243]{VoevodinLA}, то есть
  $$\left( a_{i,j}^* = \overline{a_{j,i}} \right),\;
  A \in \mathbb{C}^{n \times m}, A^* \in \mathbb{C}^{m \times n}$$

  Или же в таком виде
  $$A =
  \begin{bmatrix}
      a_{1,1} & \cdots & a_{1,m} \\
      \vdots & \ddots & \vdots \\
      a_{n,1} & \cdots & a_{n,m}
  \end{bmatrix}
      \Rightarrow
  A^* = 
  \begin{bmatrix}
      \overline{a_{1,1}} & \cdots & \overline{a_{n,1}} \\
      \vdots & \ddots & \vdots \\
      \overline{a_{1,m}} & \cdots & \overline{a_{n,m}}
  \end{bmatrix}$$
\end{definition}

\begin{remark}
  Отметим, что к матрице с действительными коэффициентами сопряжённой будет
  транспонированная матрица
  $$A \in \mathbb{R}^{n \times m} \Rightarrow A^* = A^T$$
\end{remark}

Перейдём к свойствам ковариационной матрицы
\begin{enumerate}
\index{ковариационная матрица!свойства}
  \item Симметричность. Ковариационная матрица случайного вектора $\vec{\xi}$
      равна своей сопряжённой
      $$\dCov{\vec{\xi}} = \dcCov{\vec{\xi}}$$
  \item Неотрицательная определённость\footnote{Больше о неотрицательно
      определённых операторах можно почитать в книге Ильина и Позняка
      ``Линейная алгебра'' \cite[с.~139]{IlinPoznyarLA}.
      В ней такой оператор называется положительным.}
      $$\dCov{\vec{\xi}} \ge 0$$

      Это значит следующее
      $$\forall \vec{u} \in \mathbb{R}^n:\;
      \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
      = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
      \ge 0$$

      \begin{proof}
      Распишем ковариацию по определению
      \begin{align*}
          \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
          = \sum_{i,j=1}^{n} \Mean{\left( \xi_i - \mean{\xi_i} \right)
            \cdot \left( \xi_j - \mean{\xi_j} \right)}
            \cdot u_j \cdot u_i = \\
          = \Mean{\sum_{i=1}^{n} \left( \xi_i - \mean{\xi_i} \right) \cdot u_i
            \cdot \sum_{j=1}^{n}\left( \xi_j - \mean{\xi_j} \right) \cdot u_j}
          = \mean{\left( \sum_{t=1}^{n} u_t
            \cdot \left( \xi_t - \mean{\xi_t} \right) \right)^2}
          \ge 0
      \end{align*}
      \end{proof}
  \item Для неслучайного вектора $\vec{a} \in \mathbb{R}^n$
      \begin{equation*}
        \dCov{\vec{\xi} + \vec{a}} = \dCov{\vec{\xi}}
      \end{equation*}
\end{enumerate}

\begin{remark}\label{remark:linearAlgebra:selfAdjointMatrix}
  Вспомним линейную алгебру.

  Самосопряжённая неотрицательно определённая матрица $\dCov{\vec{\xi}}$ имеет
  собственный ортонормированный базис, в котором она превращается в
  диагональную матрицу с неотрицательным элементами
  $$\begin{bmatrix}
      \lambda_1 & & \mbox{\Huge{$\varnothing$}} \\
       & \ddots &  \\
       \mbox{\Huge{$\varnothing$}} & & \lambda_n
  \end{bmatrix},\; \lambda_k \ge 0$$

  Далее будем упускать нули, подразумевая диагональные матрицы.

  Как эта матрица преобразует пространство?

  Единичная матрица не меняет ничего
  $$\begin{bmatrix}
      1 & &\\
      & \ddots & \\
      & & 1
  \end{bmatrix}$$

  Если первый элемент единичной матрицы сделать нулём, то такой оператор
  убивает первую координату вектора, на который подействует
  $$\begin{bmatrix}
      0 & & & \\
      & 1 & & \\
      & & \ddots & \\
      & & & 1
  \end{bmatrix}$$

  А такая матрица усиливает первую составляющую в десять раз и
  ослабляет остальные в десять раз
  $$\begin{bmatrix}
      10 & & &\\
      & 0.1 & & \\
      & & \ddots & \\
      & & & 0.1
  \end{bmatrix}$$

  Оказывается, через ковариационную матрицу вычисляются все характеристики
  линейных преобразований.
\end{remark}

\subsection{Ковариационная матрица}
\label{section:covMatrix}
Логичным обобщением ковариационной матрицы случайного вектора является
ковариационная матрица двух случайных векторов. Сейчас станет ясно, зачем мы
дважды писали вектор $\vec{\xi}$ в индексе оператора $\dCov{\vec{\xi}}$.

\begin{definition}[Ковариационная матрица]\label{def:covMatrix}
  \index{ковариационная матрица}
  Ковариационная матрица двух случайных векторов
  $\vec{\alpha} = \left( \alpha_1, \dots, \alpha_n \right)$ и
  $\vec{\beta} = \left( \beta_1, \dots, \beta_m \right)$ --- матрица,
  в которой на пересечении $i$ строки и $j$ столбца стоит ковариация случайных
  величин $\alpha_i$ и $\beta_j$
  $$\Cov{\vec{\alpha}}{\vec{\beta}}
      = \left\| \cov{\alpha_i, \beta_j} \right\|_{
      \substack{i= \overline{1,n},\\j= \overline{1,m}}}
      = \left\| \mean{
      \left\{ \left( \alpha_i - \mean{\alpha_i} \right)
          \cdot \left( \beta_j - \mean{\beta_j} \right)
      \right\}} \right\|_{
          \substack{i= \overline{1,n},\\j= \overline{1,m}}}$$

  $$\Cov{\vec{\alpha}}{\vec{\beta}} =
  \begin{bmatrix}
      \cov{\alpha_1, \beta_1} & \cdots & \cov{\alpha_1, \beta_m} \\
      \vdots & \ddots & \vdots \\
      \cov{\alpha_n, \beta_1} & \cdots & \cov{\alpha_n, \beta_m}
  \end{bmatrix}$$
\end{definition}

\begin{theorem}[Свойства ковариационной матрицы]\label{th:covMatrix:properties}
Ковариационная матрица обладает следующими свойствами
  \index{ковариационная матрица!свойства}
  \begin{enumerate}
    \item\label{item:covMatrix:property:transposition}
        Перестановка векторов ведёт к транспонированию матрицы (симметричность)
        $$\Cov{\beta}{\alpha} = \tCov{\alpha}{\beta}$$
    \item\label{item:covMatrix:property:linearityL}
        Линейность относительно первого аргумента
        $$\Cov{\operatorname{B} \alpha_1 + \operatorname{C} \alpha_2}{\beta}
        = \operatorname{B} \Cov{\alpha_1}{\beta}
            + \operatorname{C}\Cov{\alpha_2}{\beta}$$
    \item\label{item:covMatrix:property:linearityR}
        Линейность относительно второго аргумента
        $$\Cov{\alpha}{\operatorname{D} \beta_1 + \operatorname{E} \beta_2}
        = \Cov{\alpha}{\beta_1} \operatorname{D^T}
            + \Cov{\alpha}{\beta_2} \operatorname{E^T}$$
    \item\label{item:covMatrix:property:operatorOut}
        Ковариация вектора при линейном пробразовании
        $$\dCov{\operatorname{B} \vec{\xi}}
        = \operatorname{B} \dCov{\vec{\xi}} \operatorname{B^T}$$
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
    \item\label{item:cov:commutativity}
      Следует из равенства
      \begin{equation*}
        \left\| \cov{\vec{\beta}, \vec{\alpha}} \right\|_{i,j}
        = \left\| \Mean{\left( \beta_i - \mean{\beta_i} \right)
          \cdot \left( \alpha_j - \mean{\alpha_j} \right)} \right\|_{i,j}
        = \left\| \cov{\vec{\alpha}, \vec{\beta}} \right\|_{j,i}
      \end{equation*}
    \item\label{item:cov:leftOperator}
      Имеем
      \begin{equation*}
        \begin{split}
          &\left\| \cov{\operatorname{B} \vec{\alpha}_1
              + \operatorname{C} \vec{\alpha}_2,
            \vec{\beta}} \right\|_{i,j} = \\
          &= \left\| \Mean{\left(\operatorname{B} \vec{\alpha}_1
              + \operatorname{C} \vec{\alpha}_2
              - \Mean{\operatorname{B} \vec{\alpha}_1
              + \operatorname{C} \vec{\alpha}_2} \right)
            \cdot \left( \beta_j - \mean{\beta_j} \right)} \right\|_{i,j} = \\
          &= \left\| \Mean{\sum_{k=1}^{n} \left(b_{ik} \cdot \alpha_1^k
              + c_{ik} \cdot \alpha_2^k
              - b_{ik} \mean{\alpha_1^k}
              - c_{ik} \mean{\alpha_2^k} \right)
            \cdot \left( \beta_j - \mean{\beta_j} \right)} \right\|_{i,j} = \\
%          &= \sum_{k=1}^{n} b_{ik}
%            \cdot \Mean{\left(\alpha_1^k - \mean{\alpha_1^k}\right)
%              \cdot \left( \beta_j - \mean{\beta_j} \right)}
%            + \sum_{k=1}^{n} c_{ik}
%            \cdot \Mean{\left(\alpha_2^k - \mean{\alpha_2^k}\right)
%              \cdot \left( \beta_j - \mean{\beta_j} \right)}
          &= \left\| \sum_{k=1}^{n} b_{ik} \cdot \cov{\alpha_1^k, \beta_j}
              + \sum_{k=1}^{n} c_{ik} \cdot \cov{\alpha_2^k, \beta_j}
            \right\|_{i,j}
          = \operatorname{B} \Cov{\vec{\alpha_1}}{\vec{\beta}}
            + \operatorname{C} \Cov{\vec{\alpha_2}}{\vec{\beta}}
        \end{split}
      \end{equation*}
    \item\label{item:cov:rightOperator}
      Аналогично \ref{item:cov:leftOperator}
    \item\label{item:cov:operators}
      Получается последовательным применением \ref{item:cov:leftOperator}
      и \ref{item:cov:rightOperator}
  \end{enumerate}
\end{proof}

\section{Линейные преобразования случайных векторов}
\label{section:linearTransformations}

Рассмотрим всё тот же случайный вектор $\vec{\xi} = \left( \xi_1, \dots, \xi_n
\right)$ и произвольный постоянный вектор $\vec{\lambda} \in \mathbb{R}^n$.

\subsection{Скалярное произведение}
Определим случайную величину $\eta$ как скалярное произведения векторов
$\vec{\xi}$ и $\vec{\lambda}$
$$\eta = \left( \vec{\xi}, \vec{\lambda} \right)$$

Посчитаем математическое ожидание случайной величины $\eta$.

\begin{equation}\label{eq:scalarMulMean}
  \mean{\eta}
      = \mean{\sum_{k=1}^{n} \lambda_k \cdot \xi_k}
      = \sum_{k=1}^{n} \lambda_k \cdot \mean{\xi_k}
      = \left( \vec{\lambda}, \mean{\vec{\xi}} \right)
\end{equation}

Теперь посчитаем дисперсию
$$\dispersion{\eta}
  = \mean{\left( \eta - \mean{\eta} \right)^2}
  = \mean{\left( \sum_{k=1}^{n} \lambda_k \cdot \xi_k
      - \lambda_k \cdot \mean{\xi_k} \right)^2}$$

Полученное выражение сворачивается в математическое ожидание квадрата суммы,
которая превращается в двойную сумму произведений
$$\mean{\left\{ \sum_{k=1}^{n} \lambda_k
  \cdot \left( \xi_k - \mean{\xi_k} \right) \right\}^2}
  = \sum_{i,j=1}^{n}\Mean{\left( \xi_i - \mean{\xi_i} \right)
      \cdot \left( \xi_j - \mean{\xi_j} \right)}
      \cdot \lambda_i \cdot \lambda_j$$

А это, как мы уже знаем, %из утверждения \ref{affirmation:squaredSum},
произведение ковариационной матрицы вектора $\vec{\xi}$
на вектор $\vec{\lambda}$, умноженное на тот же вектор $\vec{\lambda}$.
То есть дисперсия $\eta$ выражается следующим образом
\begin{equation}\label{eq:linearQuadraticForm}
\dispersion{\eta}
  = \dispersion{\left( \vec{\xi}, \vec{\lambda} \right)}
  = \left( \dCov{\vec{\xi}} \vec{\lambda}, \vec{\lambda} \right)
\end{equation}

Оформим вывод в виде утверждения.

\begin{affirmation}\label{affirmation:scalarMulTransformations}
  \index{случайный вектор!скалярное произведение}
  \index{распределение!скалярного произведения}
  Скалярное произведение случайного вектора $\vec{\xi}$ и произвольного
  вектора $\vec{\lambda}$ является случайной величиной с математическим
  ожиданием $a$ и дисперсией $\sigma^2$
  $$a = \left( \lambda, \mean{\vec{\xi}} \right),\;
      \sigma^2
      = \left( \dCov{\vec{\xi}} \vec{\lambda}, \vec{\lambda} \right)$$
\end{affirmation}

Обобщением утверждения \ref{affirmation:scalarMulTransformations} на случай
линейного преобразования вектоа является следующее утверждение
\begin{affirmation}\label{affirmation:vectorRotated}
  \index{случайный вектор!вращение}
  Если на случайный вектор $\vec{\xi}$ подействовать линейным оператором
  $\operatorname{T}$, то получим случайный вектор с математическим ожиданием
  $\vec{a}$ и ковариационной матрицей $\operatorname{A}$

  $$\mean{\operatorname{T} \vec{\xi}} = \operatorname{T} \mean{\vec{\xi}}$$
  $$\dCov{\operatorname{T} \vec{\xi}}
      = \operatorname{T} \operatorname{\dCov{\vec{\xi}}} \operatorname{T^*}$$
\end{affirmation}
\begin{proof}
  Утверждение \ref{af:mean:linearity} и теорема \ref{th:covMatrix:properties}
  (пункт \ref{item:cov:operators}).
\end{proof}

\begin{affirmation}\label{affirmation:randomVector:linearTransformations}
  \index{случайный вектор!линейные преобразования}
  Есть два линейных оператора $\operatorname{C} \in \mathbb{R}^{k \times n}$
  и $\operatorname{D} \in \mathbb{R}^{k \times m}$, два случайных вектора:
  вектор $\vec{\xi}$ из $\mathbb{R}^n$ с параметрами $\vec{a}$ и
  $\operatorname{A}$, вектор $\vec{\eta}$ из $\mathbb{R}^m$ с параметрами
  $\vec{b}$ и $\operatorname{B}$.

  В таком случае сумма случайных векторов, полученных с помощью операторов
  $\operatorname{C}$ и $\operatorname{D}$, будет случайным вектором
  в $\mathbb{R}^k$ с параметрами $\vec{m}$ и $\operatorname{M}$
  \begin{align*}
      \vec{m} &= \operatorname{C} \mean{\vec{\xi}}
      + \operatorname{D} \mean{\vec{\eta}} \\
      \operatorname{M} &= \operatorname{C} \dCov{\vec{\xi}} \operatorname{C^T}
      + \operatorname{C} \Cov{\vec{\xi}}{\vec{\eta}} \operatorname{D^T}
      + \operatorname{D} \Cov{\vec{\eta}}{\vec{\xi}} \operatorname{C^T}
      + \operatorname{D} \dCov{\vec{\eta}} \operatorname{D^T}
  \end{align*}
\end{affirmation}
\begin{proof}
  Утверждение \ref{af:mean:linearity} и теорема \ref{th:covMatrix:properties}
  (пункты \ref{item:cov:commutativity}, \ref{item:cov:leftOperator} и
  \ref{item:cov:rightOperator}).
\end{proof}
