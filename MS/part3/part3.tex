\chapter{Метод наименьших квадратов}
Мы уже знаем, что нам не нужна вся выборка для построения хороших оценок ---
нам хватит достаточных статистик. Введя метод наименьших квадратов,
мы избавимся от неприятной процедуры вычисления интегралов.

\section{Гауссовские случайные вектора}

\subsection{Основные характеристики случайного вектора}

Есть $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- случайный вектор.
С функцией распределения $\cdf{\vec{\xi}}$ возникают проблемы (скучновато и
громоздко), поэтому будем использовать плотность распределения.

\begin{definition}[Плотность распределения случайного вектора]
    \index{плотность распределения!случайного вектора}
    \index{случайный вектор!плотность распределения}
    $p$ --- плотность распределения случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$, если
    \begin{enumerate}
        \item Вероятность того, что вектор $\vec{\xi}$ окажется
            в множестве $\Delta$, равна интегралу от плотности по этой области
            $$\Probability{\vec{\xi} \in \Delta}
                = \integrall{\Delta}{d\vec{u}}{\pdf{\vec{u}}}$$
        \item Во всех точках плотность неотрицательна
            $$\forall \vec{x} \in \mathbb{R}^n: \pdf{\vec{x}} \ge 0$$
        \item Выполняется условие нормировки
            $$\integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}}} = 1$$
    \end{enumerate}
\end{definition}

Естественным образом вводится определение характеристической функции.

\begin{definition}[Характеристическая функция случайного вектора]
    \index{характеристическая функция!случайного вектора}
    \index{случайный вектор!характеристическая функция}
    Значение характеристической функции случайного вектора $\vec{\xi}$
    в точке $\vec{\lambda}$ считается по формуле
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \mean{e^{i \cdot \left( \vec{\lambda}, \vec{\xi} \right)}}
        = \mean{
            \exp{\left\{ i \cdot \sum_{k=1}^n \lambda_k \cdot \xi_k \right\}}}$$

    Когда существует плотность, имеем преобразование Фурье
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}} \cdot
            e^{i \left( \vec{\lambda}, \vec{u} \right)}}$$
\end{definition}

\begin{definition}[Математическое ожидание случайного вектора]
    \index{математическое ожидание!случайного вектора}
    \index{случайный вектор!математическое ожидание}
    Математическое ожидание случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- вектор,
    элементы которого --- математические ожидания компонент
    случайного вектора $\vec{\xi}$
    $$\mean{\vec{\xi}} = \left( \mean{\xi_1}, \dots, \mean{\xi_n} \right)$$
\end{definition}

Но что же является дисперсией случайного вектора?

\subsection{Ковариационная матрица}

Начнём с определения ковариации двух случайных величин.

\begin{definition}[Ковариация]
    \index{ковариация}
    Ковариация двух случайных величин $\xi$ и $\eta$, принимающих действительные
    значения, обозначается $\cov{\xi, \eta}$ и считается по формуле
    $$\cov{\xi, \eta}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \eta - \mean{\eta} \right)}$$
\end{definition}

\begin{remark}
    Ковариация случайной величины $\xi$ с ней же --- её дисперсия
    $$\cov{\xi, \xi}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \xi - \mean{\xi} \right)}
        = \Mean{\left( \xi - \mean{\xi} \right)^2}
        = \dispersion{\xi}$$
\end{remark}

\begin{remark}
    Ковариация симметрична
    $$\cov{\xi, \eta}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \eta - \mean{\eta} \right)}
        = \Mean{\left( \eta - \mean{\eta} \right)
            \cdot \left( \xi - \mean{\xi} \right)}
        =\cov{\eta, \xi}$$
\end{remark}

\begin{remark}\label{rem:covIndepentent}
    Ковариация двух независимых случайных величин равна нулю
    \cite[с.~244]{Feller1}
\end{remark}

\begin{definition}[Ковариационная матрица случайного вектора]
    \index{ковариационная матрица!случайного вектора}
    \index{ковариация!матрица}
    \index{матрица!ковариаций}
    \begin{comment}
    Ковариационная матрица двух случайных векторов
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ и
    $\vec{\eta} = \left( \eta_1, \dots, \eta_n \right)$ --- матрица, в которой
    на пересечении $i$ строки и $j$ столбца стоит ковариация случайных величин
    $\xi_i$ и $\eta_j$
    $$\Cov{\vec{\xi}}{\vec{\eta}}
        = \left\| \cov{\xi_i, \eta_j} \right\|_{i,j=1}^n
        = \left\| \mean{
            \left\{ \left( \xi_i - \mean{\xi_i} \right)
                \cdot \left( \eta_j - \mean{\eta_j} \right)
            \right\}} \right\|_{i,j=1}^n$$

    $$\Cov{\vec{\xi}}{\vec{\eta}} =
    \begin{bmatrix}
        \cov{\xi_1, \eta_1} & \cdots & \cov{\xi_1, \eta_n} \\
        \vdots & \ddots & \vdots \\
        \cov{\xi_n, \eta_1} & \cdots & \cov{\xi_n, \eta_n}
    \end{bmatrix}$$
    \end{comment}

    Ковариационная матрица случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- матрица, на пересечении
    $i$ строки и $j$ столбца которой находятся ковариации $i$ и $j$ элементов
    вектора $\xi$
    $$\dCov{\vec{\xi}}
        = \left\| \cov{\xi_i, \xi_j} \right\|_{i,j=1}^n
        = \left\| \mean{
            \left\{ \left( \xi_i - \mean{\xi_i} \right)
                \cdot \left( \xi_j - \mean{\xi_j} \right)
            \right\}} \right\|_{i,j=1}^n$$

    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        \cov{\xi_1, \xi_1} & \cdots & \cov{\xi_1, \xi_n} \\
        \vdots & \ddots & \vdots \\
        \cov{\xi_n, \xi_1} & \cdots & \cov{\xi_n, \xi_n}
    \end{bmatrix}$$

\end{definition}

\begin{remark}
    На диагонали ковариационной матрицы $\Cov{\vec{\xi}}{\vec{\xi}}$
    случайного вектора $\xi$ стоят дисперсии компонент вектора.
\end{remark}

Случайный ветор находится во многомерном пространстве, а это значит,
что имеется много направлений его размазывания, поэтому в качестве дисперсии
нам нужна матрица.

\begin{example}
    Возьмём двумерный вектор с одним и тем же элементом
    в каждой координате --- случайной величиной из стандартного нормального
    распределения
    $$\vec{\xi} = \left( \xi, \xi \right),\; \xi \sim N\left( 0, 1 \right)$$

    Нетрудно посчитать, что ковариационная матрица будет заполнена единицами,
    так как во всех ячейках будет ковариация $\cov{\xi, \xi}$, равная
    дисперсии случайной величины $\xi$, то есть, единице
    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        1 & 1 \\
        1 & 1
    \end{bmatrix}$$
\end{example}

\begin{example}
    Возьмём опять же двумерный вектор, но с двумя независимыми
    случайными величинами из стандартного нормального распределения
    $$\vec{\xi} = \left( \xi_1, \xi_2 \right),\;
        \xi_1, \xi_2 \sim N\left( 0, 1 \right)$$

    На диагонали будут стоять единицы --- дисперсии случайных величин.
    Если две случайные величины независимы, то их ковариация равна нулю
    (замечание \ref{rem:covIndepentent}).
    Это в свою очередь означает, что вне диагонали
    будут нули
    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}$$
\end{example}

\subsection{Свойства ковариационной матрицы}
\index{ковариация!матрица!свойства}
\index{матрица!ковариаций!свойства}
\index{ковариационная матрица!свойства}

\begin{definition}[Сопряжённая матрица]
    \index{сопряжённая матрица}
    \index{матрица!сопряжённая}
    Есть матрица $A$ размером $n \times m$ с комплексными элементами.
    Тогда сопряжённая к ней матрица $A^*$ получается путём транспонирования
    матрицы $A$ и замены всех элементов на комплексно-сопряжённые
    \cite[с.~243]{VoevodinLA}, то есть
    $$\left( a_{i,j}^* = \overline{a_{j,i}} \right),\;
    A \in \mathbb{C}^{n \times m}, A^* \in \mathbb{C}^{m \times n}$$

    Или же в таком виде
    $$A =
    \begin{bmatrix}
        a_{1,1} & \cdots & a_{1,m} \\
        \vdots & \ddots & \vdots \\
        a_{n,1} & \cdots & a_{n,m}
    \end{bmatrix}
        \Rightarrow
    A^* = 
    \begin{bmatrix}
        \overline{a_{1,1}} & \cdots & \overline{a_{n,1}} \\
        \vdots & \ddots & \vdots \\
        \overline{a_{1,m}} & \cdots & \overline{a_{n,m}}
    \end{bmatrix}$$
\end{definition}

\begin{remark}
    Отметим, что к матрице с действительными коэффициентами сопряжённой будет
    транспонированная матрица
    $$A \in \mathbb{R}^{n \times m} \Rightarrow A^* = A^T$$
\end{remark}

Чтобы не разрывать целостность дальнейших повествований, введём наперёд
небольшое утверждение. Точнее, просто вспомним комбинаторику.
\begin{affirmation}\label{affirmation:squaredSum}
    Квадрат суммы раскладывается в двойную сумму следующим образом
    $$\left( \sum_{k=1}^n x_k \right)^2 = \sum_{i, j = 1}^{n} x_i \cdot x_j$$
\end{affirmation}
\begin{proof}
    Чтобы убедиться в правильности формулы, вспомним мультиномиальные
    коэффициенты --- их значение и определение.

    Мультиномиальные коэффициенты --- множители при слагаемых
    $x_1^{m_1} \cdot x_n^{m_n}$ после разложения
    $\left( x_1 + \dots + x_n \right)^m$ в сумму и считаются по следующей
    формуле \cite[с.~28]{Grimaldi}
    \begin{align*}
        {m_1, \dots, m_n \choose m} = \frac{m!}{m_1! \cdots m_n!} \\
            0 \le m_1, \dots, m_n \le m,\; m_1 + \dots + m_n = m
    \end{align*}

    То есть, вот общая формула раскрытия натуральной степени $m$ произвольной
    суммы выглядит так
    $$\left( x_1 + \dots + x_n \right)^m
        = \sum_{
                \substack{m_1 + \dots + m_n = m \\
                m_1, \dots, m_n \ge 0}}
            {m_1, \dots, m_n \choose m} \cdot x_1^{m_1} \cdots x_n^{m_n}$$

    Теперь вернёмся к нашему частному случаю: $m=2$. Тогда мультиномиальные
    коэффициенты будут иметь следующий вид
    \begin{align*}
    {m_1, \dots, m_n \choose 2} = \frac{2}{m_1! \cdots m_n!} \\
        0 \le m_1, \dots, m_n \le 2,\; m_1 + \dots + m_n = 2
    \end{align*}

    Из накладываемых ограничений видно, что в знаменателе будет либо одна
    двойка, либо две единицы, так как сумма должна равняться двойке.

    Таким образом, сумму можно разбить на две части --- квадраты ($m_k = 2$)
    и попарные произведения ($m_i \cdot m_j = 1,\; i \neq j$). Запишем
    \begin{equation}\label{eq:squaredSumStart}
        \begin{split}
            \left( x_1 + \dots + x_n \right)^2
                = \sum_{k=1}^{n} \frac{2}{2} \cdot x_k^2
                    + \sum_{i=1}^{n-1}
                        \sum_{j=i+1}^n \frac{2}{1} \cdot x_i \cdot x_j = \\
                = \sum_{k=1}^{n} x_k^2
                    + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
        \end{split}
    \end{equation}

    В связи с коммутативностью умножения последнюю удвоенную двойную сумму можно
    раскрыть как сумму по всем недиагональным элементам
    \begin{align*}
        2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
            = \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
                + \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j = \\
    \end{align*}
    \begin{align*}
            \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
                + \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j = \\
            \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
                + \sum_{j=1}^{n-1} \sum_{i=i+1}^n x_j \cdot x_i = \\
            = \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
                + \sum_{i=2}^{n} \sum_{j=1}^{i-1} x_j \cdot x_i
            = \sum_{i \neq j}^{n} x_i \cdot x_j
    \end{align*}

    Вместе с суммой квадратов диагональных элементов получится сумма по всем
    произведением. Перепишем, во что превратится формула
    \eqref{eq:squaredSumStart}
    \begin{align*}
         \left( x_1 + \dots + x_n \right)^2
            = \sum_{k=1}^{n} x_k^2
                + 2 \cdot \sum_{i=1}^{n-1}
                    \sum_{j=i+1}^n \cdot x_i \cdot x_j = \\
            = \sum_{k=1}^{n} x_k^2 + \sum_{i \neq j}^{n} x_i \cdot x_j
            = \sum_{i, j = 1}^{n} x_i \cdot x_j
    \end{align*}

    То есть, действительно квадрат суммы равен сумме попарных произведений всех
    элементов, что и требовалось доказать
    $$\left( \sum_{k=1}^n x_k \right)^2 = \sum_{i, j = 1}^{n} x_i \cdot x_j$$
\end{proof}
\begin{proof}[Простое доказательство]
    Также можно доказать это утверждение, просто расписав квадрат как
    произведение
    \begin{align*}
        \left( \sum_{k=1}^{n} x_k \right)^2
        = \left( x_1 + \dots + x_n \right)
            \cdot \left( x_1 + \dots + x_n \right) = \\
        = x_1 \cdot x_1 + x_1 \cdot x_2 + \dots + x_1 \cdot x_n
            + x_2 \cdot x_1 + x_2 \cdot x_2 + \dots + x_n \cdot x_n
    \end{align*}

    Видим, что каждый элемент умножается с каждым, и всё это дело суммируется.
    Запишем в виде суммы (с красивым значком сигма)
    \begin{align*}
        \left( \sum_{k=1}^{n} x_k \right)^2
        = \left( x_1 + \dots + x_n \right)
            \cdot \left( x_1 + \dots + x_n \right) = \\
        = \sum_{i=1}^{n}
            \left( x_i \cdot x_1 + x_i \cdot x_2 + \dots + x_i \cdot x_n \right)
        = \sum_{i=1}^{n} \sum_{j=1}^{n} x_i \cdot x_j
    \end{align*}

    Что и требовалось доказать.
\end{proof}

\begin{enumerate}
    \item Симметричность. Ковариационная матрица случайного вектора $\vec{\xi}$
        равна своей сопряжённой
        $$\dCov{\vec{\xi}} = \dcCov{\vec{\xi}}$$
    \item Неотрицательная определённость\footnote{Больше о неотрицательно
            определённых операторах можно почитать в книге Ильина и Позняка
            ``Линейная алгебра'' \cite[с.~139]{IlinPoznyarLA}.
            В ней такой оператор называется положительным.}
        $$\dCov{\vec{\xi}} \ge 0$$

        Это значит следующее
        $$\forall \vec{u} \in \mathbb{R}^n:\;
            \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
            = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
            \ge 0$$

        \begin{proof}
            Распишем ковариацию по определению и воспользуемся утверждением
            \ref{affirmation:squaredSum}
            \begin{align*}
                \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i = \\
                = \sum_{i,j=1}^{n} \Mean{\left( \xi_i - \mean{\xi_i} \right)
                        \cdot \left( \xi_j - \mean{\xi_j} \right)}
                        \cdot u_j \cdot u_i =\\
                = \mean{\left( \sum_{t=1}^{n} u_t
                    \cdot \left( \xi_t - \mean{\xi_t} \right) \right)^2}
            \end{align*}

            Поскольку все коэффициенты действительные, а математическое
            ожидание константы равно самой константе, то делаем вывод,
            что сумма неотрицательна
            $$\sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                = \mean{\left( \sum_{t=1}^{n} u_t
                    \cdot \left( \xi_t - \mean{\xi_t} \right) \right)^2}
                \ge 0$$

            Вот мы и получили желаемый результат
            $$\forall \vec{u} \in \mathbb{R}^n:\;
                \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
                = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                \ge 0$$
        \end{proof}
\end{enumerate}

\begin{remark}
    Вспомним линейную алгебру.

    Самосопряжённая неотрицательно определённая матрица $\dCov{\vec{\xi}}$ имеет
    собственный ортонормированный базис, в котором она превращается в
    диагональную матрицу с неотрицательныи элементами
    $$\begin{bmatrix}
        \lambda_1 & & \mbox{\Huge{$\varnothing$}} \\
         & \ddots &  \\
         \mbox{\Huge{$\varnothing$}} & & \lambda_n
    \end{bmatrix},\; \lambda_k \ge 0$$

    Далее будем упускать символы пустоты $\varnothing$,
    подразумевая диагональные матрицы.

    Как эта матрица преобразует пространство?

    Единичная матрица не меняет ничего
    $$\begin{bmatrix}
        1 & &\\
        & \ddots & \\
        & & 1
    \end{bmatrix}$$

    Если первый элемент единичной матрицы сделать нулём, то такой оператор
    убивает первую координату вектора, на который подействует
    $$\begin{bmatrix}
        0 & & & \\
        & 1 & & \\
        & & \ddots & \\
        & & & 1
    \end{bmatrix}$$

    А такая матрица усиливает первую составляющую в десять раз и
    ослабляет остальные в десять раз
    $$\begin{bmatrix}
        10 & & &\\
        & 0.1 & & \\
        & & \ddots & \\
        & & & 0.1
    \end{bmatrix}$$

    Оказывается, через ковариационную матрицу вычисляются все характеристики
    линейных преобразований.
\end{remark}

\subsection{Линейные преобразования случайных векторов}
Рассмотрим всё тот же случайный вектор $\vec{\xi} = \left( \xi_1, \dots, \xi_n
\right)$ и произвольный константный вектор $\vec{\lambda} \in \mathbb{R}^n$.

Определим случайную величину $\eta$ как скалярное произведения векторов
$\vec{\xi}$ и $\vec{\lambda}$
$$\eta = \left( \vec{\xi}, \vec{\lambda} \right)$$

Посчитаем математическое ожидание случайной величины $\eta$.

$$\mean{\eta}
    = \mean{\sum_{k=1}^{n} \lambda_k \cdot \xi_k}
    = \sum_{k=1}^{n} \lambda_k \cdot \mean{\xi_k}
    = \left( \vec{\lambda}, \mean{\vec{\xi}} \right)$$

Теперь посчитаем дисперсию
$$\dispersion{\eta}
    = \mean{\left( \eta - \mean{\eta} \right)^2}
    = \mean{\left( \sum_{k=1}^{n} \lambda_k \cdot \xi_k
        - \lambda_k \cdot \mean{\xi_k} \right)^2}$$

Полученное выражение сворачивается в математическое ожидание квадрата суммы,
которая превращается в двойную сумму произведений
$$\mean{\left\{ \sum_{k=1}^{n} \lambda_k
    \cdot \left( \xi_k - \mean{\xi_k} \right) \right\}^2}
    = \sum_{i,j=1}^{n}\Mean{\left( \xi_i - \mean{\xi_i} \right)
            \cdot \left( \xi_j - \mean{\xi_j} \right)}
        \cdot \lambda_i \cdot \lambda_j$$

А это, как мы уже знаем из утверждения \ref{affirmation:squaredSum},
произведение ковариационной матрицы вектора $\vec{\xi}$
на вектор $\vec{\lambda}$, умноженное на тот же вектор $\vec{\lambda}$.
То есть, дисперсия $\eta$ выражается следующим образом
\begin{equation}\label{eq:linearQuadraticForm}
\dispersion{\eta}
    = \dispersion{\left( \vec{\xi}, \vec{\lambda} \right)}
    = \left( \dCov{\vec{\xi}} \cdot \vec{\lambda}, \vec{\lambda} \right)
\end{equation}

Обобщим задачу и попробуем выяснить, каким образом зависит случайный вектор
$\vec{\eta}$, полученный путём линейных преобразованиямий вектора $\vec{\xi}$,
имеющего известное математическое ожидание и ковариационную матрицу.

Для линейных преобразований вектора нужен линейный оператор. Назовём его
$\operator{T}$. Этот оператор будет действовать из пространства $\mathbb{R}^n$
в пространство $\mathbb{R}^m$, где $n$ --- размерность вектора $\vec{\xi}$,
а $m$ --- размерность вектора $\vec{\eta}$, который будет получен
в результате преобразования
$$\vec{\eta} = \operatorOn{T}{\vec{\xi}},\; T \in \mathbb{R}^{m \times n}$$

Посчитаем математическое ожидание
$$\mean{\eta} = \Mean{\operatorOn{T}{\vec{\xi}}}$$

Очевидно, что в связи с линейностью математического ожидания можно вынести
оператор $\operator{T}$ наружу.

Мы всё-таки проделаем математические выкладки по-честному.
Итак, у нас есть математическое ожидание случайного вектора
$$\Mean{\operatorOn{T}{\vec{\xi}}}
    = \mean{\left\| \sum_{j=1}^n \left( T_{i,j} \cdot \xi_j \right)
        \right\|_{i=1}^m}$$

Математическое ожидание случайного вектора --- вектор математических ожиданий
соответствующих координат.
Дальше воспользуемся линейностью математического ожидания
$$\mean{\left\| \sum_{j=1}^n \left( T_{i,j} \cdot \xi_j \right)
        \right\|_{i=1}^m}
    = \left\| \Mean{\sum_{j=1}^n \left( T_{i,j} \cdot \xi_j \right)}
        \right\|_{i=1}^m
    = \left\| \sum_{j=1}^n \left( T_{i,j} \cdot \mean{\xi_j} \right)
        \right\|_{i=1}^m$$

Видим, что перед нами произведение матрицы $\operator{T}$ на вектор
математических ожиданий координат случайного вектора $\vec{\xi}$
$$\left\| \sum_{j=1}^n \left( T_{i,j} \cdot \mean{\xi_j} \right)
    \right\|_{i=1}^m = \operatorOn{T}{\mean{\vec{\xi}}}$$

То есть, интуиция нам подсказывала правильно и конечная формула такова
$$\mean{\eta}
    = \Mean{\operatorOn{T}{\vec{\xi}}}
    = \operatorOn{T}{\mean{\vec{\xi}}}$$

Теперь нужно посчитать ковариацию. Мы могли бы решать эту задачу,
расписав произведение матрицы, но в этот раз, пожалуй, освежим наши знания
в линейной алгебре

Возьмём произвольный вектор $\vec{e} \in \mathbb{R}^n$
и выпишем квадратичную форму ковариационной матрицы вектора $\eta$
с аргументом $\vec{e}$. Из начала подраздела \eqref{eq:linearQuadraticForm}
помним, что такая квадратичная форма равна дисперсии скалярного произведения, а
дальше воспользуемся свойством симметричности скалярного произведения
(для удобства дальнейших вычислений)
$$\left( \dCov{\vec{\eta}} \cdot \vec{e}, \vec{e} \right)
    = \dispersion{\left( \vec{\eta}, \vec{e} \right)}
    = \dispersion{\left( \vec{e}, \vec{\eta} \right)}$$

Распишем наш случайный вектор $\vec{\eta}$ через случайный вектор $\vec{\xi}$
и матрицу $\operator{T}$
$$\dispersion{\left( \vec{e}, \vec{\eta} \right)}
    = \dispersion{\left( \vec{e}, \operatorOn{T}{\vec{\xi}} \right)}$$

Далее воспользуемся ещё одним определением сопряжённого оператора\footnote{На
самом деле, это и есть изначальное определение сопряжённого оператора
\cite[с.~241]{VoevodinLA}, \cite[с.~126]{IlinPoznyarLA}}
и перенесём оператор $\operator{T}$ в левую часть скалярного произведения
$$\dispersion{\left( \vec{e}, \operatorOn{T}{\vec{\xi}} \right)}
    = \dispersion{\left( \operatorOn{T^*}{\vec{e}}, \vec{\xi} \right)}$$

Перейдём от дисперсии к квадратичной форме и посмотрим, что происходит
$$\dispersion{\left( \operatorOn{T^*}{\vec{e}}, \vec{\xi} \right)}
    = \left( \operatorOn{\dCov{\vec{\xi}}}{\operatorOn{T^*}{\vec{e}}},
        \operatorOn{T^*}{\vec{e}} \right)$$

Снова воспользуемся определением сопряжённого оператора и перенесём его
из правой стороны скалярного произведения в левую. Не забываем, что
сопряжённый оператор к сопряжённму оператору --- исходный оператор
$\left( \operator{T^*} \right)^* = \operator{T}$
$$\left( \operatorOn{\dCov{\vec{\xi}}}{\operatorOn{T^*}{\vec{e}}},
        \operatorOn{T^*}{\vec{e}} \right)
    = \left( \operatorOn{T}{
            \operatorOn{\dCov{\vec{\xi}}}{\operatorOn{T^*}{\vec{e}}}},
        \vec{e} \right)$$

Видим, что квадратичные формы совпадают, а это значит, что и операторы равны
$$\left( \operatorOn{T}{
    \operatorOn{\dCov{\vec{\xi}}}{\operatorOn{T^*}{\vec{e}}}}, \vec{e} \right)
        = \left( \dCov{\vec{\eta}} \cdot \vec{e}, \vec{e} \right)
    \Rightarrow
    \operatorOn{T}{\operatorOn{\dCov{\vec{\xi}}}{\operator{T^*}}}
        = \dCov{\vec{\eta}}$$

Подведём итоги: если на случайный вектор $\vec{\xi}$ с известным математическим
ожиданием и ковариационной матрицей подействовать оператором $\operator{T}$,
то математическое ожидание полученного вектора будет считаться по формуле
$$\mean{\operatorOn{T}{\vec{\xi}}} = \operatorOn{T}{\mean{\vec{\xi}}}$$

Расчёт ковариационной матрицы происходит в базисе вектора $\vec{\xi}$
с матрицей перехода $\operator{T}$ и матрицей $\operator{T^*}$
для перехода обратно
$$\dCov{\operatorOn{T}{\vec{\xi}}}
    = \operatorOn{T}{\operatorOn{\dCov{\vec{\xi}}}{\operator{T^*}}}$$

\subsection{Гауссовские случайные вектора}

\begin{definition}[Гауссовский случайный вектор (определение Максвелла)]
    Случайный вектор $\vec{\xi}$ в $\mathbb{R}^n$ называется гауссовским, если
    его проекция на произвольный вектор из пространства $\mathbb{R}^n$
    является гауссовской случайной величиной
    $$\forall \vec{\lambda} \in \mathbb{R}^n:
        \left( \vec{\lambda}, \vec{\xi} \right)
            \sim N\left( a_{\vec{\lambda}}, \sigma_{\vec{\lambda}}^2 \right)$$
\end{definition}

\begin{example}\label{ex:gaussVectorsIntro}
    Возьмём случайный вектор $\vec{\xi}$, координаты которого между собой равны
    и являются гауссовской случайной величиной
    $$\vec{\xi} = \left( \xi, \dots, \xi \right),\;
        \xi \sim N\left( a, \sigma^2 \right)$$

    Очевидно, что математическое ожидание --- вектор
    $\left( a, \dots, a \right)$, а ковариационная матрица состоит из
    $\sigma^2$, так как на каждом месте стоит дисперсия случайной величины $\xi$
    $$\mean{\vec{\xi}} = \left( a, \dots, a \right),\;
        \dCov{\vec{\xi}} =
        \begin{bmatrix}
            \sigma^2 & \cdots & \sigma^2 \\
            \vdots & \ddots & \vdots \\
            \sigma^2 & \cdots & \sigma^2
        \end{bmatrix}$$

    Проверим, является ли вектор $\vec{\xi}$ гауссовским.
    Возьмём произвольный вектор $\vec{\lambda} \in \mathbb{R}^n$
    и посморим, чему равно скалярное произведение
    $$\left( \vec{\lambda}, \vec{\xi} \right)
        = \sum_{k=1}^{n} \left( \lambda_k \cdot \xi \right)
        = \xi \cdot \sum_{k=1}^{n} \lambda_k$$

    Получилось произведение случайной гауссовской величины и константы,
    а распределение такой величины мы знаем. Для компактности записи
    заменим сумму большой буквой ``лямбда'' $\Lambda$
    $$\xi \cdot \sum_{k=1}^{n} \lambda_k
        = \xi \cdot \Lambda \sim N\left( a \cdot \Lambda,
            \sigma^2 \cdot \Lambda^2 \right)$$

    Вывод: данный вектор $\vec{\xi}=\left( \xi, \dots, \xi \right)$ является
    гауссовским для любой нормально распределённой случайной величины $\xi$.
\end{example}

\begin{example}
    Теперь возмём случайный вектор $\vec{\xi}$, состоящий из $n$ независимых
    случайных гауссовских величин со своими математическими ожиданиями
    и дисперсиями
    $$\vec{\xi} = \left( \xi_1, \dots, \xi_n \right),\;
        \xi_k \sim N\left( a_k, \sigma_k^2 \right)$$

    С математическим ожиданием всё очевидно: это вектор
    из $a_k$. Ковариация --- диагональная матрица дисперсий, так как вне
    диагонали должны стоять ковариации случайных величин между собой,
    но они независимы, а это значит, что их ковариации равны нулю
    (замечание \ref{rem:covIndepentent})
    $$\mean{\vec{\xi}} = \left( a_1, \dots, a_n \right),\;
        \dCov{\vec{\xi}} =
        \begin{bmatrix}
            \sigma_1^2 & & \\
            & \ddots & \\
            & & \sigma_n^2
        \end{bmatrix}$$

    Снова рассматриваем скалярное произведением и видим, что результат сложнее,
    но схож с полученным в предыдущем примере \ref{ex:gaussVectorsIntro}
    $$\left( \vec{\lambda}, \vec{\xi} \right)
        = \sum_{k=1}^{n} \lambda_k \cdot \xi_k \sim
        N\left( \sum_{k=1}^{n} \lambda_k \cdot a_k,
        \sum_{k=1}^{n} \lambda_k^2 \cdot \sigma_k^2 \right)$$

    Вывод: вектор, состоящий из независимых гауссовских случайных величин,
    является гауссовским.
\end{example}

\begin{definition}[Стандартный гауссовский вектор]
    \index{гауссовский вектор!стандартный}
    Случайный вектор $\vec{\xi}$ называется стандартным гауссовским вектором,
    если его математическое ожидание --- нулевой вектор,
    а ковариационная матрица --- единичная матрица
    $$\mean{\vec{\xi}} = \vec{0},\;
        \dCov{\vec{\xi}} =
        \begin{bmatrix}
            1 & & \\
            & \ddots & \\
            & & 1
        \end{bmatrix}$$
\end{definition}

Тут возникает вопрос: однозначно ли математическое ожидание
и ковариационная матрица определяют распределение случайного вектора?

Следующая лемма показывает, что такого определения нам вполне достаточно.
