\chapter{Метод наименьших квадратов}
Мы уже знаем, что нам не нужна вся выборка для построения хороших оценок ---
нам хватит достаточных статистик. Введя метод наименьших квадратов,
мы избавимся от неприятной процедуры вычисления интегралов.

\section{Гауссовские случайные вектора}

\subsection{Основные характеристики случайного вектора}

Есть $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- случайный вектор.
С функцией распределения $\cdf{\vec{\xi}}$ возникают проблемы (скучновато и
громоздко), поэтому будем использовать плотность распределения.

\begin{definition}[Плотность распределения случайного вектора]
    \index{плотность распределения!случайного вектора}
    \index{случайный вектор!плотность распределения}
    $p$ --- плотность распределения случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$, если
    \begin{enumerate}
        \item Вероятность того, что вектор $\vec{\xi}$ окажется
            в множестве $\Delta$, равна интегралу от плотности по этой области
            $$\Probability{\vec{\xi} \in \Delta}
                = \integrall{\Delta}{d\vec{u}}{\pdf{\vec{u}}}$$
        \item Во всех точках плотность неотрицательна
            $$\forall \vec{x} \in \mathbb{R}^n: \pdf{\vec{x}} \ge 0$$
        \item Выполняется условие нормировки
            $$\integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}}} = 1$$
    \end{enumerate}
\end{definition}

Естественным образом вводится определение характеристической функции.

\begin{definition}[Характеристическая функция случайного вектора]
    \index{характеристическая функция!случайного вектора}
    \index{случайный вектор!характеристическая функция}
    Значение характеристической функции случайного вектора $\vec{\xi}$
    в точке $\vec{\lambda}$ считается по формуле
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \mean{e^{i \cdot \left( \vec{\lambda}, \vec{\xi} \right)}}
        = \mean{
            \exp{\left\{ i \cdot \sum_{k=1}^n \lambda_k \cdot \xi_k \right\}}}$$

    Когда существует плотность, имеем преобразование Фурье
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}} \cdot
            e^{i \left( \vec{\lambda}, \vec{u} \right)}}$$
\end{definition}

\begin{definition}[Математическое ожидание случайного вектора]
    \index{математическое ожидание!случайного вектора}
    \index{случайный вектор!математическое ожидание}
    Математическое ожидание случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- вектор,
    элементы которого --- математические ожидания компонент
    случайного вектора $\vec{\xi}$
    $$\mean{\vec{\xi}} = \left( \mean{\xi_1}, \dots, \mean{\xi_n} \right)$$
\end{definition}

Но что же является дисперсией случайного вектора?

\subsection{Ковариационная матрица}

Начнём с определения ковариации двух случайных величин.

\begin{definition}[Ковариация]
    \index{ковариация}
    Ковариация двух случайных величин $\xi$ и $\eta$, принимающих действительные
    значения, обозначается $\cov{\xi, \eta}$ и считается по формуле
    $$\cov{\xi, \eta}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \eta - \mean{\eta} \right)}$$
\end{definition}

\begin{remark}
    Ковариация случайной величины $\xi$ с ней же --- её дисперсия
    $$\cov{\xi, \xi}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \xi - \mean{\xi} \right)}
        = \Mean{\left( \xi - \mean{\xi} \right)^2}
        = \dispersion{\xi}$$
\end{remark}

\begin{remark}
    Ковариация симметрична
    $$\cov{\xi, \eta}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \eta - \mean{\eta} \right)}
        = \Mean{\left( \eta - \mean{\eta} \right)
            \cdot \left( \xi - \mean{\xi} \right)}
        =\cov{\eta, \xi}$$
\end{remark}

\begin{definition}[Ковариационная матрица случайного вектора]
    \index{ковариационная матрица!случайного вектора}
    \index{ковариация!матрица}
    \index{матрица!ковариаций}
    \begin{comment}
    Ковариационная матрица двух случайных векторов
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ и
    $\vec{\eta} = \left( \eta_1, \dots, \eta_n \right)$ --- матрица, в которой
    на пересечении $i$ строки и $j$ столбца стоит ковариация случайных величин
    $\xi_i$ и $\eta_j$
    $$\Cov{\vec{\xi}}{\vec{\eta}}
        = \left\| \cov{\xi_i, \eta_j} \right\|_{i,j=1}^n
        = \left\| \mean{
            \left\{ \left( \xi_i - \mean{\xi_i} \right)
                \cdot \left( \eta_j - \mean{\eta_j} \right)
            \right\}} \right\|_{i,j=1}^n$$

    $$\Cov{\vec{\xi}}{\vec{\eta}} =
    \begin{bmatrix}
        \cov{\xi_1, \eta_1} & \cdots & \cov{\xi_1, \eta_n} \\
        \vdots & \ddots & \vdots \\
        \cov{\xi_n, \eta_1} & \cdots & \cov{\xi_n, \eta_n}
    \end{bmatrix}$$
    \end{comment}

    Ковариационная матрица случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- матрица, на пересечении
    $i$ строки и $j$ столбца которой находятся ковариации $i$ и $j$ элементов
    вектора $\xi$
    $$\dCov{\vec{\xi}}
        = \left\| \cov{\xi_i, \xi_j} \right\|_{i,j=1}^n
        = \left\| \mean{
            \left\{ \left( \xi_i - \mean{\xi_i} \right)
                \cdot \left( \xi_j - \mean{\xi_j} \right)
            \right\}} \right\|_{i,j=1}^n$$

    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        \cov{\xi_1, \xi_1} & \cdots & \cov{\xi_1, \xi_n} \\
        \vdots & \ddots & \vdots \\
        \cov{\xi_n, \xi_1} & \cdots & \cov{\xi_n, \xi_n}
    \end{bmatrix}$$

\end{definition}

\begin{remark}
    На диагонали ковариационной матрицы $\Cov{\vec{\xi}}{\vec{\xi}}$
    случайного вектора $\xi$ стоят дисперсии компонент вектора.
\end{remark}

Случайный ветор находится во многомерном пространстве, а это значит,
что имеется много направлений его размазывания, поэтому в качестве дисперсии
нам нужна матрица.

\begin{example}
    Возьмём двумерный вектор с одним и тем же элементом
    в каждой координате --- случайной величиной из стандартного нормального
    распределения
    $$\vec{\xi} = \left( \xi, \xi \right),\; \xi \sim N\left( 0, 1 \right)$$

    Нетрудно посчитать, что ковариационная матрица будет заполнена единицами,
    так как во всех ячейках будет ковариация $\cov{\xi, \xi}$, равная
    дисперсии случайной величины $\xi$, то есть, единице
    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        1 & 1 \\
        1 & 1
    \end{bmatrix}$$
\end{example}

\begin{example}
    Возьмём опять же двумерный вектор, но с двумя независимыми
    случайными величинами из стандартного нормального распределения
    $$\vec{\xi} = \left( \xi_1, \xi_2 \right),\;
        \xi_1, \xi_2 \sim N\left( 0, 1 \right)$$

    На диагонали будут стоять единицы --- дисперсии случайных величин.
    Если две случайные величины независимы, то их ковариация равна нулю
    \cite[с.~244]{Feller1}. Это в свою очередь означает, что вне диагонали
    будут нули
    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}$$
\end{example}

\subsection{Свойства ковариационной матрицы}
\index{ковариация!матрица!свойства}
\index{матрица!ковариаций!свойства}
\index{ковариационная матрица!свойства}

\begin{definition}[Сопряжённая матрица]
    \index{сопряжённая матрица}
    \index{матрица!сопряжённая}
    Есть матрица $A$ с комплексными элементами. Тогда сопряжённая к ней матрица
    $A^*$ --- транспонированная матрица $A$, где все элементы заменены на
    комплексно-сопряжённые
    $$A =
    \begin{bmatrix}
        c_{1,1} & \cdots & c_{1,n} \\
        \vdots & \ddots & \vdots \\
        c_{n,1} & \cdots & c_{n,n}
    \end{bmatrix}
        \Rightarrow
    A^* = 
    \begin{bmatrix}
        \overline{c_{1,1}} & \cdots & \overline{c_{n,1}} \\
        \vdots & \ddots & \vdots \\
        \overline{c_{1,n}} & \cdots & \overline{c_{n,n}}
    \end{bmatrix}$$
\end{definition}

\begin{enumerate}
    \item Симметричность. Ковариационная матрица случайного вектора $\vec{\xi}$
        равна своей сопряжённой
        $$\dCov{\vec{\xi}} = \dcCov{\vec{\xi}}$$
    \item Неотрицательная определённость
        $$\dCov{\vec{\xi}} \ge 0$$

        Это значит следующее
        $$\forall \vec{u} \in \mathbb{R}^n:\;
            \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
            = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
            \ge 0$$

        \begin{proof}
            Разобьём сумму на две --- сумма по диагональным элементам ($i=j=t$)
            и сумму по остальным элементам. Помним, что матрица симметричная,
            а это значит, что у нас будет удвоенная сумма элементов, которые
            находятся под (над) главной диагональю
            \begin{align*}
                \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                    = \sum_{t=1}^{n} \cov{\xi_t, \xi_t} \cdot u_t \cdot u_t + \\
                        + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
                            \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
            \end{align*}

            Распишем ковариации по определению и воспользуемся линейностью
            математического ожидания
            \begin{align*}
                \sum_{t=1}^{n} \cov{\xi_t, \xi_t} \cdot u_t \cdot u_t
                    + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
                        \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i = \\
                    = \sum_{t=1}^{n}
                            \mean{\left( \xi_t - \mean{\xi_t} \right)^2}
                            \cdot u_t^2 + \\
                        + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
                            \Mean{\left( \xi_i - \mean{\xi_i} \right)
                                \cdot \left( \xi_j - \mean{\xi_j} \right)}
                            \cdot u_j \cdot u_i = \\
                    = \Mean{\sum_{t=1}^{n}
                            \left( \xi_t - \mean{\xi_t} \right)^2 \cdot u_t^2
                        + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
                            \left( \xi_i - \mean{\xi_i} \right)
                                \cdot \left( \xi_j - \mean{\xi_j} \right)
                            \cdot u_j \cdot u_i}
            \end{align*}

            Видимм, что есть сумма квадратов диагональных элементов
            и удвоенная сумма попарных произведений всех остальных элементов.
            Значит, эти суммы сворачиваются в квадрат суммы
            $$\sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                = \mean{\left( \sum_{t=1}^{n} u_t
                    \cdot \left( \xi_t - \mean{\xi_t} \right) \right)^2}$$

            Поскольку все коэффициенты действительные, а математическое
            ожидание константы равно самой константе, то получаем требуемый
            результат
            $$\forall \vec{u} \in \mathbb{R}^n:\;
                \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
                = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                \ge 0$$
        \end{proof}
\end{enumerate}

\begin{remark}
    Вспомним линейную алгебру.

    Самосопряжённая неотрицательно определённая матрица $\dCov{\vec{\xi}}$ имеет
    собственный ортонормированный базис, в котором она превращается в
    диагональную матрицу с неотрицательныи элементами
    $$\begin{bmatrix}
        \lambda_1 & & \mbox{\Huge{$\varnothing$}} \\
         & \ddots &  \\
         \mbox{\Huge{$\varnothing$}} & & \lambda_n
    \end{bmatrix},\; \lambda_k \ge 0$$

    Далее будем упускать символы пустоты $\varnothing$,
    подразумевая диагональные матрицы.

    Как эта матрица преобразует пространство?

    Единичная матрица не меняет ничего
    $$\begin{bmatrix}
        1 & &\\
        & \ddots & \\
        & & 1
    \end{bmatrix}$$

    Если первый элемент единичной матрицы сделать нулём, то такой оператор
    убивает первую координату вектора, на который подействует
    $$\begin{bmatrix}
        0 & & & \\
        & 1 & & \\
        & & \ddots & \\
        & & & 1
    \end{bmatrix}$$

    А такая матрица усиливает первую составляющую в десять раз и
    ослабляет остальные в десять раз
    $$\begin{bmatrix}
        10 & & &\\
        & 0.1 & & \\
        & & \ddots & \\
        & & & 0.1
    \end{bmatrix}$$

    Оказывается, через ковариационную матрицу вычисляются все характеристики
    линейных преобразований.
\end{remark}

\subsection{Линейные преобразования случайных векторов}
Рассмотрим всё тот же случайный вектор $\vec{\xi} = \left( \xi_1, \dots, \xi_n
\right)$ и произвольный константный вектор $\vec{\lambda} \in \mathbb{R}^n$.

Определим случайную величину $\eta$ как скалярное произведения векторов
$\vec{\xi}$ и $\vec{\lambda}$
$$\eta = \left( \vec{\xi}, \vec{\lambda} \right)$$

Посчитаем математическое ожидание случайной величины $\eta$.

$$\mean{\eta}
    = \mean{\sum_{k=1}^{n} \lambda_k \cdot \xi_k}
    = \sum_{k=1}^{n} \lambda_k \cdot \mean{\xi_k}
    = \left( \vec{\lambda}, \mean{\vec{\xi}} \right)$$

Теперь посчитаем дисперсию
$$\dispersion{\eta}
    = \mean{\left( \eta - \mean{\eta} \right)^2}
    = \mean{\left( \sum_{k=1}^{n} \lambda_k \cdot \xi_k
        - \lambda_k \cdot \mean{\xi_k} \right)^2}$$

Полученное выражение сворачивается в математическое ожидание квадрата суммы,
которая превращается в двойную сумму произведений
$$\mean{\left\{ \sum_{k=1}^{n} \lambda_k
    \cdot \left( \xi_k - \mean{\xi_k} \right) \right\}^2}
    = \sum_{i,j=1}^{n}\Mean{\left( \xi_i - \mean{\xi_i} \right)
            \cdot \left( \xi_j - \mean{\xi_j} \right)}
        \cdot \lambda_i \cdot \lambda_j$$

А это, как мы уже знаем, произведение ковариационной матрицы вектора $\vec{\xi}$
на вектор $\vec{\lambda}$, умноженное на тот же вектор $\vec{\lambda}$.
То есть, дисперсия $\eta$ выражается следующим образом
$$\dispersion{\eta}
    = \left( \dCov{\vec{\xi}} \cdot \vec{\lambda}, \vec{\lambda} \right)$$

Обобщим задачу и попробуем выяснить, каким образом зависит случайный вектор
$\vec{\eta}$, полученный путём линейных преобразованиямий вектора $\vec{\xi}$,
имеющего известное математическое ожидание и ковариационную матрицу.

Для линейных преобразований нужен оператор. Назовём его $T$.
Этот оператор будет действовать из пространства $\mathbb{R}^n$
в пространство $\mathbb{R}^m$, где $n$ --- размерность вектора $\vec{\xi}$,
а $m$ --- размерность вектора $\vec{\eta}$, который будет получен
в результате преобразования
$$\vec{\eta} = T \cdot \vec{\xi},\; T \in \mathbb{R}^{m \times n}$$
