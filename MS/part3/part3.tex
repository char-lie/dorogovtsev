\chapter{Метод наименьших квадратов}
Мы уже знаем, что нам не нужна вся выборка для построения хороших оценок ---
нам хватит достаточных статистик. Введя метод наименьших квадратов,
мы избавимся от неприятной процедуры вычисления интегралов.

\section{Гауссовские случайные вектора}

\subsection{Основные характеристики случайного вектора}

Есть $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- случайный вектор.
С функцией распределения $\cdf{\vec{\xi}}$ возникают проблемы (скучновато и
громоздко), поэтому будем использовать плотность распределения.

\begin{definition}[Плотность распределения случайного вектора]
    \index{плотность распределения!случайного вектора}
    \index{случайный вектор!плотность распределения}
    $p$ --- плотность распределения случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$, если
    \begin{enumerate}
        \item Вероятность того, что вектор $\vec{\xi}$ окажется
            в множестве $\Delta$, равна интегралу от плотности по этой области
            $$\Probability{\vec{\xi} \in \Delta}
                = \integrall{\Delta}{d\vec{u}}{\pdf{\vec{u}}}$$
        \item Во всех точках плотность неотрицательна
            $$\forall \vec{x} \in \mathbb{R}^n: \pdf{\vec{x}} \ge 0$$
        \item Выполняется условие нормировки
            $$\integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}}} = 1$$
    \end{enumerate}
\end{definition}

Естественным образом вводится определение характеристической функции.

\begin{definition}[Характеристическая функция случайного вектора]
    \index{характеристическая функция!случайного вектора}
    \index{случайный вектор!характеристическая функция}
    Значение характеристической функции случайного вектора $\vec{\xi}$
    в точке $\vec{\lambda}$ считается по формуле
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \mean{e^{i \cdot \left( \vec{\lambda}, \vec{\xi} \right)}}
        = \mean{
            \exp{\left\{ i \cdot \sum_{k=1}^n \lambda_k \cdot \xi_k \right\}}}$$

    Когда существует плотность, имеем преобразование Фурье
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}} \cdot
            e^{i \left( \vec{\lambda}, \vec{u} \right)}}$$
\end{definition}

\begin{definition}[Математическое ожидание случайного вектора]
    \index{математическое ожидание!случайного вектора}
    \index{случайный вектор!математическое ожидание}
    Математическое ожидание случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- вектор,
    элементы которого --- математические ожидания компонент
    случайного вектора $\vec{\xi}$
    $$\mean{\vec{\xi}} = \left( \mean{\xi_1}, \dots, \mean{\xi_n} \right)$$
\end{definition}

Но что же является дисперсией случайного вектора?

\subsection{Ковариационная матрица}

Начнём с определения ковариации двух случайных величин.

\begin{definition}[Ковариация]
    \index{ковариация}
    Ковариация двух случайных величин $\xi$ и $\eta$, принимающих действительные
    значения, обозначается $\cov{\xi, \eta}$ и считается по формуле
    $$\cov{\xi, \eta}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \eta - \mean{\eta} \right)}$$
\end{definition}

\begin{remark}
    Ковариация случайной величины $\xi$ с ней же --- её дисперсия
    $$\cov{\xi, \xi}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \xi - \mean{\xi} \right)}
        = \Mean{\left( \xi - \mean{\xi} \right)^2}
        = \dispersion{\xi}$$
\end{remark}

\begin{remark}
    Ковариация симметрична
    $$\cov{\xi, \eta}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \eta - \mean{\eta} \right)}
        = \Mean{\left( \eta - \mean{\eta} \right)
            \cdot \left( \xi - \mean{\xi} \right)}
        =\cov{\eta, \xi}$$
\end{remark}

\begin{definition}[Ковариационная матрица случайного вектора]
    \index{ковариационная матрица!случайного вектора}
    \index{ковариация!матрица}
    \index{матрица!ковариаций}
    \begin{comment}
    Ковариационная матрица двух случайных векторов
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ и
    $\vec{\eta} = \left( \eta_1, \dots, \eta_n \right)$ --- матрица, в которой
    на пересечении $i$ строки и $j$ столбца стоит ковариация случайных величин
    $\xi_i$ и $\eta_j$
    $$\Cov{\vec{\xi}}{\vec{\eta}}
        = \left\| \cov{\xi_i, \eta_j} \right\|_{i,j=1}^n
        = \left\| \mean{
            \left\{ \left( \xi_i - \mean{\xi_i} \right)
                \cdot \left( \eta_j - \mean{\eta_j} \right)
            \right\}} \right\|_{i,j=1}^n$$

    $$\Cov{\vec{\xi}}{\vec{\eta}} =
    \begin{bmatrix}
        \cov{\xi_1, \eta_1} & \cdots & \cov{\xi_1, \eta_n} \\
        \vdots & \ddots & \vdots \\
        \cov{\xi_n, \eta_1} & \cdots & \cov{\xi_n, \eta_n}
    \end{bmatrix}$$
    \end{comment}

    Ковариационная матрица случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- матрица, на пересечении
    $i$ строки и $j$ столбца которой находятся ковариации $i$ и $j$ элементов
    вектора $\xi$
    $$\dCov{\vec{\xi}}
        = \left\| \cov{\xi_i, \xi_j} \right\|_{i,j=1}^n
        = \left\| \mean{
            \left\{ \left( \xi_i - \mean{\xi_i} \right)
                \cdot \left( \xi_j - \mean{\xi_j} \right)
            \right\}} \right\|_{i,j=1}^n$$

    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        \cov{\xi_1, \xi_1} & \cdots & \cov{\xi_1, \xi_n} \\
        \vdots & \ddots & \vdots \\
        \cov{\xi_n, \xi_1} & \cdots & \cov{\xi_n, \xi_n}
    \end{bmatrix}$$

\end{definition}

\begin{remark}
    На диагонали ковариационной матрицы $\Cov{\vec{\xi}}{\vec{\xi}}$
    случайного вектора $\xi$ стоят дисперсии компонент вектора.
\end{remark}

Случайный ветор находится во многомерном пространстве, а это значит,
что имеется много направлений его размазывания, поэтому в качестве дисперсии
нам нужна матрица.

\begin{example}
    Возьмём двумерный вектор с одним и тем же элементом
    в каждой координате --- случайной величиной из стандартного нормального
    распределения
    $$\vec{\xi} = \left( \xi, \xi \right),\; \xi \sim N\left( 0, 1 \right)$$

    Нетрудно посчитать, что ковариационная матрица будет заполнена единицами,
    так как во всех ячейках будет ковариация $\cov{\xi, \xi}$, равная
    дисперсии случайной величины $\xi$, то есть, единице
    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        1 & 1 \\
        1 & 1
    \end{bmatrix}$$
\end{example}

\begin{example}
    Возьмём опять же двумерный вектор, но с двумя независимыми
    случайными величинами из стандартного нормального распределения
    $$\vec{\xi} = \left( \xi_1, \xi_2 \right),\;
        \xi_1, \xi_2 \sim N\left( 0, 1 \right)$$

    На диагонали будут стоять единицы --- дисперсии случайных величин.
    Если две случайные величины независимы, то их ковариация равна нулю
    \cite[с.~244]{Feller1}. Это в свою очередь означает, что вне диагонали
    будут нули
    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}$$
\end{example}

\subsection{Свойства ковариационной матрицы}
\index{ковариация!матрица!свойства}
\index{матрица!ковариаций!свойства}
\index{ковариационная матрица!свойства}

\begin{definition}[Сопряжённая матрица]
    \index{сопряжённая матрица}
    \index{матрица!сопряжённая}
    Есть матрица $A$ размером $n \times m$ с комплексными элементами.
    Тогда сопряжённая к ней матрица $A^*$ получается путём транспонирования
    матрицы $A$ и замены всех элементов на комплексно-сопряжённые
    \cite[с.~243]{VoevodinLA}, то есть
    $$\left( a_{i,j}^* = \overline{a_{j,i}} \right),\;
    A \in \mathbb{C}^{n \times m}, A^* \in \mathbb{C}^{m \times n}$$

    Или же в таком виде
    $$A =
    \begin{bmatrix}
        a_{1,1} & \cdots & a_{1,m} \\
        \vdots & \ddots & \vdots \\
        a_{n,1} & \cdots & a_{n,m}
    \end{bmatrix}
        \Rightarrow
    A^* = 
    \begin{bmatrix}
        \overline{a_{1,1}} & \cdots & \overline{a_{n,1}} \\
        \vdots & \ddots & \vdots \\
        \overline{a_{1,m}} & \cdots & \overline{a_{n,m}}
    \end{bmatrix}$$
\end{definition}

\begin{remark}
    Отметим, что к матрице с действительными коэффициентами сопряжённой будет
    транспонированная матрица
    $$A \in \mathbb{R}^{n \times m} \Rightarrow A^* = A^T$$
\end{remark}

\begin{enumerate}
    \item Симметричность. Ковариационная матрица случайного вектора $\vec{\xi}$
        равна своей сопряжённой
        $$\dCov{\vec{\xi}} = \dcCov{\vec{\xi}}$$
    \item Неотрицательная определённость
        (положительный оператор \cite[с.~139]{IlinPoznyarLA})
        $$\dCov{\vec{\xi}} \ge 0$$

        Это значит следующее
        $$\forall \vec{u} \in \mathbb{R}^n:\;
            \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
            = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
            \ge 0$$

        \begin{proof}
            Разобьём сумму на две --- сумма по диагональным элементам ($i=j=t$)
            и сумму по остальным элементам. Помним, что матрица симметричная,
            а это значит, что у нас будет удвоенная сумма элементов, которые
            находятся под (над) главной диагональю
            \begin{align*}
                \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                    = \sum_{t=1}^{n} \cov{\xi_t, \xi_t} \cdot u_t \cdot u_t + \\
                        + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
                            \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
            \end{align*}

            Распишем ковариации по определению и воспользуемся линейностью
            математического ожидания
            \begin{align*}
                \sum_{t=1}^{n} \cov{\xi_t, \xi_t} \cdot u_t \cdot u_t
                    + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
                        \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i = \\
                    = \sum_{t=1}^{n}
                            \mean{\left( \xi_t - \mean{\xi_t} \right)^2}
                            \cdot u_t^2 + \\
                        + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
                            \Mean{\left( \xi_i - \mean{\xi_i} \right)
                                \cdot \left( \xi_j - \mean{\xi_j} \right)}
                            \cdot u_j \cdot u_i = \\
                    = \Mean{\sum_{t=1}^{n}
                            \left( \xi_t - \mean{\xi_t} \right)^2 \cdot u_t^2
                        + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
                            \left( \xi_i - \mean{\xi_i} \right)
                                \cdot \left( \xi_j - \mean{\xi_j} \right)
                            \cdot u_j \cdot u_i}
            \end{align*}

            Видим, что есть сумма квадратов диагональных элементов
            и удвоенная сумма попарных произведений всех остальных элементов.
            Значит, эти суммы сворачиваются в квадрат суммы
            \begin{equation}\label{eq:sqrSumm}
                \begin{split}
                \sum_{i,j=1}^{n} \Mean{\left( \xi_i - \mean{\xi_i} \right)
                        \cdot \left( \xi_j - \mean{\xi_j} \right)}
                        \cdot u_j \cdot u_i =\\
                    = \mean{\left( \sum_{t=1}^{n} u_t
                        \cdot \left( \xi_t - \mean{\xi_t} \right) \right)^2}
                \end{split}
            \end{equation}

            Поскольку все коэффициенты действительные, а математическое
            ожидание константы равно самой константе, то делаем вывод,
            что сумма неотрицательна
            $$\sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                = \mean{\left( \sum_{t=1}^{n} u_t
                    \cdot \left( \xi_t - \mean{\xi_t} \right) \right)^2}
                \ge 0$$

            Вот мы и получили желаемый результат
            $$\forall \vec{u} \in \mathbb{R}^n:\;
                \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
                = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                \ge 0$$
        \end{proof}
\end{enumerate}

\begin{remark}
    Вспомним линейную алгебру.

    Самосопряжённая неотрицательно определённая матрица $\dCov{\vec{\xi}}$ имеет
    собственный ортонормированный базис, в котором она превращается в
    диагональную матрицу с неотрицательныи элементами
    $$\begin{bmatrix}
        \lambda_1 & & \mbox{\Huge{$\varnothing$}} \\
         & \ddots &  \\
         \mbox{\Huge{$\varnothing$}} & & \lambda_n
    \end{bmatrix},\; \lambda_k \ge 0$$

    Далее будем упускать символы пустоты $\varnothing$,
    подразумевая диагональные матрицы.

    Как эта матрица преобразует пространство?

    Единичная матрица не меняет ничего
    $$\begin{bmatrix}
        1 & &\\
        & \ddots & \\
        & & 1
    \end{bmatrix}$$

    Если первый элемент единичной матрицы сделать нулём, то такой оператор
    убивает первую координату вектора, на который подействует
    $$\begin{bmatrix}
        0 & & & \\
        & 1 & & \\
        & & \ddots & \\
        & & & 1
    \end{bmatrix}$$

    А такая матрица усиливает первую составляющую в десять раз и
    ослабляет остальные в десять раз
    $$\begin{bmatrix}
        10 & & &\\
        & 0.1 & & \\
        & & \ddots & \\
        & & & 0.1
    \end{bmatrix}$$

    Оказывается, через ковариационную матрицу вычисляются все характеристики
    линейных преобразований.
\end{remark}

\subsection{Линейные преобразования случайных векторов}
Рассмотрим всё тот же случайный вектор $\vec{\xi} = \left( \xi_1, \dots, \xi_n
\right)$ и произвольный константный вектор $\vec{\lambda} \in \mathbb{R}^n$.

Определим случайную величину $\eta$ как скалярное произведения векторов
$\vec{\xi}$ и $\vec{\lambda}$
$$\eta = \left( \vec{\xi}, \vec{\lambda} \right)$$

Посчитаем математическое ожидание случайной величины $\eta$.

$$\mean{\eta}
    = \mean{\sum_{k=1}^{n} \lambda_k \cdot \xi_k}
    = \sum_{k=1}^{n} \lambda_k \cdot \mean{\xi_k}
    = \left( \vec{\lambda}, \mean{\vec{\xi}} \right)$$

Теперь посчитаем дисперсию
$$\dispersion{\eta}
    = \mean{\left( \eta - \mean{\eta} \right)^2}
    = \mean{\left( \sum_{k=1}^{n} \lambda_k \cdot \xi_k
        - \lambda_k \cdot \mean{\xi_k} \right)^2}$$

Полученное выражение сворачивается в математическое ожидание квадрата суммы,
которая превращается в двойную сумму произведений
$$\mean{\left\{ \sum_{k=1}^{n} \lambda_k
    \cdot \left( \xi_k - \mean{\xi_k} \right) \right\}^2}
    = \sum_{i,j=1}^{n}\Mean{\left( \xi_i - \mean{\xi_i} \right)
            \cdot \left( \xi_j - \mean{\xi_j} \right)}
        \cdot \lambda_i \cdot \lambda_j$$

А это, как мы уже знаем из доказательства неотрицательной определённости
\eqref{eq:sqrSumm}, произведение ковариационной матрицы вектора $\vec{\xi}$
на вектор $\vec{\lambda}$, умноженное на тот же вектор $\vec{\lambda}$.
То есть, дисперсия $\eta$ выражается следующим образом
\begin{equation}\label{eq:linearQuadraticForm}
\dispersion{\eta}
    = \dispersion{\left( \vec{\xi}, \vec{\lambda} \right)}
    = \left( \dCov{\vec{\xi}} \cdot \vec{\lambda}, \vec{\lambda} \right)
\end{equation}

Обобщим задачу и попробуем выяснить, каким образом зависит случайный вектор
$\vec{\eta}$, полученный путём линейных преобразованиямий вектора $\vec{\xi}$,
имеющего известное математическое ожидание и ковариационную матрицу.

Для линейных преобразований вектора нужен линейный оператор. Назовём его $T$.
Этот оператор будет действовать из пространства $\mathbb{R}^n$
в пространство $\mathbb{R}^m$, где $n$ --- размерность вектора $\vec{\xi}$,
а $m$ --- размерность вектора $\vec{\eta}$, который будет получен
в результате преобразования
$$\vec{\eta} = T \cdot \vec{\xi},\; T \in \mathbb{R}^{m \times n}$$

Посчитаем математическое ожидание
$$\mean{\eta} = \Mean{T \cdot \vec{\xi}}$$

Очевидно, что в связи с линейностью математического ожидания можно вынести
оператор $T$ наружу.

Мы всё-таки проделаем математические выкладки по-честному.
Итак, у нас есть математическое ожидание случайного вектора
$$\Mean{T \cdot \vec{\xi}}
    = \mean{\left\| \sum_{j=1}^n \left( t_{i,j} \cdot \xi_j \right)
        \right\|_{i=1}^m}$$

Математическое ожидание случайного вектора --- вектор математических ожиданий
соответствующих координат.
Дальше воспользуемся линейностью математического ожидания
$$\mean{\left\| \sum_{j=1}^n \left( t_{i,j} \cdot \xi_j \right)
        \right\|_{i=1}^m}
    = \left\| \Mean{\sum_{j=1}^n \left( t_{i,j} \cdot \xi_j \right)}
        \right\|_{i=1}^m
    = \left\| \sum_{j=1}^n \left( t_{i,j} \cdot \mean{\xi_j} \right)
        \right\|_{i=1}^m$$

Видим, что перед нами произведение матрицы $T$ на вектор
математических ожиданий координат случайного вектора $\vec{\xi}$
$$\left\| \sum_{j=1}^n \left( t_{i,j} \cdot \mean{\xi_j} \right)
    \right\|_{i=1}^m = T \cdot \mean{\vec{\xi}}$$

То есть, интуиция нам подсказывала правильно и конечная формула такова
$$\mean{\eta} = \Mean{T \cdot \vec{\xi}} = T \cdot \mean{\vec{\xi}}$$

Теперь нужно посчитать ковариацию. Мы могли бы решать эту задачу,
расписав произведение матрицы, но в этот раз, пожалуй, освежим наши знания
в линейной алгебре

Возьмём произвольный вектор $\vec{e} \in \mathbb{R}^n$
и выпишем квадратичную форму ковариационной матрицы вектора $\eta$
с аргументом $\vec{e}$. Из начала подраздела \eqref{eq:linearQuadraticForm}
помним, что такая квадратичная форма равна дисперсии скалярного произведения, а
дальше воспользуемся свойством симметричности скалярного произведения
(для удобства дальнейших вычислений)
$$\left( \dCov{\vec{\eta}} \cdot \vec{e}, \vec{e} \right)
    = \dispersion{\left( \vec{\eta}, \vec{e} \right)}
    = \dispersion{\left( \vec{e}, \vec{\eta} \right)}$$

Распишем наш случайный вектор $\vec{\eta}$ через случайный вектор $\vec{\xi}$
и матрицу $T$
$$\dispersion{\left( \vec{e}, \vec{\eta} \right)}
    = \dispersion{\left( \vec{e}, T \cdot \vec{\xi} \right)}$$

Далее воспользуемся ещё одним определением сопряжённого оператора\footnote{На
самом деле, это и есть изначальное определение сопряжённого оператора
\cite[с.~241]{VoevodinLA}, \cite[с.~126]{IlinPoznyarLA}}
и перенесём оператор $T$ в левую часть скалярного произведения
$$\dispersion{\left( \vec{e}, T \cdot \vec{\xi} \right)}
    = \dispersion{\left( T^* \cdot \vec{e}, \vec{\xi} \right)}$$

Перейдём от дисперсии к квадратичной форме и посмотрим, что происходит
$$\dispersion{\left( \operatorOn{T^*}{\vec{e}}, \vec{\xi} \right)}
    = \left( \operatorOn{\dCov{\vec{\xi}}}{\operatorOn{T^*}{\vec{e}}},
        \operatorOn{T^*}{\vec{e}} \right)$$

Снова воспользуемся определением сопряжённого оператора и перенесём его
из правой стороны скалярного произведения в левую. Не забываем, что
сопряжённый оператор сопряжённого оператора --- исходный оператор
$\left( \operator{T^*} \right)^* = \operator{T}$
$$\left( \operatorOn{\dCov{\vec{\xi}}}{\operatorOn{T^*}{\vec{e}}},
        \operatorOn{T^*}{\vec{e}} \right)
    = \left( \operatorOn{T}{
            \operatorOn{\dCov{\vec{\xi}}}{\operatorOn{T^*}{\vec{e}}}},
        \vec{e} \right)$$

Видим, что квадратичные формы совпадают, а это значит, что и операторы равны
$$\left( \operatorOn{T}{
    \operatorOn{\dCov{\vec{\xi}}}{\operatorOn{T^*}{\vec{e}}}}, \vec{e} \right)
        = \left( \dCov{\vec{\eta}} \cdot \vec{e}, \vec{e} \right)
    \Rightarrow
    \operatorOn{T}{\operatorOn{\dCov{\vec{\xi}}}{\operator{T^*}}}
        = \dCov{\vec{\eta}}$$

Подведём итоги: если на случайный вектор $\vec{\xi}$ с известным математическим
ожиданием и ковариационной матрицей подействовать оператором $\operator{T}$,
то математическое ожидание полученного вектора будет считаться по формуле
$$\mean{\operatorOn{T}{\vec{\xi}}} = \operatorOn{T}{\mean{\vec{\xi}}}$$

Расчёт ковариационной матрицы происходит в базисе вектора $\vec{\xi}$
с матрицей перехода $\operator{T}$ и матрицей $\operator{T^*}$
для перехода обратно
$$\dCov{\operatorOn{T}{\vec{\xi}}}
    = \operatorOn{T}{\operatorOn{\dCov{\vec{\xi}}}{\operator{T^*}}}$$

\subsection{Гауссовские случайные вектора}
