\chapter{Случайные вектора}

Мы уже знаем, что нам не нужна вся выборка для построения хороших оценок ---
нам хватит достаточных статистик. Введя метод наименьших квадратов,
мы избавимся от неприятной процедуры вычисления интегралов.

Тем не менее, чтобы перейти непосредственно к изучению метода, необходимо
владеть инструментарием, коим являются случайные вектора.

\section{Основные характеристики случайного вектора}

\index{вектор!случайный}
Есть $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- случайный вектор.
С функцией распределения $\cdf{\vec{\xi}}$ возникают проблемы (скучновато и
громоздко), поэтому будем использовать плотность распределения.

\begin{definition}[Плотность распределения случайного вектора]
    \index{плотность распределения!случайного вектора}
    \index{случайный вектор!плотность распределения}
    $p$ --- плотность распределения случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$, если
    \begin{enumerate}
        \item Вероятность того, что вектор $\vec{\xi}$ окажется
            в множестве $\Delta$, равна интегралу от плотности по этой области
            $$\Probability{\vec{\xi} \in \Delta}
                = \integrall{\Delta}{d\vec{u}}{\pdf{\vec{u}}}$$
        \item Во всех точках плотность неотрицательна
            $$\forall \vec{x} \in \mathbb{R}^n: \pdf{\vec{x}} \ge 0$$
        \item Выполняется условие нормировки
            $$\integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}}} = 1$$
    \end{enumerate}
\end{definition}

Естественным образом вводится определение характеристической функции.

\begin{definition}[Характеристическая функция случайного вектора]
    \index{характеристическая функция!случайного вектора}
    \index{случайный вектор!характеристическая функция}
    Значение характеристической функции случайного вектора $\vec{\xi}$
    в точке $\vec{\lambda}$ считается по формуле
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \mean{e^{i \cdot \left( \vec{\lambda}, \vec{\xi} \right)}}
        = \mean{
            \exp{\left\{ i \cdot \sum_{k=1}^n \lambda_k \cdot \xi_k \right\}}}$$

    Когда существует плотность, имеем преобразование Фурье
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \integrall{\mathbb{R}^n}{d\vec{u}}{\pdf{\vec{u}} \cdot
            e^{i \left( \vec{\lambda}, \vec{u} \right)}}$$
\end{definition}

\begin{definition}[Математическое ожидание случайного вектора]
    \index{математическое ожидание!случайного вектора}
    \index{случайный вектор!математическое ожидание}
    Математическое ожидание случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- вектор,
    элементы которого --- математические ожидания компонент
    случайного вектора $\vec{\xi}$
    $$\mean{\vec{\xi}} = \left( \mean{\xi_1}, \dots, \mean{\xi_n} \right)$$
\end{definition}

Но что же является дисперсией случайного вектора?

\subsection{Ковариационная матрица случайного вектора}

Начнём с определения ковариации двух случайных величин.

\begin{definition}[Ковариация]
    \index{ковариация}
    Ковариация двух случайных величин $\xi$ и $\eta$, принимающих действительные
    значения, обозначается $\cov{\xi, \eta}$ и считается по формуле
    $$\cov{\xi, \eta}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \eta - \mean{\eta} \right)}$$
\end{definition}

\begin{remark}
    Ковариация случайной величины $\xi$ с ней же --- её дисперсия
    $$\cov{\xi, \xi}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \xi - \mean{\xi} \right)}
        = \Mean{\left( \xi - \mean{\xi} \right)^2}
        = \dispersion{\xi}$$
\end{remark}

\begin{remark}
    Ковариация симметрична
    $$\cov{\xi, \eta}
        = \Mean{\left( \xi - \mean{\xi} \right)
            \cdot \left( \eta - \mean{\eta} \right)}
        = \Mean{\left( \eta - \mean{\eta} \right)
            \cdot \left( \xi - \mean{\xi} \right)}
        =\cov{\eta, \xi}$$
\end{remark}

\begin{remark}\label{rem:covIndepentent}
    Ковариация двух независимых случайных величин равна нулю
    \cite[с.~244]{Feller1}
\end{remark}

\begin{definition}[Ковариационная матрица случайного вектора]
    \label{def:vectorCovMatrix}
    \index{ковариационная матрица!случайного вектора}

    Ковариационная матрица случайного вектора
    $\vec{\xi} = \left( \xi_1, \dots, \xi_n \right)$ --- матрица, на пересечении
    $i$ строки и $j$ столбца которой находятся ковариации $i$ и $j$ элементов
    вектора $\xi$
    $$\dCov{\vec{\xi}}
        = \left\| \cov{\xi_i, \xi_j} \right\|_{i,j=1}^n
        = \left\| \mean{
            \left\{ \left( \xi_i - \mean{\xi_i} \right)
                \cdot \left( \xi_j - \mean{\xi_j} \right)
            \right\}} \right\|_{i,j=1}^n$$

    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        \cov{\xi_1, \xi_1} & \cdots & \cov{\xi_1, \xi_n} \\
        \vdots & \ddots & \vdots \\
        \cov{\xi_n, \xi_1} & \cdots & \cov{\xi_n, \xi_n}
    \end{bmatrix}$$

\end{definition}

\begin{remark}
    На диагонали ковариационной матрицы $\dCov{\vec{\xi}}$
    случайного вектора $\xi$ стоят дисперсии компонент вектора.
\end{remark}

Случайный ветор находится во многомерном пространстве, а это значит,
что имеется много направлений его размазывания, поэтому в качестве дисперсии
нам нужна матрица.

\begin{example}
    Возьмём двумерный вектор с одним и тем же элементом
    в каждой координате --- случайной величиной из стандартного нормального
    распределения
    $$\vec{\xi} = \left( \xi, \xi \right),\; \xi \sim N\left( 0, 1 \right)$$

    Нетрудно посчитать, что ковариационная матрица будет заполнена единицами,
    так как во всех ячейках будет ковариация $\cov{\xi, \xi}$, равная
    дисперсии случайной величины $\xi$, то есть, единице
    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        1 & 1 \\
        1 & 1
    \end{bmatrix}$$
\end{example}

\begin{example}
    Возьмём опять же двумерный вектор, но с двумя независимыми
    случайными величинами из стандартного нормального распределения
    $$\vec{\xi} = \left( \xi_1, \xi_2 \right),\;
        \xi_1, \xi_2 \sim N\left( 0, 1 \right)$$

    На диагонали будут стоять единицы --- дисперсии случайных величин.
    Если две случайные величины независимы, то их ковариация равна нулю
    (замечание \ref{rem:covIndepentent}).
    Это в свою очередь означает, что вне диагонали
    будут нули
    $$\dCov{\vec{\xi}} =
    \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}$$
\end{example}

\begin{definition}[Сопряжённая матрица]
    \index{сопряжённая матрица}
    \index{матрица!сопряжённая}
    Есть матрица $A$ размером $n \times m$ с комплексными элементами.
    Тогда сопряжённая к ней матрица $A^*$ получается путём транспонирования
    матрицы $A$ и замены всех элементов на комплексно-сопряжённые
    \cite[с.~243]{VoevodinLA}, то есть
    $$\left( a_{i,j}^* = \overline{a_{j,i}} \right),\;
    A \in \mathbb{C}^{n \times m}, A^* \in \mathbb{C}^{m \times n}$$

    Или же в таком виде
    $$A =
    \begin{bmatrix}
        a_{1,1} & \cdots & a_{1,m} \\
        \vdots & \ddots & \vdots \\
        a_{n,1} & \cdots & a_{n,m}
    \end{bmatrix}
        \Rightarrow
    A^* = 
    \begin{bmatrix}
        \overline{a_{1,1}} & \cdots & \overline{a_{n,1}} \\
        \vdots & \ddots & \vdots \\
        \overline{a_{1,m}} & \cdots & \overline{a_{n,m}}
    \end{bmatrix}$$
\end{definition}

\begin{remark}
    Отметим, что к матрице с действительными коэффициентами сопряжённой будет
    транспонированная матрица
    $$A \in \mathbb{R}^{n \times m} \Rightarrow A^* = A^T$$
\end{remark}

Чтобы не разрывать целостность дальнейших повествований, введём наперёд
небольшое утверждение. Точнее, просто вспомним комбинаторику.
\begin{affirmation}\label{affirmation:squaredSum}
    Квадрат суммы раскладывается в двойную сумму следующим образом
    $$\left( \sum_{k=1}^n x_k \right)^2 = \sum_{i, j = 1}^{n} x_i \cdot x_j$$
\end{affirmation}
\begin{proof}
    Чтобы убедиться в правильности формулы, вспомним мультиномиальные
    коэффициенты --- их значение и определение.

    Мультиномиальные коэффициенты --- множители при слагаемых
    $x_1^{m_1} \cdot x_n^{m_n}$ после разложения
    $\left( x_1 + \dots + x_n \right)^m$ в сумму и считаются по следующей
    формуле \cite[с.~28]{Grimaldi}
    \begin{align*}
        {m_1, \dots, m_n \choose m} = \frac{m!}{m_1! \cdots m_n!} \\
            0 \le m_1, \dots, m_n \le m,\; m_1 + \dots + m_n = m
    \end{align*}

    То есть, вот общая формула раскрытия натуральной степени $m$ произвольной
    суммы выглядит так
    $$\left( x_1 + \dots + x_n \right)^m
        = \sum_{
                \substack{m_1 + \dots + m_n = m \\
                m_1, \dots, m_n \ge 0}}
            {m_1, \dots, m_n \choose m} \cdot x_1^{m_1} \cdots x_n^{m_n}$$

    Теперь вернёмся к нашему частному случаю: $m=2$. Тогда мультиномиальные
    коэффициенты будут иметь следующий вид
    \begin{align*}
    {m_1, \dots, m_n \choose 2} = \frac{2}{m_1! \cdots m_n!} \\
        0 \le m_1, \dots, m_n \le 2,\; m_1 + \dots + m_n = 2
    \end{align*}

    Из накладываемых ограничений видно, что в знаменателе будет либо одна
    двойка, либо две единицы, так как сумма должна равняться двойке.

    Таким образом, сумму можно разбить на две части --- квадраты ($m_k = 2$)
    и попарные произведения ($m_i \cdot m_j = 1,\; i \neq j$). Запишем
    \begin{equation}\label{eq:squaredSumStart}
        \begin{split}
            \left( x_1 + \dots + x_n \right)^2
                = \sum_{k=1}^{n} \frac{2}{2} \cdot x_k^2
                    + \sum_{i=1}^{n-1}
                        \sum_{j=i+1}^n \frac{2}{1} \cdot x_i \cdot x_j = \\
                = \sum_{k=1}^{n} x_k^2
                    + 2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
        \end{split}
    \end{equation}

    В связи с коммутативностью умножения последнюю удвоенную двойную сумму можно
    раскрыть как сумму по всем недиагональным элементам
    \begin{align*}
        2 \cdot \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
            = \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
                + \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j = \\
    \end{align*}
    \begin{align*}
            \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
                + \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j = \\
            \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
                + \sum_{j=1}^{n-1} \sum_{i=i+1}^n x_j \cdot x_i = \\
            = \sum_{i=1}^{n-1} \sum_{j=i+1}^n x_i \cdot x_j
                + \sum_{i=2}^{n} \sum_{j=1}^{i-1} x_j \cdot x_i
            = \sum_{i \neq j}^{n} x_i \cdot x_j
    \end{align*}

    Вместе с суммой квадратов диагональных элементов получится сумма по всем
    произведением. Перепишем, во что превратится формула
    \eqref{eq:squaredSumStart}
    \begin{align*}
         \left( x_1 + \dots + x_n \right)^2
            = \sum_{k=1}^{n} x_k^2
                + 2 \cdot \sum_{i=1}^{n-1}
                    \sum_{j=i+1}^n \cdot x_i \cdot x_j = \\
            = \sum_{k=1}^{n} x_k^2 + \sum_{i \neq j}^{n} x_i \cdot x_j
            = \sum_{i, j = 1}^{n} x_i \cdot x_j
    \end{align*}

    То есть, действительно квадрат суммы равен сумме попарных произведений всех
    элементов, что и требовалось доказать
    $$\left( \sum_{k=1}^n x_k \right)^2 = \sum_{i, j = 1}^{n} x_i \cdot x_j$$
\end{proof}
\begin{proof}[Простое доказательство]
    Также можно доказать это утверждение, просто расписав квадрат как
    произведение
    \begin{align*}
        \left( \sum_{k=1}^{n} x_k \right)^2
        = \left( x_1 + \dots + x_n \right)
            \cdot \left( x_1 + \dots + x_n \right) = \\
        = x_1 \cdot x_1 + x_1 \cdot x_2 + \dots + x_1 \cdot x_n
            + x_2 \cdot x_1 + x_2 \cdot x_2 + \dots + x_n \cdot x_n
    \end{align*}

    Видим, что каждый элемент умножается с каждым, и всё это дело суммируется.
    Запишем в виде суммы (с красивым значком сигма)
    \begin{align*}
        \left( \sum_{k=1}^{n} x_k \right)^2
        = \left( x_1 + \dots + x_n \right)
            \cdot \left( x_1 + \dots + x_n \right) = \\
        = \sum_{i=1}^{n}
            \left( x_i \cdot x_1 + x_i \cdot x_2 + \dots + x_i \cdot x_n \right)
        = \sum_{i=1}^{n} \sum_{j=1}^{n} x_i \cdot x_j
    \end{align*}

    Что и требовалось доказать.
\end{proof}

Теперь мы готовы перейти к свойствам ковариационной матрицы
\begin{enumerate}
\index{ковариационная матрица!свойства}
    \item Симметричность. Ковариационная матрица случайного вектора $\vec{\xi}$
        равна своей сопряжённой
        $$\dCov{\vec{\xi}} = \dcCov{\vec{\xi}}$$
    \item Неотрицательная определённость\footnote{Больше о неотрицательно
            определённых операторах можно почитать в книге Ильина и Позняка
            ``Линейная алгебра'' \cite[с.~139]{IlinPoznyarLA}.
            В ней такой оператор называется положительным.}
        $$\dCov{\vec{\xi}} \ge 0$$

        Это значит следующее
        $$\forall \vec{u} \in \mathbb{R}^n:\;
            \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
            = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
            \ge 0$$

        \begin{proof}
            Распишем ковариацию по определению и воспользуемся утверждением
            \ref{affirmation:squaredSum}
            \begin{align*}
                \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i = \\
                = \sum_{i,j=1}^{n} \Mean{\left( \xi_i - \mean{\xi_i} \right)
                        \cdot \left( \xi_j - \mean{\xi_j} \right)}
                        \cdot u_j \cdot u_i =\\
                = \mean{\left( \sum_{t=1}^{n} u_t
                    \cdot \left( \xi_t - \mean{\xi_t} \right) \right)^2}
            \end{align*}

            Поскольку все коэффициенты действительные, а математическое
            ожидание константы равно самой константе, то делаем вывод,
            что сумма неотрицательна
            $$\sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                = \mean{\left( \sum_{t=1}^{n} u_t
                    \cdot \left( \xi_t - \mean{\xi_t} \right) \right)^2}
                \ge 0$$

            Вот мы и получили желаемый результат
            $$\forall \vec{u} \in \mathbb{R}^n:\;
                \left( \dCov{\vec{\xi}} \cdot \vec{u}, \vec{u} \right)
                = \sum_{i,j=1}^{n} \cov{\xi_i, \xi_j} \cdot u_j \cdot u_i
                \ge 0$$
        \end{proof}
\end{enumerate}

\begin{remark}\label{remark:linearAlgebra:selfAdjointMatrix}
    Вспомним линейную алгебру.

    Самосопряжённая неотрицательно определённая матрица $\dCov{\vec{\xi}}$ имеет
    собственный ортонормированный базис, в котором она превращается в
    диагональную матрицу с неотрицательныи элементами
    $$\begin{bmatrix}
        \lambda_1 & & \mbox{\Huge{$\varnothing$}} \\
         & \ddots &  \\
         \mbox{\Huge{$\varnothing$}} & & \lambda_n
    \end{bmatrix},\; \lambda_k \ge 0$$

    Далее будем упускать символы пустоты $\varnothing$,
    подразумевая диагональные матрицы.

    Как эта матрица преобразует пространство?

    Единичная матрица не меняет ничего
    $$\begin{bmatrix}
        1 & &\\
        & \ddots & \\
        & & 1
    \end{bmatrix}$$

    Если первый элемент единичной матрицы сделать нулём, то такой оператор
    убивает первую координату вектора, на который подействует
    $$\begin{bmatrix}
        0 & & & \\
        & 1 & & \\
        & & \ddots & \\
        & & & 1
    \end{bmatrix}$$

    А такая матрица усиливает первую составляющую в десять раз и
    ослабляет остальные в десять раз
    $$\begin{bmatrix}
        10 & & &\\
        & 0.1 & & \\
        & & \ddots & \\
        & & & 0.1
    \end{bmatrix}$$

    Оказывается, через ковариационную матрицу вычисляются все характеристики
    линейных преобразований.
\end{remark}

\subsection{Ковариационная матрица}
Логичным обобщением ковариационной матрицы случайного вектора является
ковариационная матрица двух случайных векторов. Сейчас станет ясно, зачем мы
дважды писали вектор $\vec{\xi}$ в индексе оператора $\dCov{\vec{\xi}}$.

\begin{definition}[Ковариационная матрица]\label{def:covMatrix}
    \index{ковариационная матрица}
    Ковариационная матрица двух случайных векторов
    $\vec{\alpha} = \left( \alpha_1, \dots, \alpha_n \right)$ и
    $\vec{\beta} = \left( \beta_1, \dots, \beta_m \right)$ --- матрица,
    в которой на пересечении $i$ строки и $j$ столбца стоит ковариация случайных
    величин $\alpha_i$ и $\beta_j$
    $$\Cov{\vec{\alpha}}{\vec{\beta}}
        = \left\| \cov{\alpha_i, \beta_j} \right\|_{
            \substack{i=\overline{1,n},\\j=\overline{1,m}}}
        = \left\| \mean{
            \left\{ \left( \alpha_i - \mean{\alpha_i} \right)
                \cdot \left( \beta_j - \mean{\beta_j} \right)
            \right\}} \right\|_{
                \substack{i=\overline{1,n},\\j=\overline{1,m}}}$$

    $$\Cov{\vec{\alpha}}{\vec{\beta}} =
    \begin{bmatrix}
        \cov{\alpha_1, \beta_1} & \cdots & \cov{\alpha_1, \beta_m} \\
        \vdots & \ddots & \vdots \\
        \cov{\alpha_n, \beta_1} & \cdots & \cov{\alpha_n, \beta_m}
    \end{bmatrix}$$
\end{definition}

Ковариационная матрица обладает следующими свойствами
\index{ковариационная матрица!свойства}
\begin{enumerate}
    \item $\Cov{\beta}{\alpha} = \tCov{\alpha}{\beta}$
    \item $\Cov{\alpha_1 + \alpha_2}{\beta}
        = \Cov{\alpha_1}{\beta} + \Cov{\alpha_2}{\beta}$
    \item $\Cov{\operatorname{B} \alpha}{\beta}
        = \operatorname{B} \Cov{\alpha}{\beta}$
    \item $\Cov{\alpha}{\operatorname{D} \beta}
        = \Cov{\alpha}{\beta} \operatorname{D^T}$
    \item $\dCov{\operatorname{B} \vec{\xi}}
        = \operatorname{B} \dCov{\vec{\xi}} \operatorname{B^T}$
\end{enumerate}



\section{Линейные преобразования случайных векторов}
\label{section:linearTransformations}
Рассмотрим всё тот же случайный вектор $\vec{\xi} = \left( \xi_1, \dots, \xi_n
\right)$ и произвольный константный вектор $\vec{\lambda} \in \mathbb{R}^n$.

Определим случайную величину $\eta$ как скалярное произведения векторов
$\vec{\xi}$ и $\vec{\lambda}$
$$\eta = \left( \vec{\xi}, \vec{\lambda} \right)$$

Посчитаем математическое ожидание случайной величины $\eta$.

\begin{equation}\label{eq:scalarMulMean}
    \mean{\eta}
        = \mean{\sum_{k=1}^{n} \lambda_k \cdot \xi_k}
        = \sum_{k=1}^{n} \lambda_k \cdot \mean{\xi_k}
        = \left( \vec{\lambda}, \mean{\vec{\xi}} \right)
\end{equation}

Теперь посчитаем дисперсию
$$\dispersion{\eta}
    = \mean{\left( \eta - \mean{\eta} \right)^2}
    = \mean{\left( \sum_{k=1}^{n} \lambda_k \cdot \xi_k
        - \lambda_k \cdot \mean{\xi_k} \right)^2}$$

Полученное выражение сворачивается в математическое ожидание квадрата суммы,
которая превращается в двойную сумму произведений
$$\mean{\left\{ \sum_{k=1}^{n} \lambda_k
    \cdot \left( \xi_k - \mean{\xi_k} \right) \right\}^2}
    = \sum_{i,j=1}^{n}\Mean{\left( \xi_i - \mean{\xi_i} \right)
            \cdot \left( \xi_j - \mean{\xi_j} \right)}
        \cdot \lambda_i \cdot \lambda_j$$

А это, как мы уже знаем из утверждения \ref{affirmation:squaredSum},
произведение ковариационной матрицы вектора $\vec{\xi}$
на вектор $\vec{\lambda}$, умноженное на тот же вектор $\vec{\lambda}$.
То есть, дисперсия $\eta$ выражается следующим образом
\begin{equation}\label{eq:linearQuadraticForm}
\dispersion{\eta}
    = \dispersion{\left( \vec{\xi}, \vec{\lambda} \right)}
    = \left( \dCov{\vec{\xi}} \cdot \vec{\lambda}, \vec{\lambda} \right)
\end{equation}

Обобщим задачу и попробуем выяснить, каким образом зависит случайный вектор
$\vec{\eta}$, полученный путём линейных преобразованиямий вектора $\vec{\xi}$,
имеющего известное математическое ожидание и ковариационную матрицу.

Для линейных преобразований вектора нужен линейный оператор. Назовём его
$\operatorname{T}$. Этот оператор будет действовать из пространства
$\mathbb{R}^n$
в пространство $\mathbb{R}^m$, где $n$ --- размерность вектора $\vec{\xi}$,
а $m$ --- размерность вектора $\vec{\eta}$, который будет получен
в результате преобразования
$$\vec{\eta} = \operatorname{T} \vec{\xi} ,\; T \in \mathbb{R}^{m \times n}$$

Посчитаем математическое ожидание
$$\mean{\eta} = \Mean{\operatorname{T} \vec{\xi} }$$

Очевидно, что в связи с линейностью математического ожидания можно вынести
оператор $\operatorname{T}$ наружу.

Мы всё-таки проделаем математические выкладки по-честному.
Итак, у нас есть математическое ожидание случайного вектора
$$\Mean{\operatorname{T} \vec{\xi} }
    = \mean{\left\| \sum_{j=1}^n \left( T_{i,j} \cdot \xi_j \right)
        \right\|_{i=1}^m}$$

Математическое ожидание случайного вектора --- вектор математических ожиданий
соответствующих координат.
Дальше воспользуемся линейностью математического ожидания
$$\mean{\left\| \sum_{j=1}^n \left( T_{i,j} \cdot \xi_j \right)
        \right\|_{i=1}^m}
    = \left\| \Mean{\sum_{j=1}^n \left( T_{i,j} \cdot \xi_j \right)}
        \right\|_{i=1}^m
    = \left\| \sum_{j=1}^n \left( T_{i,j} \cdot \mean{\xi_j} \right)
        \right\|_{i=1}^m$$

Видим, что перед нами произведение матрицы $\operatorname{T}$ на вектор
математических ожиданий координат случайного вектора $\vec{\xi}$
$$\left\| \sum_{j=1}^n \left( T_{i,j} \cdot \mean{\xi_j} \right)
    \right\|_{i=1}^m = \operatorname{T} \mean{\vec{\xi}} $$

То есть, интуиция нам подсказывала правильно и конечная формула такова
$$\mean{\eta}
    = \Mean{\operatorname{T} \vec{\xi} }
    = \operatorname{T} \mean{\vec{\xi}} $$

Теперь нужно посчитать ковариацию. Мы могли бы решать эту задачу,
расписав произведение матрицы, но в этот раз, пожалуй, освежим наши знания
в линейной алгебре

Возьмём произвольный вектор $\vec{e} \in \mathbb{R}^n$
и выпишем квадратичную форму ковариационной матрицы вектора $\eta$
с аргументом $\vec{e}$. Из начала подраздела \eqref{eq:linearQuadraticForm}
помним, что такая квадратичная форма равна дисперсии скалярного произведения, а
дальше воспользуемся свойством симметричности скалярного произведения
(для удобства дальнейших вычислений)
$$\left( \dCov{\vec{\eta}} \cdot \vec{e}, \vec{e} \right)
    = \dispersion{\left( \vec{\eta}, \vec{e} \right)}
    = \dispersion{\left( \vec{e}, \vec{\eta} \right)}$$

Распишем наш случайный вектор $\vec{\eta}$ через случайный вектор $\vec{\xi}$
и матрицу $\operatorname{T}$
$$\dispersion{\left( \vec{e}, \vec{\eta} \right)}
    = \dispersion{\left( \vec{e}, \operatorname{T} \vec{\xi} \right)}$$

Далее воспользуемся ещё одним определением сопряжённого оператора\footnote{На
самом деле, это и есть изначальное определение сопряжённого оператора
\cite[с.~241]{VoevodinLA}, \cite[с.~126]{IlinPoznyarLA}}
и перенесём оператор $\operatorname{T}$ в левую часть скалярного произведения
$$\dispersion{\left( \vec{e}, \operatorname{T} \vec{\xi} \right)}
    = \dispersion{\left( \operatorname{T^*} \vec{e} , \vec{\xi} \right)}$$

Перейдём от дисперсии к квадратичной форме и посмотрим, что происходит
$$\dispersion{\left( \operatorname{T^*} \vec{e} , \vec{\xi} \right)}
    = \left( \operatorname{\dCov{\vec{\xi}}} \operatorname{T^*} \vec{e} ,
        \operatorname{T^*} \vec{e} \right)$$

Снова воспользуемся определением сопряжённого оператора и перенесём его
из правой стороны скалярного произведения в левую. Не забываем, что
сопряжённый оператор к сопряжённму оператору --- исходный оператор
$\left( \operatorname{T^*} \right)^* = \operatorname{T}$
$$\left( \operatorname{\dCov{\vec{\xi}}} \operatorname{T^*} \vec{e} ,
        \operatorname{T^*} \vec{e} \right)
    = \left( \operatorname{T}[
            \operatorname{\dCov{\vec{\xi}}} \operatorname{T^*} \vec{e}] ,
        \vec{e} \right)$$

Видим, что квадратичные формы совпадают, а это значит, что и операторы равны
$$\left( \operatorname{T}[
    \operatorname{\dCov{\vec{\xi}}} \operatorname{T^*} \vec{e}] , \vec{e} \right)
        = \left( \dCov{\vec{\eta}} \cdot \vec{e}, \vec{e} \right)
    \Rightarrow
    \operatorname{T} \operatorname{\dCov{\vec{\xi}}} \operatorname{T^*} 
        = \dCov{\vec{\eta}}$$

Подведём итоги: если на случайный вектор $\vec{\xi}$ с известным математическим
ожиданием и ковариационной матрицей подействовать оператором $\operatorname{T}$,
то математическое ожидание полученного вектора будет считаться по формуле
$$\mean{\operatorname{T} \vec{\xi}]} = \operatorname{T}[\mean{\vec{\xi}} $$

Расчёт ковариационной матрицы происходит в базисе вектора $\vec{\xi}$
с матрицей перехода $\operatorname{T}$ и матрицей $\operatorname{T^*}$
для перехода обратно
$$\dCov{\operatorname{T} \vec{\xi} }
    = \operatorname{T} \operatorname{\dCov{\vec{\xi}}} \operatorname{T^*} $$

\section{Гауссовские случайные вектора}

\subsection{Определения}
\begin{definition}[Гауссовский случайный вектор (определение Максвелла)]
    \label{def:gaussianVector}
    Случайный вектор $\vec{\xi}$ в $\mathbb{R}^n$ называется гауссовским, если
    его проекция на произвольный вектор из пространства $\mathbb{R}^n$
    является гауссовской случайной величиной
    $$\forall \vec{\lambda} \in \mathbb{R}^n:
        \left( \vec{\lambda}, \vec{\xi} \right)
            \sim N\left( a_{\vec{\lambda}}, \sigma_{\vec{\lambda}}^2 \right)$$
\end{definition}

\begin{example}\label{ex:gaussVectorsIntro}
    Возьмём случайный вектор $\vec{\xi}$, координаты которого между собой равны
    и являются гауссовской случайной величиной
    $$\vec{\xi} = \left( \xi, \dots, \xi \right),\;
        \xi \sim N\left( a, \sigma^2 \right)$$

    Очевидно, что математическое ожидание --- вектор
    $\left( a, \dots, a \right)$, а ковариационная матрица состоит из
    $\sigma^2$, так как на каждом месте стоит дисперсия случайной величины $\xi$
    $$\mean{\vec{\xi}} = \left( a, \dots, a \right),\;
        \dCov{\vec{\xi}} =
        \begin{bmatrix}
            \sigma^2 & \cdots & \sigma^2 \\
            \vdots & \ddots & \vdots \\
            \sigma^2 & \cdots & \sigma^2
        \end{bmatrix}$$

    Проверим, является ли вектор $\vec{\xi}$ гауссовским.
    Возьмём произвольный вектор $\vec{\lambda} \in \mathbb{R}^n$
    и посморим, чему равно скалярное произведение
    $$\left( \vec{\lambda}, \vec{\xi} \right)
        = \sum_{k=1}^{n} \left( \lambda_k \cdot \xi \right)
        = \xi \cdot \sum_{k=1}^{n} \lambda_k$$

    Получилось произведение случайной гауссовской величины и константы,
    а распределение такой величины мы знаем. Для компактности записи
    заменим сумму большой буквой ``лямбда'' $\Lambda$
    $$\xi \cdot \sum_{k=1}^{n} \lambda_k
        = \xi \cdot \Lambda \sim N\left( a \cdot \Lambda,
            \sigma^2 \cdot \Lambda^2 \right)$$

    Вывод: данный вектор $\vec{\xi}=\left( \xi, \dots, \xi \right)$ является
    гауссовским для любой нормально распределённой случайной величины $\xi$.
\end{example}

\begin{example}
    Теперь возмём случайный вектор $\vec{\xi}$, состоящий из $n$ независимых
    случайных гауссовских величин со своими математическими ожиданиями
    и дисперсиями
    $$\vec{\xi} = \left( \xi_1, \dots, \xi_n \right),\;
        \xi_k \sim N\left( a_k, \sigma_k^2 \right)$$

    С математическим ожиданием всё очевидно: это вектор
    из $a_k$. Ковариация --- диагональная матрица дисперсий, так как вне
    диагонали должны стоять ковариации случайных величин между собой,
    но они независимы, а это значит, что их ковариации равны нулю
    (замечание \ref{rem:covIndepentent})
    $$\mean{\vec{\xi}} = \left( a_1, \dots, a_n \right),\;
        \dCov{\vec{\xi}} =
        \begin{bmatrix}
            \sigma_1^2 & & \\
            & \ddots & \\
            & & \sigma_n^2
        \end{bmatrix}$$

    Снова рассматриваем скалярное произведением и видим, что результат сложнее,
    но схож с полученным в предыдущем примере \ref{ex:gaussVectorsIntro}
    $$\left( \vec{\lambda}, \vec{\xi} \right)
        = \sum_{k=1}^{n} \lambda_k \cdot \xi_k \sim
        N\left( \sum_{k=1}^{n} \lambda_k \cdot a_k,
        \sum_{k=1}^{n} \lambda_k^2 \cdot \sigma_k^2 \right)$$

    Вывод: вектор, состоящий из независимых гауссовских случайных величин,
    является гауссовским.
\end{example}

\begin{definition}[Стандартный гауссовский вектор]
    \index{гауссовский вектор!стандартный}
    \index{вектор!стандартный гауссовский}
    Гауссовский случайный вектор $\vec{\xi}$ называется стандартным гауссовским
    вектором, если его математическое ожидание --- нулевой вектор,
    а ковариационная матрица --- единичная матрица
    $$\mean{\vec{\xi}} = \vec{0},\;
        \dCov{\vec{\xi}} =
        \begin{bmatrix}
            1 & & \\
            & \ddots & \\
            & & 1
        \end{bmatrix}$$
\end{definition}

Тут возникает вопрос: однозначно ли математическое ожидание
и ковариационная матрица определяют распределение случайного вектора?

Следующая лемма показывает, что такого определения нам вполне достаточно.

\begin{lemma}\label{lemma:gaussianVector:characteristicFunction}
    Распределение гауссовского вектора однозначно определяется его средним и
    ковариационной матрицей.
\end{lemma}
\begin{proof}
    Для краткости введём новые обозначения
    $$\mean{\vec{\xi}} = \vec{a},\; \dCov{\vec{\xi}} = A$$

    Поскольку характеристическая функция однозначно определяет распределение,
    то достаточно показать, что она является функцией математического ожидания
    и ковариационной матрицы
    $$\varphi_{\vec{\xi}} = f\left( \vec{a}, A \right)$$

    Введём старое обозначение скалярного произведения случайного вектора
    $\vec{\xi}$ с произвольным вектором $\vec{\lambda}$
    $$\eta = \left( \vec{\lambda}, \vec{\xi} \right)$$

    И начнём писать, чему равна характеристическая функция. Ясно, что она будет
    функцией не числа $t$, а вектора $\vec{\lambda}$
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \mean{e^{i \cdot \left( \vec{\lambda}, \vec{\xi} \right)}}
        = \mean{e^{i \cdot \eta \cdot 1}}
        = \varphi_{\eta}\left( 1 \right)$$

    Случайная величина $\eta$ является гауссовской по определению гауссовского
    вектора \ref{def:gaussianVector}, а это значит, что её характеристическая
    функция имеет вид
    $$\varphi_{\eta}\left( t \right)
        = \exp{\left\{ i \cdot t \cdot \mean{\eta}
            - \frac{t^2 \cdot \dispersion{\eta}}{2} \right\}}$$

    Очевидно, что в точке $t=1$ она принимает значение
    $$\varphi_{\eta}\left( 1 \right)
        = \exp{\left\{ i \cdot \mean{\eta}
            - \frac{\dispersion{\eta}}{2} \right\}}$$

    Из начала подраздела \ref{section:linearTransformations} о линейных
    преобразованиях помним формулы для математического ожадания
    \eqref{eq:scalarMulMean} и дисперсии \eqref{eq:linearQuadraticForm}
    случайной величины $\eta$, которая является скалярным произведением
    случайного вектора $\vec{\xi}$ с произвольным вектором $\vec{\lambda}$
    $$\mean{\eta} = \left( \vec{\lambda}, \mean{\vec{\xi}} \right),\;
        \dispersion{\eta}
            = \left( \operatorname{\dCov{\vec{\xi}}} \vec{\lambda} ,
                \vec{\lambda} \right)
            = \left( \operatorname{A} \vec{\lambda} ,
                \vec{\lambda} \right)$$

    Перепишем характеристическую функцию, воспользовавшись тем, что имеем
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \exp{\left\{ i \cdot \left( \vec{\lambda}, \vec{a} \right)
            - \frac{1}{2} \cdot \left( \operatorname{A} \vec{\lambda} ,
                \vec{\lambda} \right) \right\}}$$

    Видим, что характеристическая функция полностью восстанавливающая
    распределение, определяется исключительно математическим ожиданием
    и ковариационной матрицей, что и требовалось доказать.
\end{proof}

\begin{definition}[Гауссовское распределение]
    \index{распределение!гауссовское}
    \index{гауссовский вектор!распределение}
    Тот факт, что случайный вектор $\vec{\xi}$ имеет гауссовское распределение
    со средним $\vec{a}$ и ковариационной матрицей
    $\operatorname{A}$, будем обозначать привычнм образом
    $$\vec{\xi} \sim N\left( \vec{a}, A \right)$$
\end{definition}

\begin{remark}[Характеристическая функция гауссовского распределения]
    \index{характеристическая функция!гауссовского вектора}
    \index{гауссовский вектор!характеристическая функция}
    Как было показано в предыдущей лемме
    \ref{lemma:gaussianVector:characteristicFunction}, характеристическая
    функция вектора $\vec{\xi}$, имеющего гауссовское распределение с
    параметрами $\vec{a}$ и $\operatorname{A}$, имеет следующий вид
    $$\varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
        = \exp{\left\{ i \cdot \left( \vec{\lambda}, \vec{a} \right)
            - \frac{1}{2} \cdot \left( \operatorname{A} \vec{\lambda} ,
                \vec{\lambda} \right) \right\}},\;
        \vec{\xi} \sim N\left( \vec{a}, A \right)$$
\end{remark}

\subsection{Линейные преобразования}

\begin{lemma}\label{lemma:gaussTransformed}
    Пускай $\vec{\xi}$ --- случайный $n$-элементный вектор, имеющий гауссовское
    распределение с параметрами $\vec{a}$ и $\operatorname{A}$
    $$\vec{\xi} \sim N\left( \vec{a}, A \right)$$

    Тогда при воздействии оператора $\operatorname{T}$ на случайный вектор
    $\vec{\xi}$ получим случайный вектор, имеющий гауссовское распределение с
    параметрами $\operatorname{T} \vec{a} $ и
    $\operatorname{T} \operatorname{A} \operatorname{T^*}$
    $$\operatorname{T} \vec{\xi} \sim N\left( \operatorname{T}\vec{a} ,
        \operatorname{T} \operatorname{A} \operatorname{T^*} \right),\;
        \operatorname{T} \in \mathbb{R}^{m \times n}$$
\end{lemma}
\begin{proof}
    Из подраздела \ref{section:linearTransformations} мы знаем, что оператор
    меняет среднее значение и ковариационную матрицу именно таким образом, как
    это указано в лемме. Значит, нам нужно проверить то, что вектор остался
    гауссовским. Воспользовавшись определением \ref{def:gaussianVector}, видим,
    что всё прекрасно
    $$\forall \vec{e} \in \mathbb{R}^n:\;
        \left( \vec{e}, \operatorname{T} \vec{\xi} \right)
            = \left( \operatorname{T^*} \vec{e} , \vec{\xi} \right) \sim N$$

    В силу того, что вектор $\vec{\xi}$ является гауссовским, то значение с
    правой стороны равенства является случайной гауссовской величиной, Так как
    вектор $\vec{a}$ после воздействия на него оператором $\operatorname{T^*}$ всё
    равно остаётся одним из векторов пространства (разве что размерность вектора
    стала такой, какая нам нужна), а скалярное произведение в итоге будет
    гауссовской величиной по определению случайного гауссовского вектора.

    Поскольку справа имеем случайную величину, имеющую нормальный закон
    распределения, то слева от знака равенства стоит та же случайная величина.
    То есть, гауссовский вектор остаётся таковым после линейных преобразований.
\end{proof}

\begin{lemma}\label{lemma:gaussPlusVec}
    Есть гауссовский вектор $\vec{\xi}$ с параметрами $\vec{a}$ и $\operatorname{A}$
    и произвольный вектор $\vec{b}$ из $\mathbb{R}^n$.
    Сумма $\vec{\xi} + \vec{b}$ будет иметь гауссовское распределение с
    параметрами $\vec{a} + \vec{b}$ и $\operatorname{A}$
    $$\vec{\xi} + \vec{b} \sim N\left( \vec{a} + \vec{b}, \operatorname{A} \right)$$
\end{lemma}
\begin{proof}
    Начнём с того, что полученный вектор действительно гауссовский.
    Снова воспользуемся определением \ref{def:gaussianVector}, а также
    аддитивностью (распределительное свойство \cite[с.~82]{VoevodinLA})
    и симметричностью скалярного произведения
    $$\forall \vec{e} \in \mathbb{R}^n:\;
        \left( \vec{e}, \vec{\xi} + \vec{b} \right)
            = \left( \vec{e}, \vec{\xi} \right)
                + \left( \vec{e}, \vec{b} \right)$$

    Очевидно, что последнее скалярное произведение
    $\left( \vec{e}, \vec{b} \right)$ --- константа. Поскольку скалярное
    произведение $\left( \vec{e}, \vec{\xi} \right)$ является гауссовской
    случайной величиной, то константа лишь сдвинет его математическое
    ожидание. То есть, $\vec{\xi} + \vec{b}$ действительно является
    гауссовским вектором.

    Математическое ожидание распишем по формуле. Тут всё элементарно --- лишь
    воспользуемся линейностью математического ожидания и векторов
    $$\Mean{\vec{\xi} + \vec{b}}
        =   \begin{bmatrix}
                \Mean{\xi_1 + b_1} \\ \vdots \\ \Mean{\xi_n + b_n}
            \end{bmatrix}
        =   \begin{bmatrix}
                \mean{\xi_1} + b_1 \\ \vdots \\ \mean{\xi_n} + b_n
            \end{bmatrix}
        =   \begin{bmatrix}
                \mean{\xi_1} \\ \vdots \\ \mean{\xi_n}
            \end{bmatrix}
            +
            \begin{bmatrix}
                b_1 \\ \vdots \\ b_n
            \end{bmatrix}
        = \mean{\vec{\xi}} + \vec{b}$$

    Осталась ковариация. Начнём с определения \ref{def:vectorCovMatrix} и опять же
    используем линейность математического ожидания
    \begin{align*}
        \dCov{\vec{\xi} + \vec{b}}
        = \left\| \mean{
            \left\{ \left( \xi_i + b_i - \Mean{\xi_i + b_i} \right)
                \cdot \left( \xi_j + b_j - \Mean{\xi_j + b_j} \right)
            \right\}} \right\|_{i,j=1}^n = \\
        = \left\| \mean{
            \left\{ \left( \xi_i + b_i - \Mean{\xi_i} - b_i \right)
                \cdot \left( \xi_j + b_j - \Mean{\xi_j} - b_j \right)
            \right\}} \right\|_{i,j=1}^n = \\
        = \left\| \mean{
            \left\{ \left( \xi_i - \mean{\xi_i} \right)
                \cdot \left( \xi_j - \mean{\xi_j} \right)
            \right\}} \right\|_{i,j=1}^n
        = \dCov{\vec{\xi}} = \operatorname{A}
    \end{align*}

    То есть, вектор $\vec{\xi} + \vec{b}$ --- случайный гауссовский вектор с
    параметрами $\vec{a} + \vec{b}$ и $\operatorname{A}$, что и требовалось доказать
    $$\vec{\xi} + \vec{b} \sim N\left( \vec{a} + \vec{b}, \operatorname{A} \right)$$
\end{proof}

\begin{theorem}\label{theorem:gaussianVectorExistance}
    Для произвольных $\vec{a} \in \mathbb{R}^n$ и симметричной неотрицательной
    матрицы $\operatorname{A}$ существует гауссовский вектор с распределением
    $N\left( \vec{a}, \operatorname{A} \right)$
\end{theorem}
\begin{proof}
    Пускай $\vec{\xi}$ --- стандартный гауссовский вектор в $\mathbb{n}$
    $$\vec{\xi} \sim N\left( 0, \operatorname{I} \right)$$

    Тогда возьмём неотрицательную матрицу $\operatorname{V}$, вектор $\vec{a}$ и
    слепим случайный вектор $\vec{\eta}$
    $$\vec{\eta} = \operatorname{V} \vec{\xi} + \vec{a}$$

    Из предыдущих лемм \ref{lemma:gaussTransformed} и \ref{lemma:gaussPlusVec}
    знаем, что новый вектор будет иметь гауссовское распределение с параметрами
    $\vec{a}$ и $\operatorname{V} \operatorname{I} \operatorname{V}^* 
    = \operatorname{V} \operatorname{V^*} $
    $$\eta \sim N\left( \vec{a}, \operatorname{V} \operatorname{V^*} \right)$$

    Теперь задача состоит в том, чтобы подобрать такую матрицу $\operatorname{V}$,
    чтобы её произведение с сопряжённой равнялось нужной нам $\operatorname{A}$
    $$\operatorname{V} \operatorname{V^*} = \operatorname{A}$$

    В замечании \ref{remark:linearAlgebra:selfAdjointMatrix} мы вспоминали
    линейную алгебру, а именно --- тот момент, что самосопряжённая
    неотрицательная определённая матрица $\operatorname{A}$ имеет собственный
    ортонормированный базис, в котором она превращается в диагональную матрицу
    с неотрицательныи элементами
    $$
    \begin{bmatrix}
        \lambda_1 & & \\
        & \ddots &  \\
        & & \lambda_n
    \end{bmatrix},\;
        \lambda_k \ge 0$$

    Создадим в этом базисе самосопряжённую матрицу $\operatorname{V}$, в ячейках
    которой которой будут корни соответствующих элементов исходной матрицы
    (в этом базисе)
    $$
    \begin{bmatrix}
        \sqrt{\lambda_1} & & \\
        & \ddots &  \\
        & & \sqrt{\lambda_n}
    \end{bmatrix},\; \lambda_k \ge 0$$

    Диагональная матрица с действительными элементами очевидно является
    самосопряжённой. То есть, произведение матрицы $\operatorname{V}$ на
    сопряжённую к ней матрицу $\operatorname{V^*}$ в этом базисе будет давать
    исходную матрицу $\operatorname{A}$.

    Останется лишь перейти в исходный базис и матрица будет готова. То есть,
    существует такая матрица $\operatorname{V}$, что вектор $\vec{\eta}$ будет
    иметь нужное нам распределение
    $$\exists \vec{a}, \operatorname{V}:\;
        \vec{\eta} = \operatorname{V} \operatorname{V^*} \vec{\xi} + \vec{a}
        \sim N\left( \vec{a}, \operatorname{A} \right)$$
\end{proof}

\begin{theorem}
    Если матрица $\operatorname{A}$ невырожденная
    $\operatorname{A} > 0 \Leftrightarrow \det{\operatorname{A}} \neq 0$,
    то у случайного вектора, имеющего гауссовское распределение с параметрами
    $\vec{a}$ и $\operatorname{A}$, есть плотность распределения
    $$\pdf{\vec{u}}
        = \frac{1}{\sqrt{2 \cdot \pi}^n \cdot \sqrt{\det{\operatorname{A}}}}
            \cdot \exp{\left\{ -\frac{1}{2} \cdot \left(
                \operatorname{A^{-1}} \left( \vec{u} - \vec{a} \right),
                \vec{u} - \vec{a} \right) \right\}}$$
\end{theorem}
\begin{proof}
    Из доказательства прошлой теоремы \ref{theorem:gaussianVectorExistance}
    помним, что для невырожденной матрицы $\operatorname{A}$ найдётся такая
    матрица $\operatorname{V}$, что при её умножении на сопряжённую получится
    матрица $\operatorname{A}$
    $$\exists \operatorname{V} = \operatorname{A^{\frac{1}{2}}}:\;
        \operatorname{V}\operatorname{V^*} = \operatorname{V^2} = A$$

    Там же мы построили вектор, имеющий гауссовское распределение с параметрами
    $\vec{a}$ и $\operatorname{A}$
    $$\vec{\xi} \sim N\left( \vec{0}, \operatorname{I} \right),\;
        \vec{\eta} = \operatorname{V} \vec{\xi} +\vec{a}
            \sim N\left( \vec{a}, \operatorname{A} \right)$$

    Ясно, что плотность распределения стандартного гауссовского вектора
    $\vec{\xi}$ имеет вид
    $$q\left( \vec{v} \right)
        = \frac{1}{\sqrt{2 \cdot \pi}^n}
            \cdot e^{-\frac{1}{2} \cdot \left( \vec{v}, \vec{v} \right)}$$

    Распишем, чему равна вероятность того, что вектор $\vec{\eta}$ очутился
    в области $\Delta \in \mathfrak{B}$
    \begin{equation}\label{eq:gaussVectorProbabilityStart}
        \Probability{\vec{\eta} \in \Delta}
        = \Probability{\operatorname{V} \vec{\xi} + \vec{a} \in \Delta}
        = \Probability{\vec{\xi} \in \left\{ \vec{v}:
            \operatorname{V} \vec{v} + \vec{a} \in \Delta \right\}}
    \end{equation}

    Теперь у нас появилось более чёткое представление об области интегрирования.
    Перепишем вероятность через интеграл
    $$\Probability{\vec{\xi} \in \left\{ \vec{v}:
            \operatorname{V} \vec{v} + \vec{a} \in \Delta \right\}}
        = \integrall{\left\{ \vec{v}: \operatorname{V} \vec{v} + \vec{a}
            \in \Delta \right\}}{d\vec{v}}{q\left( \vec{v} \right)}$$

    Введём замену
    $$\vec{u} = \operatorname{V} \vec{v} + \vec{a}
        \Rightarrow
        \vec{v} = \operatorname{V^{-1}} \left( \vec{u} - \vec{a} \right)$$

    Поскольку дифференциал $d\vec{u}$ --- площадь $n$-мерного параллелограмма,
    а матрица $\operatorname{V^{-1}}$ ортогональна, то она будет увеличивать
    площадь параллелограмма в $\det{\operatorname{V^{-1}}}$ раз
    $$d\vec{v} = \det{\operatorname{V^{-1}}} \cdot d\vec{u}$$

    Можем переписать интеграл
    \begin{equation}\label{eq:gaussDencityIntegral}
    \integrall{\left\{ \vec{v}: \operatorname{V} \vec{v} + \vec{a}
            \in \Delta \right\}}{d\vec{v}}{q\left( \vec{v} \right)}
        = \integrall{\Delta}{d\vec{u}}{ q\left( \operatorname{V^{-1}}
                \left( \vec{u} - \vec{a} \right) \right)
            \cdot \det{\operatorname{V^{-1}}}}
    \end{equation}

    Теперь вспомним несколько моментов, а именно:
    \begin{enumerate}
        \item Определитель --- мультипликативная функция матрицы. Из этого
            следует красивый вывод
            $$\operatorname{V^2} = \operatorname{A}
                \Rightarrow \det{\operatorname{V}}
                    = \sqrt{\det{\operatorname{A}}}
                \Rightarrow \det{\operatorname{V}^{-1}}
                    = \frac{1}{\sqrt{\det{\operatorname{A}}}}$$
        \item Рассмотрим скалярное произведение, которое получится в результате
            расписывания плотности и перенесём матрицу с правой части в левую
            $$\left( \operatorname{V^{-1}} \left( \vec{u} - \vec{a} \right),
                    \operatorname{V^{-1}} \left( \vec{u} - \vec{a} \right)
                    \right)
                = \left( \left( \operatorname{V^{-1}} \right)^*
                        \operatorname{V^{-1}} \left( \vec{u} - \vec{a} \right),
                        \left( \vec{u} - \vec{a} \right) \right)$$

            Сопряжённая к обратной матрице --- обратная к сопряжённой, а
            наша матрица самосопряжённая, поэтому просто получаем обратную
            \begin{align*}
                \left( \left( \operatorname{V^{-1}} \right)^*
                        \operatorname{V^{-1}} \left( \vec{u} - \vec{a} \right),
                        \left( \vec{u} - \vec{a} \right) \right)
                = \left( \left( \operatorname{V^*} \right)^{-1}
                        \operatorname{V^{-1}} \left( \vec{u} - \vec{a} \right),
                        \left( \vec{u} - \vec{a} \right) \right) = \\
                = \left( \operatorname{V^{-1}}
                        \operatorname{V^{-1}} \left( \vec{u} - \vec{a} \right),
                        \left( \vec{u} - \vec{a} \right) \right)
                = \left( \operatorname{V^{-2}} \left( \vec{u} - \vec{a} \right),
                        \left( \vec{u} - \vec{a} \right) \right)
            \end{align*}

            Помним, что квадрат матрицы $\operatorname{V}$ --- это матрица
            $\operatorname{A}$, и вводим эту замену
            $$\operatorname{V^2} = \operatorname{A} \Rightarrow
                \left( \operatorname{V^{-2}} \left( \vec{u} - \vec{a} \right),
                        \left( \vec{u} - \vec{a} \right) \right)
                = \left( \operatorname{A^{-1}} \left( \vec{u} - \vec{a} \right),
                        \left( \vec{u} - \vec{a} \right) \right)$$
    \end{enumerate}

    Возвращаемся к интегралу \eqref{eq:gaussDencityIntegral} и вводим
    только что оговоренные замены
    $$\integrall{\Delta}{d\vec{u}}{ q\left( \operatorname{V^{-1}}
                \left( \vec{u} - \vec{a} \right) \right)
            \cdot \det{\operatorname{V^{-1}}}}
        = \integrall{\Delta}{d\vec{u}}{\frac{1}{\sqrt{2 \cdot \pi}^n}
            \cdot \frac{1}{\sqrt{\det{\operatorname{A}}}}
            \cdot e^{\left( \operatorname{A^{-1}}
                    \left( \vec{u} - \vec{a} \right),
                \left( \vec{u} - \vec{a} \right) \right)}}$$

    Остановимся и внимательно посмотрим, что же это такое. Из равенства
    \eqref{eq:gaussVectorProbabilityStart} вспоминаем, что этот интеграл ---
    вероятность того, что вектор $\vec{\eta}$ попадёт в область $\Delta$.
    Напишем это, чтобы не забыть
    $$\Probability{\vec{\eta} \in \Delta}
        = \integrall{\Delta}{d\vec{u}}{\pdf{\vec{u}}}
        = \integrall{\Delta}{d\vec{u}}{
            \frac{\exp{\left\{\left( \operatorname{A^{-1}}
                    \left( \vec{u} - \vec{a} \right),
                \left( \vec{u} - \vec{a} \right) \right) \right\}}}{
                \sqrt{2 \cdot \pi}^n \cdot \sqrt{\det{\operatorname{A}}}}}$$

    Отсюда уже очевидно, что плотность случайного гауссовского вектора с
    параметрами $\vec{a}$ и $\operatorname{A}$ считается по формуле
    $$\pdf{\vec{u}}
        = \frac{1}{\sqrt{2 \cdot \pi}^n \cdot \sqrt{\det{\operatorname{A}}}}
            \cdot \exp{\left\{ -\frac{1}{2} \cdot \left(
                \operatorname{A^{-1}} \left( \vec{u} - \vec{a} \right),
                \vec{u} - \vec{a} \right) \right\}}$$
\end{proof}

\begin{affirmation}
    Если матрица $\operatorname{A}$ вырождена $\det\operatorname{A} = 0$, то не
    существует плотности распределения у гауссовского вектора с параметрами
    $\vec{a}$ и $\operatorname{A}$
\end{affirmation}
\begin{proof}
    По определению \ref{def:vectorCovMatrix} ковариационная матрица считается по
    формуле
    $$\operatorname{A}
        = \dCov{\vec{\xi}}
        = \left\| \cov{\xi_i, \xi_j} \right\|_{i,j=1}^n$$

    Если определитель равен нулю, то это значит, что строки матрицы линейно
    зависимы. То есть, найдётся такой ненулевой вектор $\vec{\alpha}$, что
    его произведение с любым столбиком матрицы будет давать ноль
    $$\left( \exists \vec{\alpha} \neq \vec{0} \right) \left( \forall j \right):
        \;\sum_{i=1}^{n} \alpha_i \cdot \cov{\xi_i, \xi_j} = 0$$

    Имеем право внести сумму и коэффициенты под знак ковариации\footnote{Третье
    свойство ковариации \cite[с.~310]{MGTUXVI}}
    $$\left( \exists \vec{\alpha} \neq \vec{0} \right) \left( \forall j \right):
        \;\cov{\sum_{i=1}^{n} \alpha_i \cdot \xi_i, \xi_j} = 0$$

    Поскольку это равенство выполняется для любых $j$, то их сумма тоже будет
    равняться нулю, причём каждое слагаемое можно умножить на любую константу.
    Пускай это будут $\alpha_j$ в каждом слагаемом
    $$\exists \vec{\alpha} \neq \vec{0}:\; \sum_{j=1}^{n} \alpha_j
            \cdot \cov{\sum_{i=1}^{n} \alpha_i \cdot \xi_i, \xi_j} = 0$$

    Эту сумму мы внесём во второй аргумент ковариации
    $$\exists \vec{\alpha} \neq \vec{0}:\;
        \cov{\sum_{i=1}^{n} \alpha_i \cdot \xi_i,
            \sum_{j=1}^{n} \alpha_j \cdot \xi_j} = 0$$

    Заменив $j$ на $i$ во второй сумме (ничего не поменяется кроме обозначения),
    явно видно, что перед нами ковариация случайной величины с самой собой, что
    равняется её дисперсии
    $$\exists \vec{\alpha} \neq \vec{0}:\;
        \cov{\sum_{i=1}^{n} \alpha_i \cdot \xi_i,
            \sum_{i=1}^{n} \alpha_i \cdot \xi_i}
        = \dispersion{\sum_{i=1}^{n} \alpha_i \cdot \xi_i}
        = \dispersion{\left( \vec{\alpha}, \vec{\xi} \right)}
        = 0$$

    То есть, случайная величина, равная скалярному произведению некоего
    ненулевого вектора $\vec{\alpha}$ и случайного вектора $\vec{\xi}$,
    не рассеивается. Значит, она равна константе с единичной вероятностью
    $$\dispersion{\left( \vec{\alpha}, \vec{\xi} \right)} = 0
        \Rightarrow \exists c:\;
        \Probability{\left( \vec{\alpha}, \vec{\xi} \right) = c} = 1$$

    То есть, мы получили уравнение гиперплоскости ---
    $\left( n-1 \right)$-мерное подпространство
    $$H = \left\{ \vec{x} \in \mathbb{R}^n
        \mcond \left( \vec{\alpha}, \vec{x} \right) = c \right\}$$

    Вероятность того, что случайный вектор $\vec{\xi}$ попадёт в это
    подпространство, равна единице
    $$\Probability{\vec{\xi} \in H} = 1$$

    Объём подпространства $H$ в $n$-мерном пространстве равен нулю
\end{proof}

\subsection{Независимость компонент гауссовского вектора}

\begin{definition}[Конкатенация векторов]
    \index{вектор!конкатенация}
    \index{случайный вектор!конкатенация}
    Есть два вектора $\vec{\xi}$ из пространства $\mathbb{R}^n$ и $\vec{\eta}$
    из пространства $\mathbb{R}^m$
    $$\vec{\xi} = \left( \xi_1, \dots, \xi_n \right),
        \vec{\eta} = \left( \eta_1, \dots, \eta_m \right)$$

    Результатом конкатенации (сцепления) будем называть вектор
    $\vec{\xi} \circ \vec{\eta}$, первая половина которого состоит из
    элементов вектора $\vec{\xi}$, а вторая половина --- из элементов вектора
    $\vec{\eta}$. Естественно, результат будет в пространстве $\mathbb{R}^{n+m}$
    $$\vec{\xi} \circ \vec{\eta}
        = \left( \xi_1, \dots, \xi_n, \eta_1, \dots, \eta_m \right)$$
\end{definition}

Пускай $\vec{\xi} \circ \vec{\eta}$ --- гауссовский случайный вектор.

\index{случайный вектор!конкатенация!математическое ожидание}
Тогда вектор математического ожидания будет выглядеть следующим образом
$$\vec{a}
    = \Mean{\vec{\xi} \circ \vec{\eta}}
    = \mean{\vec{\xi}} \circ \mean{\vec{\eta}}
    = \left( \mean{\xi_1}, \dots, \mean{\xi_n},
        \mean{\eta_1}, \dots, \mean{\eta_m} \right)$$

\index{случайный вектор!конкатенация!ковариационная матрица}
Матрицу ковариации запишем в виде такой мозаики
\begin{equation}\label{eq:covMatrix:mozaik}
    \operatorname{A}
    = \dCov{\vec{\xi} \circ \vec{\eta}}
    = \left[ \begin{array}{r|l}
            \dCov{\vec{\xi}} & \Cov{\vec{\xi}}{\vec{\eta}} \\
            \hline
            \Cov{\vec{\eta}}{\vec{\xi}} & \dCov{\vec{\eta}} \\
        \end{array} \right]
\end{equation}

\begin{theorem}
    Для гауссовских случайных векторов независимость эквивалентна
    некоррелированности.
    То есть, два случайных гауссовских вектора $\vec{\xi}$ и $\vec{\eta}$
    независимы тогда и только тогда, когда их ковариационная матрица нулевая
    $$\Cov{\vec{\xi}}{\vec{\eta}} = 0$$
\end{theorem}
\begin{proof}
    Необходимость очевидна: ковариация двух независимых случайных величин равна
    нулю (смотрите замечание \ref{rem:covIndepentent}). Поскольку
    ковариационная матрица этих векторов по определению \ref{def:covMatrix}
    состоит из ковариаций элементов векторов, то все ячейки матрицы будут
    заполнены нулями
    $$\Cov{\vec{\xi}}{\vec{\eta}}
        = \left\| \cov{\xi_i, \eta_j} \right\|_{
            \substack{i=\overline{1,n},\\j=\overline{1,m}}}
        = \left\| 0 \right \|_{
            \substack{i=\overline{1,n},\\j=\overline{1,m}}}$$

    Достаточность тоже доказать несложно. Помним, что характеристическая
    функция суммы двух случайных величин равна произведению характеристических
    функций тогда и только тогда, когда случайные величины независыми
    \cite[с.~354]{Shiryayev1}. Посмотрим же, чему равна характеристическая
    функция
    \begin{equation}\label{eq:concatinationCFStart}
        \varphi_{\vec{\xi} \circ \vec{\eta}}\left(
            \vec{\lambda} \circ \vec{\mu} \right)
        = \exp{\left\{
                i \cdot \left( \vec{\lambda} \circ \vec{\mu},
                    \Mean{\vec{\xi} \circ \vec{\eta}} \right)
                - \frac{1}{2} \cdot \left( \operatorname{A}
                    \left[ \vec{\lambda} \circ \vec{\mu} \right],
                    \vec{\lambda} \circ \vec{\mu} \right)
            \right\}}
    \end{equation}

    Естественно, вектора $\vec{\lambda}$ и $\vec{\mu}$ имеют размерности $n$
    и $m$ соответственно, как случайные вектора $\vec{\xi}$ и $\vec{\eta}$.

    Проделаем небольшой трюк со скалярным произведением, в котором присутствует
    математическое ожидание
    \begin{align*}
        \left( \vec{\lambda} \circ \vec{\mu},
            \Mean{\vec{\xi} \circ \vec{\eta}} \right) = \\
        = \lambda_1 \cdot \mean{\xi_1} + \dots + \lambda_n \cdot \mean{\xi_n} +
            \mu_1 \cdot \mean{\eta_1} + \dots + \mu_n \cdot \mean{\eta_n} = \\
        = \left( \vec{\lambda}, \mean{\vec{\xi}} \right)
            + \left( \vec{\mu}, \mean{\vec{\eta}} \right)
    \end{align*}

    Из формулы \eqref{eq:covMatrix:mozaik} помним, как выглядит ковариационная
    матрица конкатенации. Применим то, что совместная
    ковариация случайных векторов $\vec{\xi}$ и $\vec{\eta}$ нулевая
    $$\begin{cases}
        \operatorname{A}
            = \left[ \begin{array}{r|l}
                \dCov{\vec{\xi}} & \Cov{\vec{\xi}}{\vec{\eta}} \\
                \hline
                \Cov{\vec{\eta}}{\vec{\xi}} & \dCov{\vec{\eta}} \\
            \end{array} \right] \\
        \Cov{\vec{\xi}}{\vec{\eta}} = \Cov{\vec{\eta}}{\vec{\xi}} = 0
        \end{cases}
        \Rightarrow
        \operatorname{A}
            = \left[ \begin{array}{c|c}
                \dCov{\vec{\xi}} & \varnothing \\
                \hline
                \varnothing & \dCov{\vec{\eta}} \\
            \end{array} \right]$$

    Дальше будем действовать по аналогии с математическим ожиданием
    \begin{align*}
        \left( \operatorname{A} \left[ \vec{\lambda} \circ \vec{\mu} \right],
            \vec{\lambda} \circ \vec{\mu} \right)
        = \left( \dCov{\vec{\xi}} \vec{\lambda}
                \circ \dCov{\vec{\eta}} \vec{\mu},
            \vec{\lambda} \circ \vec{\mu} \right) = \\
        = \left( \dCov{\vec{\xi}} \vec{\lambda}, \vec{\lambda} \right)
            + \left( \dCov{\vec{\eta}} \vec{\mu}, \vec{\mu} \right)
    \end{align*}

    С учётом вышесказанного характеристическая функция
    \eqref{eq:concatinationCFStart} принимает вид
    \begin{align*}
        \varphi_{\vec{\xi} \circ \vec{\eta}}\left(
            \vec{\lambda} \circ \vec{\mu} \right)
        = \exp{\left\{
                i \cdot \left( \vec{\lambda} \circ \vec{\mu},
                    \Mean{\vec{\xi} \circ \vec{\eta}} \right)
                - \frac{1}{2} \cdot \left( \operatorname{A}
                    \left[ \vec{\lambda} \circ \vec{\mu} \right],
                    \vec{\lambda} \circ \vec{\mu} \right)
            \right\}} = \\
        = \exp{\left\{ 
            i \cdot \left( \vec{\lambda}, \mean{\vec{\xi}} \right)
                + i \cdot \left( \vec{\mu}, \mean{\vec{\eta}} \right)
                - \frac{1}{2} \cdot
                    \left( \dCov{\vec{\xi}} \vec{\lambda}, \vec{\lambda} \right)
                - \frac{1}{2} \cdot
                    \left( \dCov{\vec{\eta}} \vec{\mu}, \vec{\mu} \right)
                \right\}}
    \end{align*}

    Сгруппировав переменные, видим, что характеристическая функция разбилась
    на произведение двух характеристических функций
    \begin{align*}
        \varphi_{\vec{\xi} \circ \vec{\eta}}\left(
            \vec{\lambda} \circ \vec{\mu} \right)
        = e^{
            i \cdot \left( \vec{\lambda}, \mean{\vec{\xi}} \right)
                - \frac{1}{2} \cdot
                    \left( \dCov{\vec{\xi}} \vec{\lambda}, \vec{\lambda} \right)
                }
            \cdot e^{
                i \cdot \left( \vec{\mu}, \mean{\vec{\eta}} \right)
                    - \frac{1}{2} \cdot
                        \left( \dCov{\vec{\eta}} \vec{\mu}, \vec{\mu} \right)
                    } = \\
        = \varphi_{\vec{\xi}}\left( \vec{\lambda} \right)
            \cdot \varphi_{\vec{\mu}}\left( \vec{\mu} \right)
    \end{align*}

    Характеристическая функция конкатенации разбилась в произведение
    характеристических функций, а это значит, что теорема доказана.
\end{proof}

\begin{remark}
    Важно, чтобы вектор $\vec{\xi} \circ \vec{\eta}$ был гауссовским, иначе
    $\vec{\xi}$ и $\vec{\eta}$ могут быть зависимыми даже при нулевой ковариации
    $\Cov{\vec{\xi}}{\vec{\eta}}$.
    Если $\vec{\xi}$ гауссовский и $\vec{\eta}$ тоже, это ещё не гарантирует
    того, что их конкатенация будет гауссовским вектором.
\end{remark}
