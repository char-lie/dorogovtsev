\chapter{Методы оценивания неизвестных параметров}

\section{Вступление}

Мы занимаемся гауссовскими векторами, поскольку они геометричны. Об этом говорят
такие важные свойства
\begin{enumerate}
    \item Некоррелированность гауссовских векторов эквивалентна их независимости
        (теорема \ref{theorem:gaussianVector:independence})
    \item После поворота гауссовский вектор остаётся гауссовским
        (лемма \ref{lemma:gaussRotated})
    \item Чтобы вычислить совместное распределение гауссовских векторов,
        достаточно матриц
        (теорема \ref{theorem:gaussVector:conditionalDistribution})
\end{enumerate}

Рассмотрим пример, который наведёт нас на необходимые размышления.

\begin{example}\label{example:leastSquares:scalarOptimalEstimator}
    Есть выборка $x_1, \dots, x_n$ из гауссовского распределения с неизвестным
    средним $x_i \sim N\left( \theta, 1 \right)$.

    Как проверить, что оценка $\theta_*$ оптимальная?

    Поскольку гауссовское распределение является экспоненциальным
    (определение \ref{def:exponentialDistribution}),
    то для него существует эффективная оценка
    (утверждение \ref{affirmation:efficientEstimator:exponentialExsistance}).

    Выпишем функцию правдоподобия (определение \ref{def:likehoodFunction})
    \begin{align*}
        L\left( \vec{x}, \theta \right)
        = \frac{1}{\sqrt{2 \cdot \pi}^2}
            \cdot e^{- \frac{1}{2}
                \cdot \sum_{k=1}^{n}\left( x_i - \theta \right)^2}
    \end{align*}

    Эффективная оценка считается по формуле (определение
    \ref{def:maximumLikelihoodEstimation})
    \begin{align*}
        \theta_* = \argmaxof{\ln{L\left( \vec{x},\theta \right)}}{\theta}
    \end{align*}

    Поскольку $\theta$ находится лишь в экспоненте, то нужно максимизировать
    экспоненту
    \begin{align*}
        \theta_* = \argmaxof{\left( e^{- \frac{1}{2}
            \cdot \sum_{k=1}^{n}\left( x_i - \theta \right)^2} \right)}{\theta}
    \end{align*}

    Сумма неотрицательна, а это значит, что для того, чтобы экспонента приняла
    максимальное значение, нужно минимизировать эту сумму
    \begin{align*}
        \theta_* = \argminof{
            \sum_{k=1}^{n}\left( x_i - \theta \right)^2}{\theta}
    \end{align*}

    Вспомним геометрию с физикой,\footnote{
        Есть вектор $\vec{y}$, есть вектора $\vec{a_1}, \dots, \vec{a_n}$.

        В точке $\vec{a}$ находится центр масс тела, состоящего из точек
        $\vec{a_k}$ (массу каждой точки считаем равной единице)
        \begin{align*}
            \vec{a} = \frac{1}{n} \cdot \sum_{k=1}^{n} \vec{a_k}
        \end{align*}

        Момент инерции относительно точки $\vec{y}$ --- сумма квадратов
        расстояний, умноженных на массы точек
        \begin{align*}
            \sum_{k=1}^{n} \left\| \vec{y} - \vec{a_k} \right\|^2
            = n \cdot \left\| \vec{y} - \vec{a} \right\|^2
                + \sum_{k=1}^{n} \left\| \vec{a} - \vec{a_k} \right\|^2
        \end{align*}

        Момент инерции минимальный у центра масс.}
    понимаем, что качестве оценки $\theta_*$ нужно взять в выборочное среднее
    \begin{align*}
        \theta_* = \frac{1}{n} \cdot \sum_{k=1}^{n} x_k
    \end{align*}
\end{example}

\begin{example}\label{example:leastSquares:vectorOptimalEstimator}
    Обобщим задачу \ref{example:leastSquares:scalarOptimalEstimator}.
    Мы ведь изучали гауссовские вектора для того, чтобы иметь дело с многомерным
    случаем.

    Возьмём $m$-мерную оценку $\vec{\theta}$
    \begin{align*}
        \vec{\theta} \in \mathbb{R}^m,\; m \le n
    \end{align*}

    Матрица $\operatorname{A}$
    \begin{align*}
        A:\; \mathbb{R}^m \rightarrow \mathbb{R}^n
    \end{align*}

    Вместо выборки $x_1, \dots, x_n$ будем рассматривать вектор $\vec{y}$
    \begin{align*}
        \vec{y} = \operatorname{A} \vec{\theta} + \vec{\xi},\;
            \vec{\xi} \sim N\left( \vec{0}, \operatorname{I} \right)
    \end{align*}

    Эта ситуация является модификацией исходной задачи. Из леммы
    \ref{lemma:gaussMoved} Помним, что гауссовский вектор в сумме с произвольным
    --- смещённый гауссовский вектор
    \begin{align*}
        \left( x_1, \dots, x_n \right)
        = \overbrace{\left( \theta, \dots, \theta \right)}^{n} + \vec{\xi},\;
        m=1 \Rightarrow \operatorname{A} \theta
            = \overbrace{\left( \theta, \dots, \theta \right)}^{n}
    \end{align*}

    Мы чуть-чуть усложнили задачу и теперь нужно найти оптимальную оценку для
    векторной величины
    \begin{align*}
        \vec{\theta}_* = ?
    \end{align*}

    Поскольку имеем дело с экспоненциальным распределением, то дальше делаем то
    же, что и в предыдущем примере. Для начала выписываем функцию правдоподобия
    \begin{align*}
        L\left( \vec{\xi}, \vec{\theta} \right)
        = \frac{1}{\sqrt{2 \cdot \pi}} \cdot e^{-\frac{1}{2}
            \cdot \left\| \vec{x} - \operatorname{A} \vec{\theta} \right\|^2}
    \end{align*}

    Чтобы максимизировать функцию правдоподобия, нужно минимизировать ту норму,
    что в экспоненте
    \begin{align*}
        \vec{\theta}_*
        = \argminof{\left\| \vec{\xi}
            - \operatorname{A} \vec{\theta} \right\|^2}{
                \vec{\theta} \in \mathbb{R}^m}
    \end{align*}
\end{example}

Увидели, что поиск оптимальной оценки среднего для гауссовского вектора
теперь сводится не к дифференциальным уровнениям, а к поиску минимума.

\section{Оценка метода наименьших квадратов}
\index{метод!наименьших квадратов}
\index{оценка!метода наименьших квадратов}

\subsection{Размышления}

Есть случайные величины $x$ и $y$, между которыми нужно определить зависимость.

Гипотеза: эта зависимость может быть линейной
\begin{align*}
    y = \sum_{k=1}^{m}f_k\left( x \right) \cdot \theta_k
\end{align*}

В формуле $f_k$ --- известная функция, $\theta_k$ --- неизвестный коэффициент.

\begin{example}
    \begin{align*}
        \begin{cases}
            f_1\left( x \right) = 1 \\
            f_2\left( x \right) = x \\
        \end{cases} \Rightarrow
        f\left( x \right) = \theta_1 + \theta_2 \cdot x
    \end{align*}
\end{example}

\begin{example}
    Пускай функции будут степенями $x$
    \begin{align*}
        \begin{cases}
            f_1\left( x \right) = 1 \\
            f_2\left( x \right) = x \\
            \begin{array}{c}
            \vdots
            \end{array} \\
            f_m\left( x \right) = x^{m-1}
        \end{cases} \Rightarrow
        y = \sum_{k=1}^{m} \theta_k \cdot x^{k-1}
    \end{align*}

    Получили полиномиальную зависимость.
\end{example}

Когда есть наблюдения $y_1, \dots, y_n$, то возникают и помехи
$\xi_1, \dots, \xi_n \sim N\left( 0, 1 \right)$\footnote{Считаем, что смещение
помех нулевое, так как иначе это будет означать неправильную постановку
эксперимента. Также считаем, что эти случайные величины независимы между собой
по той же причине.

У нас идеальные условия, и экспериментаторы --- люди толковые.

Ковариация выбрана единичной для удобства. Потом мы это дело обобщим.}
\begin{align*}
    \begin{array}{c}
        y_1 = \sum_{k=1}^{m} f_k\left( x_1 \right) \cdot \theta_k + \xi_1 \\
        \vdots \\
        y_n = \sum_{k=1}^{m} f_k\left( x_1 \right) \cdot \theta_k + \xi_n
    \end{array}
\end{align*}

Какая же оценка будет оптимальной для
$\vec{\theta} = \left( \theta_1, \dots, \theta_m \right)$?

Рассмотрим в контексте задачи \ref{example:leastSquares:vectorOptimalEstimator}.

\begin{example}
    Помехи составляют стандартный гауссовский вектор как совокупность
    независимых стандартных гауссовских величин
    \begin{align*}
        \vec{\xi} \sim N\left( \vec{0}, \operatorname{I} \right)
    \end{align*}

    Также у нас есть параметр $\vec{\theta} \in \mathbb{R}^m$.

    Матрица $\operatorname{A}$ принимает следующий вид
    \begin{align*}
        \operatorname{A} = \left\| f_j\left( x_i \right) \right\|_{
            \substack{i = \overline{1, n} \\ j = \overline{1, m}}}
    \end{align*}

    Тогда вектор $\vec{y}$ примет понятный из контекста примера
    \ref{example:leastSquares:vectorOptimalEstimator} вид
    \begin{align*}
        \vec{y} = \operatorname{A} \vec{\theta} + \vec{\xi}
    \end{align*}

    Тут мы уже знаем, как искать миниммум
    \begin{align*}
        \argminof{\left\| \vec{y} - \operatorname{A} \vec{\theta} \right\|^2}{
            \vec{\theta} \in \mathbb{R}^m}
        = \argminof{\sum_{k=1}^{n} \left( y_k
                - \left( \operatorname{A} \vec{\theta} \right)_k \right)^2}{
            \vec{\theta} \in \mathbb{R}^m}
    \end{align*}
\end{example}

Если для гауссовских случайных величин этот метод работает хорошо, то обобщим
его.

\subsection{Общая постановка задачи метода наименьших квадратов}

Вектор $\vec{\xi}$ --- случайный вектор в $\mathbb{R}^n$ с нулевым средним и
единичной ковариацией.

Оператор $\operatorname{A}$ действует из $\mathbb{R}^m$ в $\mathbb{R}^n$. Чтобы
он не уничтожал параметры, скажем, что его ранг равен $m$
\begin{align*}
    \rank{\operatorname{A}} = m
\end{align*}

Нужно найти оптимальную линейную оценку для параметра $\vec{\theta}$ по
наблюденям $\vec{y}$
\begin{align*}
    \vec{\theta}_* = \operatorname{B} \vec{y},\;
        \vec{y} = \operatorname{A} \vec{\theta} + \vec{\xi}
\end{align*}

\begin{theorem}
    \label{theorem:gaussMarkov}
    Пусть матрица $\operatorname{A} \in \mathbb{R}^{n \times m}$ имеет ранг $m$,
    а случайный вектор $\vec{\xi}$ находится в $\mathbb{R}^n$ и обладает нулевым
    средним и единичной ковариацией.

    Тогда оценка $\vec{\theta}_*$ является единственным решением задачи
    минимизации
    \begin{align*}
        \vec{\theta}_*
        = \argminof{\left\| \vec{y} - \operatorname{A} \vec{\theta} \right\|^2}{
            \vec{\theta} \in \mathbb{R}^m}
    \end{align*}

    То есть, ковариация оценки $\vec{\theta}_*$ не меньше любой несмещённой
    оценки $\vec{\tau}$ параметра $\vec{\theta}$ (разность
    ковариационных матриц --- неотрицательно определённая матрица)
    \begin{align*}
        \dCov{\vec{\theta}_*} \le \dCov{\vec{\tau}}
        \Rightarrow \dCov{\vec{\tau}} - \dCov{\vec{\theta}_*} \ge 0
    \end{align*}
\end{theorem}
\begin{proof}
    Разобьёт доказательство на три части

    \begin{enumerate}
        \item Нужно найти минимум --- значит, возьмём производную по $m$
            аргументам. Это оператор набла по параметру $\vec{\theta}$
            \begin{align*}
                \nabla_{\vec{\theta}} \cdot \left\| \vec{y}
                    - \operatorname{A} \vec{\theta} \right\|^2
                = \nabla_{\vec{\theta}} \cdot
                    \left( \vec{y} - \operatorname{A} \vec{\theta},
                    \vec{y} - \operatorname{A} \vec{\theta} \right) = \\
                = \nabla_{\vec{\theta}} \cdot \left( \vec{y}, \vec{y} \right)
                    - 2 \cdot \nabla_{\vec{\theta}} \cdot
                        \left( \operatorname{A} \vec{\theta}, \vec{y} \right)
                    + \nabla_{\vec{\theta}} \cdot
                        \left( \operatorname{A} \vec{\theta},
                            \operatorname{A} \vec{\theta} \right) = \\
                = \vec{0} - 2 \cdot \operatorname{A^*} \vec{y}
                    + 2 \cdot \operatorname{A^*} \operatorname{A} \vec{\theta}
                    + \vec{0}
            \end{align*}

            Приравниваем к нулю
            \begin{align*}
                \operatorname{A^*} \operatorname{A} \vec{\theta}
                = \operatorname{A^*} \vec{y}
            \end{align*}

            И тут важный момент
            \begin{align*}
                \rank{\operatorname{A}} = \rank{\operatorname{A^*}} = m
            \end{align*}

            С помощью оператора $\operatorname{A}$ мы переходим из пространства
            $\mathbb{R}^m$ в пространство $\mathbb{R}^n$, а с помощью оператора
            $\operatorname{A^*}$ мы переходим обратно.

            Выполняется следующее тождество
            \begin{align*}
                \operatorname{A^*} \operatorname{A} \mathbb{R}^m = \mathbb{R}^m
            \end{align*}

            Значит, матрица $\operatorname{A^*} \operatorname{A}$ невырождена
            \begin{align*}
                \det{\operatorname{A^*} \operatorname{A}} > 0
            \end{align*}

            Значит, можно решить уравнения единственным образом
            \begin{equation}\label{eq:leastSquares:formula}
                \index{оценка!метода наименьших квадратов!формула}
                \vec{\theta}_*
                = \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
                    \operatorname{A^*} \vec{y}
            \end{equation}

            Это и есть явный вид оценки метода наименьших квадратов.

        \item Возьмём случайный вектор $\vec{\eta}$ со средним $\vec{m}$
            и ковариацией $\operatorname{B}$.
            
            Возьмём матрицу $\operatorname{C}$. Тогда, согласно утверждению
            \ref{affirmation:vectorRotated}, среднее и ковариационная матрица
            вектора $\operatorname{C}\vec{\eta}$ примут следующий вид
            \begin{align*}
                \mean{\operatorname{C}\vec{\eta}}
                    &= \operatorname{C} \vec{m}\\
                \dCov{\operatorname{C} \vec{\eta}}
                    &= \operatorname{C} \operatorname{B} \operatorname{C^*}
            \end{align*}

            Вектор $\vec{y}$ --- случайный вектор со средним
            $\operatorname{A} \vec{\theta}$ и единичной ковариацией.

            Оценка $\vec{\theta}_*$ действительно несмещённая. Чтобы это
            показать, воспользуемся определением \ref{def:estimatorBias}
            и формулой \ref{eq:leastSquares:formula}
            \begin{align*}
                \mean{\vec{\theta}_*}
                = \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
                    \operatorname{A^*} \mean{\vec{y}}
                = \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
                    \operatorname{A^*} \operatorname{A} \vec{\theta}
                = \vec{\theta}
            \end{align*}

            Посмотрим, какая у оценки ковариационная матрица $\vec{\theta}_*$
            \begin{align*}
                \dCov{\vec{\theta}_*}
                = \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
                    \operatorname{A^*} \operatorname{I} \left\{
                        \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
                    \operatorname{A^*} \right\}^*
            \end{align*}

            Вспомним, что сопряжение меняет местами множители
            \begin{align*}
                \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
                    \operatorname{A^*} \operatorname{I} \left\{
                        \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
                    \operatorname{A^*} \right\}^* = \\
                = \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
                    \operatorname{A^*} \operatorname{A}
                    \left\{ \left( \operatorname{A^*} \operatorname{A}
                        \right)^{-1} \right\}^*
                = \left( \operatorname{A^*} \operatorname{A} \right)^{-1}
            \end{align*}

        \item Почему же оценка $\vec{\theta}_*$ будет оптимальной в классе
            линейных несмещённых оценок?

            \index{пространство!несмещённых оценок!линейных}
            Обозначим пространство линейных несмещённых оценок параметра
            $\vec{\theta}$ с вектором шума $\vec{\xi}$ как
            $\mathfrak{L}_{\vec{\xi}}$.

            \index{пространство!несмещённых оценок}
            \index{оценка!несмещённая!пространство}
            Пространство всех несмещённых оценок параметра $\vec{\theta}$ с
            помехами $\vec{\xi}$ будет $\mathfrak{M}_{\vec{\xi}}$.

            Очевидно, что пространство линейных несмещённых оценок параметра
            $\vec{\theta}$ является подпространством всех несмещённых оценок
            параметра $\vec{\theta}$
            \begin{align*}
                \mathfrak{L}_{\vec{\xi}} \subset \mathfrak{M}_{\vec{\xi}}
            \end{align*}

            Введём ещё одну группу обозначений.

            Пускай $\vec{\eta}$ --- случайный вектор в $\mathbb{R}^n$ с нулевым
            средним и единичной ковариацией, как у вектора $\vec{\xi}$
            \begin{align*}
                \vec{\eta} &\sim N\left( \vec{0}, \operatorname{I} \right) \\
                \mean{\vec{\eta}} &= \mean{\vec{\xi}} = \vec{0} \\
                \dCov{\vec{\eta}} &= \dCov{\vec{\xi}} = \operatorname{I}
            \end{align*}

            Вектору $\vec{\eta}$ поставим в соответствие пространства
            $\mathfrak{L}_{\vec{\eta}}$ и $\mathfrak{M}_{\vec{\eta}}$.

            \index{оценка!линейная несмещённая}
            Что такое линейная несмещённая оценка параметраметра $\vec{\theta}$?
            Это матрицы, применённые к выборке
            \begin{align*}
                \mathfrak{L}_{\vec{\xi}} \ni \operatorname{T} \vec{y}:\;
                \mean{\operatorname{T} \vec{y}}
                = \operatorname{T} \operatorname{A} \vec{\theta}
                = \vec{\theta}
            \end{align*}

            Возьмём последнее равенство из формулы и видим, что тут ничего не
            сказано о помехах --- ни о $\vec{\eta}$, ни о $\vec{\xi}$
            \begin{align*}
                \operatorname{T} \operatorname{A} \vec{\theta}
                = \vec{\theta}
            \end{align*}

            Тогда делаем вывод, что пространство линейных несмещённых оценок
            едино для любых шумов
            \begin{align*}
                \mathfrak{L}_{\vec{\eta}}
                = \mathfrak{L}_{\vec{\xi}}
                \ni \operatorname{T} \vec{y}
            \end{align*}

            Подсчитаем ковариацию вектора $\operatorname{T} \vec{y}$ и видим,
            что она от шумов тоже не зависит
            \begin{align*}
                \dCov{\operatorname{T} \vec{y}}
                = \operatorname{T} \operatorname{T^*}
            \end{align*}

            Следовательно, искать оптимальную линейную несмещённую оценку в
            $\mathfrak{L}_{\vec{\xi}}$ --- то же самое, что искать оптимальную
            линейную несмещённую оценку в $\mathfrak{L}_{\vec{\eta}}$.

            Значит, мы будем использовать пространство
            $\mathfrak{L}_{\vec{\eta}}$, поскольку вектор $\vec{\eta}$
            гауссовский, а для него мы уже умеем искать оптимальную оценку.

            Оценка в формуле \eqref{eq:leastSquares:formula} является
            оптимальной в пространстве несмещённых оценок
            $\mathfrak{M}_{\vec{\xi}}$. Ещё она линейна, а это значит, что
            входит в пространство $\mathfrak{L}_{\vec{\xi}}$.
            То есть, она лучшая в пространстве линейных несмещённых оценок.
    \end{enumerate}
\end{proof}
\subsection{Оценка метода наименьших квадратов при неединичной ковариации}
\index{оценка!метода наименьших квадратов!при неединичной ковариации}
\index{метод!наименьших квадратов!неединичная ковариация}

Есть вектор помех $\vec{\xi}$ с нулевым математическим ожиданием и положительной
ковариацией. Между ``случайными'' помехами есть связь
\begin{align*}
    \mean{\vec{\xi}} &= \vec{0} \\
    \dCov{\vec{\xi}} &= \operatorname{V} > 0
\end{align*}

Умножим вектор $\vec{\xi}$ на матрицу $\operatorname{B}$, квадрат которой равен
обратной к $\operatorname{V}$ матрице, и назовём полученный вектор $\vec{\eta}$
\begin{align*}
    \operatorname{B} &= \operatorname{V^{-\frac{1}{2}}} \\
    \operatorname{\vec{\xi}} &= \vec{\eta} \\
    \dCov{\vec{\eta}} &= \operatorname{B} \dCov{\vec{\xi}} \operatorname{B^*}
        = \operatorname{V^{-\frac{1}{2}}} \operatorname{V}
            \operatorname{V^{-\frac{1}{2}}} = \operatorname{I}
\end{align*}

Тогда, подействовав оператором $\operatorname{B}$ на вектор измерений $\vec{y}$,
получим следующее
\begin{align*}
    \operatorname{B} \vec{y}
    = \operatorname{B} \operatorname{A} \vec{\theta}
        + \operatorname{B} \vec{\xi}
    = \operatorname{V^{-\frac{1}{2}}} \operatorname{A} \vec{\theta} + \vec{\eta}
\end{align*}

Матрица $\operatorname{V^{-\frac{1}{2}}}$ невырожденная и квадратная, а это
значит, что при умножении её на матрицу $\operatorname{A}$ получим результат
с рангом $m$
\begin{align*}
    \rank{\operatorname{V^{-\frac{1}{1}}} \operatorname{A}}
    = \rank{\operatorname{A}}
    = m
\end{align*}

Тогда оценка метода минимальных квадратов будет считаться как минимум такого
выражения
\begin{align*}
    \vec{\theta}_*
    = \argminof{\left\| \operatorname{V^{-\frac{1}{2}}} \operatorname{A}
        \operatorname{V^{-\frac{1}{2}}} \vec{y} \right\|^2}{\vec{\theta}}
\end{align*}

Подставим наши данные в исходную формулу \eqref{eq:leastSquares:formula}
\begin{align*}
    \vec{\theta}_*
    = \left( \operatorname{A^*} \operatorname{V^{-\frac{1}{2}}}
            \operatorname{V^{-\frac{1}{2}}} \operatorname{A} \right)^{-1}
        \operatorname{V^{-\frac{1}{2}}} \operatorname{A^*} \vec{y}
\end{align*}

Получили конечную формулу метода оценки наименьших квадратов при неединичной
ковариации
\begin{equation}\label{eq:leastSquares:formulaAdvanced}
    \index{оценка!метода наименьших квадратов!формула}
    \vec{\theta}_*
    = \left( \operatorname{A^*} \operatorname{V^{-1}}
            \operatorname{A} \right)^{-1}
        \operatorname{V^{-\frac{1}{2}}} \operatorname{A^*} \vec{y}
\end{equation}

\section{Интервальное оценивание}

Есть выборка $x_1, \dots, x_n$ из распределения $\cdfof{\theta}{x}$,
неизвестный параметр $\theta$ --- число действительное
$\theta \in \Theta \subset \mathbb{R}$.

Есть две статистики (функции от выборки, определение \ref{def:statistic})
$T_1 \le T_2$ и константа $\alpha \in \left( 0; 1 \right)$

\begin{definition}[Доверительный интервал]
    \index{доверительный интервал}
    Промежуток $\left[ T_1, T_2 \right]$ ($T_1 \le T_2$ --- статистики)
    называется доверительным интервалом для действительного параметра
    $\theta \in \mathbb{R}$ с уровнем доверия $\alpha$, если
    \begin{align*}
        \forall \theta \in \Theta:\;
        \Probability[\theta]{\theta \in \left[ T_1; T_2 \right]} \ge \alpha
    \end{align*}
\end{definition}

\begin{definition}[Центральная статистика]
    \index{центральная статистика}
    \index{статистика!центральная}
    Функция $G\left( \vec{x}, \theta \right)$, зависящая от выборки и
    неизвестного параметра $\theta$, называется центральной статистикой,
    если выполняются два свойства
    \begin{enumerate}
        \item $G\left( \vec{x}, \theta \right)$ имеет известное распределение
        \item $G\left( \vec{x}, \cdot \right)$ строго монотонная и непрерывная
            функция ($G\left( \vec{\xi}, \theta \right)$ строго монотонна и
            непрерывна по $\theta$)
    \end{enumerate}
\end{definition}
